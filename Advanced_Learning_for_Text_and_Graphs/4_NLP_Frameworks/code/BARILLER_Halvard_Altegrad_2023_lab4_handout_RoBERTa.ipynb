{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsD-LMKT7XMt"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks - RoBERTa</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Halvard Bariller\n",
        "\n",
        "</center>\n",
        "\n",
        "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      },
      "source": [
        "## <b>Preparing the environment and installing libraries, models and data for Part 1 and Part 2</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
        "\n",
        "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. </b>\n",
        "\n",
        "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cGqZ9ZB84Cd",
        "outputId": "e77f985d-8081-4654-aa4e-dfb8365441d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/altegrad.lab3/libs\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 19737, done.\u001b[K\n",
            "remote: Total 19737 (delta 0), reused 0 (delta 0), pack-reused 19737\u001b[K\n",
            "Receiving objects: 100% (19737/19737), 18.58 MiB | 25.09 MiB/s, done.\n",
            "Resolving deltas: 100% (14323/14323), done.\n",
            "Collecting git+https://github.com/hadi-abdine/fairseq\n",
            "  Cloning https://github.com/hadi-abdine/fairseq to /tmp/pip-req-build-r86hgg6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hadi-abdine/fairseq /tmp/pip-req-build-r86hgg6q\n",
            "  Resolved https://github.com/hadi-abdine/fairseq to commit acdd05d6c5c099d7573e199e340df938b5594e99\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.5)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->fairseq==0.12.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19006071 sha256=424089e7ef435df86403bf86cb09ffd8a70224e07f5558f0c6136dd97343115b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tuk_yx89/wheels/32/80/d1/eda3a61a2e0a8d52f2b774f671b0427d1a42d75866252a1515\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=591f3a4a5d85f967499d63c4c58d998ada13c779c44505198a2f0a38ce9cdff5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 169793, done.\u001b[K\n",
            "remote: Counting objects: 100% (2491/2491), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1518/1518), done.\u001b[K\n",
            "remote: Total 169793 (delta 1501), reused 1579 (delta 884), pack-reused 167302\u001b[K\n",
            "Receiving objects: 100% (169793/169793), 171.55 MiB | 22.86 MiB/s, done.\n",
            "Resolving deltas: 100% (127865/127865), done.\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-2a5qwoqd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-2a5qwoqd\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 78f6ed6c70b29c1560780e3869a7ad4c6b3d2710\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.36.0.dev0)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.36.0.dev0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.36.0.dev0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.36.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=7994863 sha256=f5a5c1dc3e7a619c0cceb0a89a6fb79cf38483ec1d0beab6805d38c9bd1ff2df\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w03txml4/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.36.0.dev0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs\n",
        "%cd altegrad.lab3/libs\n",
        "!git clone https://github.com/hadi-abdine/fairseq\n",
        "!pip install git+https://github.com/hadi-abdine/fairseq\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxHffsPm-EqW",
        "outputId": "34458629-4859-42e9-ece1-6b786bafd8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/altegrad.lab3/models\n",
            "--2023-11-14 13:16:52--  https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264887103 (253M) [application/zip]\n",
            "Saving to: ‘model_fairseq.zip’\n",
            "\n",
            "model_fairseq.zip   100%[===================>] 252.62M  18.9MB/s    in 14s     \n",
            "\n",
            "2023-11-14 13:17:07 (17.9 MB/s) - ‘model_fairseq.zip’ saved [264887103/264887103]\n",
            "\n",
            "Archive:  model_fairseq.zip\n",
            "   creating: RoBERTa_small_fr/\n",
            "  inflating: RoBERTa_small_fr/model.pt  \n",
            "  inflating: __MACOSX/RoBERTa_small_fr/._model.pt  \n",
            "  inflating: RoBERTa_small_fr/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr/dict.txt  \n",
            "--2023-11-14 13:17:19--  https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53984187 (51M) [application/zip]\n",
            "Saving to: ‘model_huggingface.zip’\n",
            "\n",
            "model_huggingface.z 100%[===================>]  51.48M  12.8MB/s    in 4.0s    \n",
            "\n",
            "2023-11-14 13:17:23 (12.8 MB/s) - ‘model_huggingface.zip’ saved [53984187/53984187]\n",
            "\n",
            "Archive:  model_huggingface.zip\n",
            "   creating: RoBERTa_small_fr_HuggingFace/\n",
            "  inflating: RoBERTa_small_fr_HuggingFace/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/config.json  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/pytorch_model.bin  \n"
          ]
        }
      ],
      "source": [
        "!cd .. && mkdir models\n",
        "%cd ../models\n",
        "\n",
        "!wget -c 'https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip' -O \"model_fairseq.zip\"\n",
        "!unzip model_fairseq.zip\n",
        "!rm model_fairseq.zip\n",
        "!rm -rf __MACOSX/\n",
        "\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\" -O \"model_huggingface.zip\"\n",
        "!unzip model_huggingface.zip\n",
        "!rm model_huggingface.zip\n",
        "!rm -rf __MACOSX/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfZ_znATNdya",
        "outputId": "12b1f601-7773-423b-ba1b-8ebfc4b3804a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/altegrad.lab3/data\n",
            "--2023-11-14 13:17:25--  https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955190 (933K) [application/zip]\n",
            "Saving to: ‘cls.books.zip’\n",
            "\n",
            "cls.books.zip       100%[===================>] 932.80K  1.48MB/s    in 0.6s    \n",
            "\n",
            "2023-11-14 13:17:26 (1.48 MB/s) - ‘cls.books.zip’ saved [955190/955190]\n",
            "\n",
            "Archive:  cls.books.zip\n",
            "   creating: cls.books/\n",
            "  inflating: cls.books/valid.label   \n",
            "  inflating: cls.books/test.review   \n",
            "  inflating: cls.books/valid.review  \n",
            "  inflating: cls.books/train.review  \n",
            "  inflating: cls.books/train.label   \n",
            "  inflating: cls.books/test.label    \n",
            "/content/altegrad.lab3\n"
          ]
        }
      ],
      "source": [
        "!cd .. && mkdir data\n",
        "%cd ../data\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\" -O \"cls.books.zip\"\n",
        "!unzip cls.books.zip\n",
        "!rm cls.books.zip\n",
        "!rm -rf __MACOSX/\n",
        "!mkdir cls.books-json\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H40TxVIvEWyu"
      },
      "source": [
        "# <b>Part 1: Fairseq</b>\n",
        "\n",
        "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
        "\n",
        " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
        " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
        " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
        "\n",
        " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvpyZEexOXHm"
      },
      "source": [
        "## <b> Number of parameters of the model</b>\n",
        "\n",
        "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch (Only the base model with out the head). (<b>Hint:</b> you can check the architecture of the model using model['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7isz60LOwlV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-22f0yxRDKo",
        "outputId": "d26bf4d3-9740-4fc6-c848-100be3b7f6e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['encoder.sentence_encoder.version', 'encoder.sentence_encoder.embed_tokens.weight', 'encoder.sentence_encoder.embed_positions.weight', 'encoder.sentence_encoder.layernorm_embedding.weight', 'encoder.sentence_encoder.layernorm_embedding.bias', 'encoder.sentence_encoder.layers.0.self_attn.k_proj.weight', 'encoder.sentence_encoder.layers.0.self_attn.k_proj.bias', 'encoder.sentence_encoder.layers.0.self_attn.v_proj.weight', 'encoder.sentence_encoder.layers.0.self_attn.v_proj.bias', 'encoder.sentence_encoder.layers.0.self_attn.q_proj.weight', 'encoder.sentence_encoder.layers.0.self_attn.q_proj.bias', 'encoder.sentence_encoder.layers.0.self_attn.out_proj.weight', 'encoder.sentence_encoder.layers.0.self_attn.out_proj.bias', 'encoder.sentence_encoder.layers.0.self_attn_layer_norm.weight', 'encoder.sentence_encoder.layers.0.self_attn_layer_norm.bias', 'encoder.sentence_encoder.layers.0.fc1.weight', 'encoder.sentence_encoder.layers.0.fc1.bias', 'encoder.sentence_encoder.layers.0.fc2.weight', 'encoder.sentence_encoder.layers.0.fc2.bias', 'encoder.sentence_encoder.layers.0.final_layer_norm.weight', 'encoder.sentence_encoder.layers.0.final_layer_norm.bias', 'encoder.sentence_encoder.layers.1.self_attn.k_proj.weight', 'encoder.sentence_encoder.layers.1.self_attn.k_proj.bias', 'encoder.sentence_encoder.layers.1.self_attn.v_proj.weight', 'encoder.sentence_encoder.layers.1.self_attn.v_proj.bias', 'encoder.sentence_encoder.layers.1.self_attn.q_proj.weight', 'encoder.sentence_encoder.layers.1.self_attn.q_proj.bias', 'encoder.sentence_encoder.layers.1.self_attn.out_proj.weight', 'encoder.sentence_encoder.layers.1.self_attn.out_proj.bias', 'encoder.sentence_encoder.layers.1.self_attn_layer_norm.weight', 'encoder.sentence_encoder.layers.1.self_attn_layer_norm.bias', 'encoder.sentence_encoder.layers.1.fc1.weight', 'encoder.sentence_encoder.layers.1.fc1.bias', 'encoder.sentence_encoder.layers.1.fc2.weight', 'encoder.sentence_encoder.layers.1.fc2.bias', 'encoder.sentence_encoder.layers.1.final_layer_norm.weight', 'encoder.sentence_encoder.layers.1.final_layer_norm.bias', 'encoder.sentence_encoder.layers.2.self_attn.k_proj.weight', 'encoder.sentence_encoder.layers.2.self_attn.k_proj.bias', 'encoder.sentence_encoder.layers.2.self_attn.v_proj.weight', 'encoder.sentence_encoder.layers.2.self_attn.v_proj.bias', 'encoder.sentence_encoder.layers.2.self_attn.q_proj.weight', 'encoder.sentence_encoder.layers.2.self_attn.q_proj.bias', 'encoder.sentence_encoder.layers.2.self_attn.out_proj.weight', 'encoder.sentence_encoder.layers.2.self_attn.out_proj.bias', 'encoder.sentence_encoder.layers.2.self_attn_layer_norm.weight', 'encoder.sentence_encoder.layers.2.self_attn_layer_norm.bias', 'encoder.sentence_encoder.layers.2.fc1.weight', 'encoder.sentence_encoder.layers.2.fc1.bias', 'encoder.sentence_encoder.layers.2.fc2.weight', 'encoder.sentence_encoder.layers.2.fc2.bias', 'encoder.sentence_encoder.layers.2.final_layer_norm.weight', 'encoder.sentence_encoder.layers.2.final_layer_norm.bias', 'encoder.sentence_encoder.layers.3.self_attn.k_proj.weight', 'encoder.sentence_encoder.layers.3.self_attn.k_proj.bias', 'encoder.sentence_encoder.layers.3.self_attn.v_proj.weight', 'encoder.sentence_encoder.layers.3.self_attn.v_proj.bias', 'encoder.sentence_encoder.layers.3.self_attn.q_proj.weight', 'encoder.sentence_encoder.layers.3.self_attn.q_proj.bias', 'encoder.sentence_encoder.layers.3.self_attn.out_proj.weight', 'encoder.sentence_encoder.layers.3.self_attn.out_proj.bias', 'encoder.sentence_encoder.layers.3.self_attn_layer_norm.weight', 'encoder.sentence_encoder.layers.3.self_attn_layer_norm.bias', 'encoder.sentence_encoder.layers.3.fc1.weight', 'encoder.sentence_encoder.layers.3.fc1.bias', 'encoder.sentence_encoder.layers.3.fc2.weight', 'encoder.sentence_encoder.layers.3.fc2.bias', 'encoder.sentence_encoder.layers.3.final_layer_norm.weight', 'encoder.sentence_encoder.layers.3.final_layer_norm.bias', 'encoder.lm_head.weight', 'encoder.lm_head.bias', 'encoder.lm_head.dense.weight', 'encoder.lm_head.dense.bias', 'encoder.lm_head.layer_norm.weight', 'encoder.lm_head.layer_norm.bias'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model[\"model\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulpadxhteiqv"
      },
      "outputs": [],
      "source": [
        "param_not_to_count = ['encoder.sentence_encoder.version', 'encoder.lm_head.weight', 'encoder.lm_head.bias',\n",
        "                      'encoder.lm_head.dense.weight', 'encoder.lm_head.dense.bias',\n",
        "                      'encoder.lm_head.layer_norm.weight', 'encoder.lm_head.layer_norm.bias']\n",
        "\n",
        "# param_not_to_count = []\n",
        "params = 0\n",
        "\n",
        "for param in model[\"model\"]:\n",
        "  if param not in param_not_to_count:\n",
        "    params += model[\"model\"][str(param)].numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4MUaLVFjMMr",
        "outputId": "f524f9ca-56b6-4dc0-e4a7-a0299061448e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of parameters:  22829056\n"
          ]
        }
      ],
      "source": [
        "print(\"The total number of parameters: \",  params )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz8fnWOSI0eF"
      },
      "source": [
        "## <b>Tokenizing the reviews</b>\n",
        "\n",
        "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
        "\n",
        "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
        "\n",
        "Documentation: https://github.com/google/sentencepiece#readme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-hOlotmOW7f"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "s = spm.SentencePieceProcessor(model_file='models/RoBERTa_small_fr/sentencepiece.bpe.model')\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "SENTS=\"review\"\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.'+SENTS, 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "        if split == 'train':\n",
        "          train_length = len(reviews)\n",
        "        for i, review in enumerate(reviews):\n",
        "          reviews[i] = ' '.join(s.encode_as_pieces(review))\n",
        "\n",
        "        # It should look something like this :\n",
        "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
        "\n",
        "    with open('data/cls.books/'+split+'.spm.'+SENTS, 'w') as f:\n",
        "        for review in reviews:\n",
        "          f.write(review+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voq8_GCSQrB1",
        "outputId": "f3719fac-78ba-4446-c4a0-43b4d6e5c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁même ▁pour ▁des ▁né o phy tes ▁en ▁physique ▁des ▁matériaux , ▁comment ▁et ▁pourquoi ▁la ▁version ▁officielle ▁concernant ▁l ' ef fond re ment ▁des ▁3 ▁tour s ▁du ▁W TC ▁ne ▁tient ▁tout ▁simplement ▁pas ▁de bou t . ▁Et ▁c ' est ▁sans ▁appel ▁! ▁Il ▁constitu e ▁un ▁très ▁bon ▁argument aire ▁scientifique ▁permettant ▁de ▁soutenir ▁l ' hy po th èse ▁la ▁plus ▁vrai sem bla ble ▁pour ▁explique r ▁ce ▁qui ▁s ' est ▁réellement ▁passé ▁à ▁New ▁York , ▁ce ▁jour - là , ▁l ' hy po th èse ▁d ' une ▁dé moli tion ▁cont rô lé e ▁! ▁A ▁lire ▁absolument ▁!\n"
          ]
        }
      ],
      "source": [
        "print(reviews[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGNK19XuKBk0"
      },
      "source": [
        "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
        "\n",
        "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
        "\n",
        "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>. You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
        "\n",
        "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
        "\n",
        "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIF1wvWoFp4d",
        "outputId": "5ecd0b04-065b-4073-defb-99e1955e34d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-14 13:20:04.396863: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:20:04.396923: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:20:04.396960: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:20:04.405667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:20:05.888520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:20:09 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.spm.review', validpref='data/cls.books/valid.spm.review', testpref='data/cls.books/test.spm.review', align_suffix=None, destdir='data/cls.books-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
            "2023-11-14 13:20:09 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-14 13:20:12 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
            "2023-11-14 13:20:12 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-14 13:20:13 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
            "2023-11-14 13:20:13 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-14 13:20:15 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
            "2023-11-14 13:20:15 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls.books-bin/input0\n",
            "2023-11-14 13:20:20.386823: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:20:20.386883: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:20:20.386915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:20:20.394554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:20:21.496298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:20:23 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.label', validpref='data/cls.books/valid.label', testpref='data/cls.books/test.label', align_suffix=None, destdir='data/cls.books-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
            "2023-11-14 13:20:23 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-14 13:20:24 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls.books-bin/label\n"
          ]
        }
      ],
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --trainpref data/cls.books/train.spm.review \\\n",
        "              --validpref data/cls.books/valid.spm.review \\\n",
        "              --testpref data/cls.books/test.spm.review \\\n",
        "              --destdir data/cls.books-bin/input0 \\\n",
        "              --srcdict models/RoBERTa_small_fr/dict.txt \\\n",
        "              --workers 8)#binarize the tokenized reviews\n",
        "\n",
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --trainpref data/cls.books/train.label \\\n",
        "              --validpref data/cls.books/valid.label \\\n",
        "              --testpref data/cls.books/test.label \\\n",
        "              --destdir data/cls.books-bin/label \\\n",
        "              --workers 8)# binarize the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SSjBcQJnuSL"
      },
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
        "\n",
        "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV2112YPJEDA"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "TASK= \"sentence_prediction\" # sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH= \"/content/altegrad.lab3/data/cls.books-bin\" \n",
        "MODEL_PATH= \"/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt\" \n",
        "MAX_EPOCH= 5 \n",
        "MAX_SENTENCES= 8 # batch size\n",
        "MAX_UPDATE= int(MAX_EPOCH * train_length / MAX_SENTENCES) # number of backward propagation steps\n",
        "LR= 1e-5 \n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = \"accuracy\" # use the accuracy metric\n",
        "NUM_CLASSES= 2 #number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = int(.06 * MAX_UPDATE) # warmup ratio=6% of the whole training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mdznms-EYyz",
        "outputId": "0d7b8eec-518e-4df1-ead3-7d7bba6d5a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-14 13:20:27.550881: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:20:27.550941: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:20:27.550976: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:20:27.558476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:20:29.173326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:20:30 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:20:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', 'restore_file': '/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/0', restore_file='/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:20:35 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:20:35 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:20:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:20:36 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:20:36 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:20:36 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:20:36 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:20:36 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:20:36 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:20:43 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:20:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:20:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:20:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:20:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:20:43 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:20:43 | INFO | fairseq.trainer | Preparing to load checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt\n",
            "2023-11-14 13:20:43 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-14 13:20:43 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-14 13:20:43 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-14 13:20:43 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-14 13:20:43 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:20:44 | INFO | fairseq.trainer | Loaded checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-14 13:20:44 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:20:44 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:20:44 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:20:44 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:20:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:20:44 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:20:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 224/225 [00:15<00:00, 21.02it/s, loss=0.774, nll_loss=0.006, accuracy=87.5, wps=22850.6, ups=21.3, wpb=1072.6, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=3.85, train_wall=0, gb_free=14.2, wall=16]2023-11-14 13:20:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 3/25 [00:00<00:00, 26.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 43.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 18/25 [00:00<00:00, 60.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:21:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.919 | nll_loss 0.007 | accuracy 64 | wps 76000.7 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:21:00 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   3% 8/250 [00:00<00:06, 39.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 66.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 70.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 72.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 72.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 73.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 74.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 74.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 73.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 73.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 73.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  42% 105/250 [00:01<00:01, 72.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 74.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 73.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 74.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 137/250 [00:01<00:01, 73.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 73.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 74.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 72.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 72.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  71% 177/250 [00:02<00:01, 71.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  74% 185/250 [00:02<00:00, 72.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  77% 193/250 [00:02<00:00, 71.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  80% 201/250 [00:02<00:00, 73.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  84% 209/250 [00:02<00:00, 73.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  87% 217/250 [00:03<00:00, 73.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  90% 225/250 [00:03<00:00, 73.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  93% 233/250 [00:03<00:00, 71.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  96% 241/250 [00:03<00:00, 73.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset: 100% 250/250 [00:03<00:00, 76.90it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:21:03 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.871 | nll_loss 0.007 | accuracy 67.7 | wps 76846.3 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:21:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:21:03 | INFO | train | epoch 001 | loss 0.971 | nll_loss 0.007 | accuracy 59.1 | wps 14603.9 | ups 13.68 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 3.533 | train_wall 14 | gb_free 14.3 | wall 20\n",
            "2023-11-14 13:21:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:21:03 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:21:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 224/225 [00:11<00:00, 18.75it/s, loss=0.833, nll_loss=0.006, accuracy=67.5, wps=21520.3, ups=18.2, wpb=1182.2, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=8.296, train_wall=0, gb_free=14.3, wall=32]2023-11-14 13:21:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 25.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 37.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 40.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 48.09it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:21:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.838 | nll_loss 0.006 | accuracy 68.5 | wps 52588 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 68.5\n",
            "2023-11-14 13:21:16 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:33,  7.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 44.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 49.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 50.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 52.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 53.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 47/250 [00:00<00:03, 53.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 53.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 54.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 54.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 54.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 54.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 53.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 89/250 [00:01<00:03, 52.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 52.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 52.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 53.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 53.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 53.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 53.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 53.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 53.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  57% 143/250 [00:02<00:02, 53.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 57.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  63% 158/250 [00:03<00:01, 62.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  66% 166/250 [00:03<00:01, 65.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 68.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  73% 182/250 [00:03<00:00, 70.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 190/250 [00:03<00:00, 71.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 72.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 73.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 71.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 72.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 72.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  95% 238/250 [00:04<00:00, 73.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 75.53it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:21:20 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.781 | nll_loss 0.006 | accuracy 73.9 | wps 63603.5 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:21:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:21:20 | INFO | train | epoch 002 | loss 0.757 | nll_loss 0.006 | accuracy 74.2 | wps 14204.6 | ups 13.31 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 6.419 | train_wall 11 | gb_free 14.3 | wall 37\n",
            "2023-11-14 13:21:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:21:20 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:21:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 224/225 [00:11<00:00, 17.96it/s, loss=0.743, nll_loss=0.005, accuracy=75, wps=18904.1, ups=17.24, wpb=1096.6, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=7.444, train_wall=0, gb_free=14.3, wall=48]2023-11-14 13:21:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 38.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 19/25 [00:00<00:00, 48.33it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:21:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.61 | nll_loss 0.005 | accuracy 81 | wps 57925.8 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 81\n",
            "2023-11-14 13:21:32 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:34,  7.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 40.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   7% 18/250 [00:00<00:04, 46.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 49.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 50.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  14% 36/250 [00:00<00:04, 52.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  17% 42/250 [00:00<00:04, 51.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  19% 48/250 [00:01<00:03, 52.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 52.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 53.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 53.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 51.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 49.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 50.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  36% 90/250 [00:01<00:03, 51.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 51.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 102/250 [00:02<00:02, 53.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 54.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 54.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 55.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 54.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  53% 132/250 [00:02<00:02, 55.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  55% 138/250 [00:02<00:02, 54.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 53.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 53.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  62% 156/250 [00:03<00:01, 53.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  65% 162/250 [00:03<00:01, 53.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  67% 168/250 [00:03<00:01, 52.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 53.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 53.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  74% 186/250 [00:03<00:01, 53.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  77% 192/250 [00:03<00:01, 53.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 54.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 56.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 210/250 [00:04<00:00, 56.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  86% 216/250 [00:04<00:00, 56.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  89% 223/250 [00:04<00:00, 56.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 230/250 [00:04<00:00, 58.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 59.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 244/250 [00:04<00:00, 62.02it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:21:37 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.65 | nll_loss 0.005 | accuracy 79.3 | wps 57354.7 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:21:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:21:37 | INFO | train | epoch 003 | loss 0.642 | nll_loss 0.005 | accuracy 79.7 | wps 14242.2 | ups 13.35 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 7.723 | train_wall 11 | gb_free 14.3 | wall 54\n",
            "2023-11-14 13:21:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:21:37 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:21:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 223/225 [00:11<00:00, 21.25it/s, loss=0.596, nll_loss=0.004, accuracy=80, wps=24154.7, ups=20.87, wpb=1157.6, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=9.9, train_wall=0, gb_free=14.3, wall=65]2023-11-14 13:21:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 54.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:21:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.644 | nll_loss 0.005 | accuracy 77 | wps 72803.2 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 81\n",
            "2023-11-14 13:21:49 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 17.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 40.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 54.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 61.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 66.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 40/250 [00:00<00:03, 69.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  19% 47/250 [00:00<00:03, 65.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 54/250 [00:00<00:03, 62.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  24% 61/250 [00:01<00:03, 61.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 60.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  30% 75/250 [00:01<00:02, 60.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  33% 82/250 [00:01<00:02, 59.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  35% 88/250 [00:01<00:02, 57.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 57.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  40% 100/250 [00:01<00:02, 56.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  42% 106/250 [00:01<00:02, 56.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  45% 112/250 [00:01<00:02, 56.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  47% 118/250 [00:02<00:02, 56.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 124/250 [00:02<00:02, 56.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  52% 130/250 [00:02<00:02, 56.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 136/250 [00:02<00:01, 57.81it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 55.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  59% 148/250 [00:02<00:01, 56.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  62% 154/250 [00:02<00:01, 56.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 57.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 57.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  69% 172/250 [00:02<00:01, 56.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 178/250 [00:03<00:01, 57.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  74% 184/250 [00:03<00:01, 57.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 190/250 [00:03<00:01, 57.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 196/250 [00:03<00:00, 56.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  81% 202/250 [00:03<00:00, 55.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  83% 208/250 [00:03<00:00, 54.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 55.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 55.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 226/250 [00:03<00:00, 56.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  93% 232/250 [00:04<00:00, 56.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 58.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 61.52it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:21:53 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.659 | nll_loss 0.005 | accuracy 78.9 | wps 61659.5 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:21:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:21:53 | INFO | train | epoch 004 | loss 0.58 | nll_loss 0.004 | accuracy 81.9 | wps 14571.3 | ups 13.65 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 8.378 | train_wall 11 | gb_free 14.3 | wall 70\n",
            "2023-11-14 13:21:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:21:53 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:21:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 224/225 [00:12<00:00, 20.56it/s, loss=0.378, nll_loss=0.003, accuracy=87.5, wps=23162.4, ups=20.74, wpb=1117, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=9.47, train_wall=0, gb_free=14.3, wall=82]2023-11-14 13:22:06 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:22:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 16.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 43.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 58.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:22:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.589 | nll_loss 0.004 | accuracy 81.5 | wps 73888.2 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 81.5\n",
            "2023-11-14 13:22:06 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 41.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 64.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 67.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 40/250 [00:00<00:03, 69.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 68.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 69.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 62/250 [00:00<00:02, 68.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 69.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 77/250 [00:01<00:02, 69.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  34% 85/250 [00:01<00:02, 70.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  37% 93/250 [00:01<00:02, 70.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 101/250 [00:01<00:02, 69.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 109/250 [00:01<00:02, 70.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  47% 117/250 [00:01<00:01, 68.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:01<00:01, 69.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  53% 133/250 [00:01<00:01, 70.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 70.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 70.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 69.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  66% 164/250 [00:02<00:01, 69.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  69% 172/250 [00:02<00:01, 70.24it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  72% 180/250 [00:02<00:00, 70.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  75% 188/250 [00:02<00:00, 67.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  78% 196/250 [00:02<00:00, 68.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 204/250 [00:02<00:00, 69.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 70.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 70.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  91% 228/250 [00:03<00:00, 70.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  94% 236/250 [00:03<00:00, 71.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 244/250 [00:03<00:00, 72.24it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:22:10 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.639 | nll_loss 0.005 | accuracy 79.9 | wps 73602.7 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:22:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:22:10 | INFO | train | epoch 005 | loss 0.551 | nll_loss 0.004 | accuracy 83.3 | wps 14537.4 | ups 13.62 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 8.613 | train_wall 11 | gb_free 14.3 | wall 87\n",
            "2023-11-14 13:22:10 | INFO | fairseq_cli.train | done training in 86.1 seconds\n",
            "2023-11-14 13:22:15.563094: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:22:15.563176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:22:15.563222: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:22:15.575181: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:22:17.373985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:22:19 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:22:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', 'restore_file': '/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/1', restore_file='/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:22:22 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:22:22 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:22:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:22:23 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:22:23 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:22:23 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:22:23 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:22:23 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:22:23 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:22:25 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:22:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:22:25 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:22:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:22:25 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:22:25 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:22:25 | INFO | fairseq.trainer | Preparing to load checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt\n",
            "2023-11-14 13:22:25 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-14 13:22:25 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-14 13:22:25 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-14 13:22:25 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-14 13:22:25 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:22:26 | INFO | fairseq.trainer | Loaded checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-14 13:22:26 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:22:26 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:22:26 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:22:26 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:22:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:22:26 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:22:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 224/225 [00:13<00:00, 18.23it/s, loss=0.79, nll_loss=0.006, accuracy=75, wps=19228.9, ups=18.54, wpb=1037, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=4.151, train_wall=0, gb_free=14.3, wall=13]2023-11-14 13:22:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 24.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 37.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 41.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 47.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:22:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.872 | nll_loss 0.007 | accuracy 65.5 | wps 54254.3 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:22:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:29,  8.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 43.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 46.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 48.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 50.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 52.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 47/250 [00:01<00:03, 53.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 53.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 54.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 54.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 53.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 53.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 53.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 91/250 [00:01<00:02, 59.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  40% 99/250 [00:01<00:02, 63.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 66.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 115/250 [00:02<00:01, 68.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  49% 122/250 [00:02<00:01, 68.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 130/250 [00:02<00:01, 69.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 137/250 [00:02<00:01, 69.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 69.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 69.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 71.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  67% 168/250 [00:02<00:01, 72.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  70% 176/250 [00:02<00:01, 72.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  74% 184/250 [00:03<00:00, 72.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  77% 192/250 [00:03<00:00, 73.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  80% 200/250 [00:03<00:00, 72.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  83% 208/250 [00:03<00:00, 71.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 216/250 [00:03<00:00, 71.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  90% 224/250 [00:03<00:00, 68.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  93% 232/250 [00:03<00:00, 70.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  96% 240/250 [00:03<00:00, 70.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset: 100% 249/250 [00:03<00:00, 73.39it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:22:43 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.816 | nll_loss 0.006 | accuracy 72 | wps 67093 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:22:43 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:22:43 | INFO | train | epoch 001 | loss 0.961 | nll_loss 0.007 | accuracy 60.6 | wps 14149.2 | ups 13.27 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 3.618 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-14 13:22:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:22:43 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:22:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:11<00:00, 19.01it/s, loss=0.662, nll_loss=0.005, accuracy=80, wps=20129.4, ups=19.78, wpb=1017.4, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=7.551, train_wall=0, gb_free=14.2, wall=30]2023-11-14 13:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 23.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 10/25 [00:00<00:00, 33.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 43.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 23/25 [00:00<00:00, 50.47it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:22:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.745 | nll_loss 0.006 | accuracy 71 | wps 55642.1 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 71\n",
            "2023-11-14 13:22:56 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:36,  6.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 35.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 40.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 44.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 47.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 48.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 39/250 [00:00<00:04, 48.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  18% 45/250 [00:01<00:04, 49.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  20% 51/250 [00:01<00:03, 50.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  23% 57/250 [00:01<00:03, 50.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  25% 63/250 [00:01<00:03, 50.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 69/250 [00:01<00:03, 50.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  30% 75/250 [00:01<00:03, 50.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 81/250 [00:01<00:03, 50.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  35% 87/250 [00:01<00:03, 51.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  37% 93/250 [00:01<00:03, 51.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 99/250 [00:02<00:02, 51.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 105/250 [00:02<00:02, 50.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  44% 111/250 [00:02<00:02, 51.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  47% 117/250 [00:02<00:02, 51.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  49% 123/250 [00:02<00:02, 51.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 52.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  54% 135/250 [00:02<00:02, 51.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  56% 141/250 [00:02<00:02, 51.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  59% 148/250 [00:03<00:01, 53.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 154/250 [00:03<00:01, 52.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 160/250 [00:03<00:01, 51.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  66% 166/250 [00:03<00:01, 51.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  69% 172/250 [00:03<00:01, 51.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  71% 178/250 [00:03<00:01, 51.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 184/250 [00:03<00:01, 53.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 190/250 [00:03<00:01, 52.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  78% 196/250 [00:03<00:01, 52.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  81% 203/250 [00:04<00:00, 54.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 210/250 [00:04<00:00, 56.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 216/250 [00:04<00:00, 56.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  89% 222/250 [00:04<00:00, 55.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 228/250 [00:04<00:00, 55.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 234/250 [00:04<00:00, 53.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 52.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 51.20it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:23:01 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.712 | nll_loss 0.005 | accuracy 76.4 | wps 54258.5 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:23:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:23:01 | INFO | train | epoch 002 | loss 0.755 | nll_loss 0.006 | accuracy 75.7 | wps 13722.5 | ups 12.86 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 6.695 | train_wall 11 | gb_free 14.3 | wall 36\n",
            "2023-11-14 13:23:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:23:01 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:23:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 223/225 [00:13<00:00, 16.96it/s, loss=0.598, nll_loss=0.004, accuracy=87.5, wps=18878.9, ups=16.96, wpb=1113.4, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=8.536, train_wall=0, gb_free=14.3, wall=49]2023-11-14 13:23:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 35.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 41.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 51.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:23:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.631 | nll_loss 0.005 | accuracy 80 | wps 54948.3 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 80\n",
            "2023-11-14 13:23:15 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:33,  7.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 4/250 [00:00<00:13, 18.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   4% 10/250 [00:00<00:07, 34.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 42.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 22/250 [00:00<00:04, 46.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 48.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 50.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 49.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 52.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 52.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 51.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 52.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 54.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 52.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 52.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 88/250 [00:01<00:02, 54.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 53.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  40% 100/250 [00:02<00:02, 55.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 55.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 52.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 118/250 [00:02<00:02, 52.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 124/250 [00:02<00:02, 53.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 130/250 [00:02<00:02, 52.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 136/250 [00:02<00:02, 51.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  57% 142/250 [00:02<00:02, 51.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 148/250 [00:02<00:01, 51.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  62% 154/250 [00:03<00:01, 52.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 160/250 [00:03<00:01, 53.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 166/250 [00:03<00:01, 54.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 57.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 58.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  75% 187/250 [00:03<00:01, 59.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  77% 193/250 [00:03<00:00, 58.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 57.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  82% 205/250 [00:03<00:00, 54.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 211/250 [00:04<00:00, 55.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  87% 218/250 [00:04<00:00, 57.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  90% 224/250 [00:04<00:00, 56.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 230/250 [00:04<00:00, 56.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  94% 236/250 [00:04<00:00, 56.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 244/250 [00:04<00:00, 60.49it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:23:20 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.652 | nll_loss 0.005 | accuracy 78.5 | wps 57125 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:23:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:23:20 | INFO | train | epoch 003 | loss 0.651 | nll_loss 0.005 | accuracy 78.9 | wps 12705.2 | ups 11.91 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 7.772 | train_wall 12 | gb_free 14.3 | wall 55\n",
            "2023-11-14 13:23:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:23:20 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:23:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 21.34it/s, loss=0.582, nll_loss=0.005, accuracy=77.5, wps=23913.4, ups=23.89, wpb=1001, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=7.653, train_wall=0, gb_free=14.3, wall=66]2023-11-14 13:23:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 42.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 53.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 65.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:23:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.629 | nll_loss 0.005 | accuracy 76 | wps 71497.8 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 80\n",
            "2023-11-14 13:23:32 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.66it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 45.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 60.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 64.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 65.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  18% 46/250 [00:00<00:02, 70.04it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 54/250 [00:00<00:02, 71.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  25% 62/250 [00:00<00:02, 71.25it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 71.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  31% 78/250 [00:01<00:02, 71.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 71.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 72.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 71.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 110/250 [00:01<00:01, 71.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 70.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 71.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 71.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 69.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 64.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  62% 156/250 [00:02<00:01, 63.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  65% 163/250 [00:02<00:01, 61.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 170/250 [00:02<00:01, 59.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 177/250 [00:02<00:01, 57.43it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 183/250 [00:02<00:01, 56.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 189/250 [00:02<00:01, 56.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 195/250 [00:03<00:01, 54.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 54.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 55.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 54.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 219/250 [00:03<00:00, 54.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 225/250 [00:03<00:00, 53.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 54.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  95% 237/250 [00:03<00:00, 54.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  97% 243/250 [00:03<00:00, 51.26it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:23:36 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.654 | nll_loss 0.005 | accuracy 78.7 | wps 65680.9 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:23:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:23:36 | INFO | train | epoch 004 | loss 0.581 | nll_loss 0.004 | accuracy 82.1 | wps 14484.3 | ups 13.57 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 8.375 | train_wall 11 | gb_free 14.3 | wall 71\n",
            "2023-11-14 13:23:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:23:36 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:23:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:12<00:00, 20.95it/s, loss=0.471, nll_loss=0.004, accuracy=85, wps=20505.3, ups=21.51, wpb=953.4, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=7.841, train_wall=0, gb_free=14.3, wall=84]2023-11-14 13:23:49 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:23:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 15.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 7/25 [00:00<00:00, 32.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 52.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 63.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:23:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.605 | nll_loss 0.005 | accuracy 79 | wps 67656.1 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 80\n",
            "2023-11-14 13:23:50 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   1% 2/250 [00:00<00:16, 15.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   4% 10/250 [00:00<00:05, 46.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   7% 18/250 [00:00<00:03, 58.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 62.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 67.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 71.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 71.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 73.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 74.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 73.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 73.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 73.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 74.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  42% 105/250 [00:01<00:01, 73.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 73.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 74.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 73.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 137/250 [00:01<00:01, 72.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 72.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 73.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 73.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 74.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  71% 177/250 [00:02<00:00, 75.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  74% 185/250 [00:02<00:00, 74.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  77% 193/250 [00:02<00:00, 73.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 201/250 [00:02<00:00, 71.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  84% 209/250 [00:02<00:00, 71.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  87% 217/250 [00:03<00:00, 72.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  90% 225/250 [00:03<00:00, 71.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  93% 233/250 [00:03<00:00, 71.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  96% 241/250 [00:03<00:00, 73.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset: 100% 250/250 [00:03<00:00, 76.37it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:23:53 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.646 | nll_loss 0.005 | accuracy 79.5 | wps 76535.9 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:23:53 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:23:53 | INFO | train | epoch 005 | loss 0.556 | nll_loss 0.004 | accuracy 83.2 | wps 14447.8 | ups 13.54 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 8.608 | train_wall 11 | gb_free 14.3 | wall 88\n",
            "2023-11-14 13:23:53 | INFO | fairseq_cli.train | done training in 87.3 seconds\n",
            "2023-11-14 13:23:57.659104: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:23:57.659161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:23:57.659201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:23:57.671318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:23:59.361141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:24:01 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:24:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', 'restore_file': '/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu1125_lr1e-05_me5/2', restore_file='/content/altegrad.lab3/models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:24:05 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:24:05 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:24:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:24:06 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:24:06 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:24:06 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:24:06 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:24:06 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:24:06 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:24:09 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:24:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:24:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:24:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:24:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:24:09 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:24:09 | INFO | fairseq.trainer | Preparing to load checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt\n",
            "2023-11-14 13:24:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-14 13:24:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-14 13:24:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-14 13:24:09 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-14 13:24:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:24:09 | INFO | fairseq.trainer | Loaded checkpoint /content/altegrad.lab3/models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-14 13:24:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:24:09 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:24:09 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:24:09 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:24:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:24:09 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:24:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:12<00:00, 17.85it/s, loss=0.683, nll_loss=0.005, accuracy=85, wps=19635.1, ups=17.45, wpb=1125.2, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=4.574, train_wall=0, gb_free=14.2, wall=13]2023-11-14 13:24:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 4/25 [00:00<00:01, 19.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 31.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 41.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 23/25 [00:00<00:00, 52.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:24:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.826 | nll_loss 0.006 | accuracy 71 | wps 54398.2 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:24:22 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:37,  6.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 6/250 [00:00<00:09, 26.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 12/250 [00:00<00:06, 38.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 18/250 [00:00<00:05, 44.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   9% 23/250 [00:00<00:05, 44.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 48.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 50.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 41/250 [00:00<00:04, 51.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 47/250 [00:01<00:03, 53.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 53.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 53.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 53.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 54.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 54.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 54.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 54.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 54.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 54.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 53.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 53.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 54.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 54.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 54.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 54.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 55.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 54.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 53.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 53.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 53.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 53.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 54.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 55.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 55.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  79% 197/250 [00:03<00:00, 56.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 55.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  84% 209/250 [00:04<00:00, 54.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 215/250 [00:04<00:00, 53.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 52.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 53.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 54.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 53.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  99% 247/250 [00:04<00:00, 56.97it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:24:27 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.798 | nll_loss 0.006 | accuracy 71.5 | wps 56551.3 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:24:27 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:24:27 | INFO | train | epoch 001 | loss 0.949 | nll_loss 0.007 | accuracy 61.6 | wps 13980.8 | ups 13.08 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 3.729 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-14 13:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:24:27 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:24:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:11<00:00, 18.30it/s, loss=0.666, nll_loss=0.006, accuracy=87.5, wps=15706.9, ups=18.89, wpb=831.4, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=5.447, train_wall=0, gb_free=14.3, wall=30]2023-11-14 13:24:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 25.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 34.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 19/25 [00:00<00:00, 46.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 46.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:24:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.755 | nll_loss 0.006 | accuracy 69.5 | wps 54175.6 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 71\n",
            "2023-11-14 13:24:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:39,  6.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:11, 21.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 34.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 41.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 45.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 48.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 50.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:04, 52.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 47/250 [00:01<00:03, 51.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 52.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 52.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 52.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 51.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 51.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 53.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 54.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 54.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 52.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 54.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 52.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 53.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 54.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 54.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 53.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  57% 143/250 [00:02<00:02, 52.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 51.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 51.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 51.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 50.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 51.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 49.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 50.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 51.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  79% 197/250 [00:03<00:01, 51.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  81% 203/250 [00:04<00:00, 52.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 209/250 [00:04<00:00, 52.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 215/250 [00:04<00:00, 52.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 50.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 51.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 51.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 53.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 48.29it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:24:44 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.717 | nll_loss 0.005 | accuracy 75.9 | wps 54397.3 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:24:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:24:44 | INFO | train | epoch 002 | loss 0.744 | nll_loss 0.006 | accuracy 75.8 | wps 13736 | ups 12.87 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 6.145 | train_wall 11 | gb_free 14.3 | wall 36\n",
            "2023-11-14 13:24:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:24:44 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:24:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 222/225 [00:12<00:00, 20.58it/s, loss=0.544, nll_loss=0.004, accuracy=87.5, wps=24970.3, ups=21.13, wpb=1182, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=7.22, train_wall=0, gb_free=14.3, wall=48]  2023-11-14 13:24:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 39.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 55.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 67.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:24:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.618 | nll_loss 0.005 | accuracy 81.5 | wps 73109.3 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 81.5\n",
            "2023-11-14 13:24:57 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 40.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 54.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 59.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 65.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 65.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 45/250 [00:00<00:03, 66.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  21% 53/250 [00:00<00:02, 70.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  24% 61/250 [00:00<00:02, 68.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  27% 68/250 [00:01<00:02, 68.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 75/250 [00:01<00:02, 68.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  33% 82/250 [00:01<00:02, 67.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 68.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 64.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 62.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 61.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 117/250 [00:01<00:02, 57.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  49% 123/250 [00:01<00:02, 58.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 58.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 135/250 [00:02<00:02, 55.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 55.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 56.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 56.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 56.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 165/250 [00:02<00:01, 56.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  68% 171/250 [00:02<00:01, 57.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  71% 178/250 [00:02<00:01, 58.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 59.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 58.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  79% 197/250 [00:03<00:00, 58.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 58.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 209/250 [00:03<00:00, 58.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 58.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 221/250 [00:03<00:00, 56.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  91% 227/250 [00:03<00:00, 57.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  94% 234/250 [00:03<00:00, 58.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 57.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 55.37it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:25:01 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.646 | nll_loss 0.005 | accuracy 79.6 | wps 63431.4 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:25:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:25:01 | INFO | train | epoch 003 | loss 0.646 | nll_loss 0.005 | accuracy 79.9 | wps 14202.9 | ups 13.31 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 7.181 | train_wall 11 | gb_free 14.2 | wall 53\n",
            "2023-11-14 13:25:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:25:01 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:25:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 21.95it/s, loss=0.689, nll_loss=0.005, accuracy=77.5, wps=24275.3, ups=21, wpb=1156, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=8.666, train_wall=0, gb_free=14.3, wall=65]2023-11-14 13:25:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 7/25 [00:00<00:00, 36.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 54.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 67.05it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:25:14 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.586 | nll_loss 0.004 | accuracy 83.5 | wps 73398.6 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 83.5\n",
            "2023-11-14 13:25:14 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 44.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:03, 58.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 62.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 66.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 64.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  19% 47/250 [00:00<00:03, 67.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 70.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 69.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 70.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 70.28it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 71.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 71.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 111/250 [00:01<00:01, 70.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  48% 119/250 [00:01<00:01, 71.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  51% 127/250 [00:01<00:01, 71.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 135/250 [00:01<00:01, 72.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 72.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 151/250 [00:02<00:01, 71.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 71.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  67% 167/250 [00:02<00:01, 71.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  70% 175/250 [00:02<00:01, 71.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 183/250 [00:02<00:00, 71.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 191/250 [00:02<00:00, 69.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  79% 198/250 [00:02<00:00, 69.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  82% 206/250 [00:02<00:00, 70.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 69.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 71.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 71.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 73.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 75.25it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:25:18 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.637 | nll_loss 0.005 | accuracy 80.1 | wps 74819.4 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:25:18 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:25:18 | INFO | train | epoch 004 | loss 0.591 | nll_loss 0.004 | accuracy 81.1 | wps 14516.5 | ups 13.6 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 7.802 | train_wall 11 | gb_free 14.3 | wall 69\n",
            "2023-11-14 13:25:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:25:18 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:25:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:12<00:00, 17.45it/s, loss=0.637, nll_loss=0.005, accuracy=77.5, wps=19551.1, ups=17.86, wpb=1094.4, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=8.2, train_wall=0, gb_free=14.3, wall=82]2023-11-14 13:25:30 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:25:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 23.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 36.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 38.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 50.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:25:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.587 | nll_loss 0.004 | accuracy 82 | wps 55052 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 83.5\n",
            "2023-11-14 13:25:31 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 41.47it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 53.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 62.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 66.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 68.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 70.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 70.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 71.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 72.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 73.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 72.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 72.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 111/250 [00:01<00:01, 71.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:01<00:01, 71.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  51% 127/250 [00:01<00:01, 73.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  54% 135/250 [00:01<00:01, 73.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 73.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 151/250 [00:02<00:01, 72.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 72.47it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 167/250 [00:02<00:01, 72.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 175/250 [00:02<00:01, 73.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 183/250 [00:02<00:00, 72.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 191/250 [00:02<00:00, 71.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 199/250 [00:02<00:00, 71.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  83% 207/250 [00:02<00:00, 70.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 70.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 70.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 71.72it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  96% 239/250 [00:03<00:00, 73.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  99% 247/250 [00:03<00:00, 75.30it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:25:35 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.632 | nll_loss 0.005 | accuracy 80.4 | wps 75258.1 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:25:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:25:35 | INFO | train | epoch 005 | loss 0.56 | nll_loss 0.004 | accuracy 83.6 | wps 14438 | ups 13.53 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 7.998 | train_wall 11 | gb_free 14.3 | wall 86\n",
            "2023-11-14 13:25:35 | INFO | fairseq_cli.train | done training in 85.5 seconds\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi1U19Uunnse"
      },
      "source": [
        "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
        "\n",
        "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSLWP6VUhUlC"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "TASK= \"sentence_prediction\" # sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr_random'\n",
        "DATA_PATH= \"/content/altegrad.lab3/data/cls.books-bin\" \n",
        "MODEL_PATH= '/content/' \n",
        "MAX_EPOCH= 5 \n",
        "MAX_SENTENCES= 8 # batch size\n",
        "MAX_UPDATE= int(MAX_EPOCH * train_length / MAX_SENTENCES) # n_epochs * n_train_examples / total batch size\n",
        "LR= 1e-5 \n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = \"accuracy\" # use the accuracy metric\n",
        "NUM_CLASSES= 2 #number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = int(.06 * MAX_UPDATE) # warmup ratio=6% of the whole training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtsCysc4hb42",
        "outputId": "91d285c9-10b3-4b86-ba77-14eb56629b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-14 13:25:38.642633: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:25:38.642685: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:25:38.642719: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:25:38.650352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:25:39.782071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:25:40 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:25:45 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', 'restore_file': '/content/', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/0', restore_file='/content/', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:25:45 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:25:45 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:25:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:25:46 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:25:46 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:25:46 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:25:46 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:25:46 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:25:46 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:25:49 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:25:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:25:49 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:25:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:25:49 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:25:49 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:25:49 | INFO | fairseq.trainer | Preparing to load checkpoint /content/\n",
            "2023-11-14 13:25:49 | INFO | fairseq.trainer | No existing checkpoint found /content/\n",
            "2023-11-14 13:25:49 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:25:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:25:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:25:49 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:25:49 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:25:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:25:50 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:25:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 224/225 [00:12<00:00, 18.48it/s, loss=0.944, nll_loss=0.007, accuracy=75, wps=20403.3, ups=19.02, wpb=1072.6, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=4.43, train_wall=0, gb_free=14.2, wall=13]2023-11-14 13:26:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 4/25 [00:00<00:01, 20.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 39.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 42.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 47.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:26:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.001 | nll_loss 0.008 | accuracy 50 | wps 52549.6 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:26:03 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:32,  7.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 41.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 18/250 [00:00<00:04, 47.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 51.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 53.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 36/250 [00:00<00:03, 54.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  17% 42/250 [00:00<00:03, 55.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 48/250 [00:00<00:03, 56.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 54.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 55.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 56.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 56.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 56.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 84/250 [00:01<00:02, 56.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 90/250 [00:01<00:02, 55.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 57.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 58.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  44% 111/250 [00:02<00:02, 58.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  47% 117/250 [00:02<00:02, 57.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  49% 123/250 [00:02<00:02, 57.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 56.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  54% 135/250 [00:02<00:02, 57.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 55.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 54.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 54.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 53.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 165/250 [00:03<00:01, 53.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  68% 171/250 [00:03<00:01, 53.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  71% 177/250 [00:03<00:01, 53.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  73% 183/250 [00:03<00:01, 53.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 53.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  78% 195/250 [00:03<00:01, 53.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 54.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 53.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 54.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 54.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  90% 225/250 [00:04<00:00, 53.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 231/250 [00:04<00:00, 53.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 54.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  97% 243/250 [00:04<00:00, 50.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset: 100% 250/250 [00:04<00:00, 55.21it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:26:07 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.997 | nll_loss 0.008 | accuracy 50.1 | wps 57636.5 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:26:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:26:07 | INFO | train | epoch 001 | loss 1.003 | nll_loss 0.008 | accuracy 51.3 | wps 14486.1 | ups 13.57 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 4.629 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-14 13:26:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:26:07 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:26:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 224/225 [00:12<00:00, 18.32it/s, loss=1.06, nll_loss=0.007, accuracy=35, wps=20684, ups=17.5, wpb=1182.2, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=6.888, train_wall=0, gb_free=14.3, wall=30]2023-11-14 13:26:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 38.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 20/25 [00:00<00:00, 51.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:26:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.981 | nll_loss 0.007 | accuracy 59.5 | wps 60107.1 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 59.5\n",
            "2023-11-14 13:26:20 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:35,  7.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 6/250 [00:00<00:08, 28.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 40.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 18/250 [00:00<00:04, 46.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 49.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 52.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 36/250 [00:00<00:04, 52.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  17% 42/250 [00:00<00:03, 52.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 48/250 [00:01<00:03, 52.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 50.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 49.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 47.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 47.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  30% 75/250 [00:01<00:03, 46.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 46.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  34% 85/250 [00:01<00:03, 46.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 90/250 [00:01<00:03, 46.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 95/250 [00:02<00:03, 47.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 100/250 [00:02<00:03, 47.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 49.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  44% 111/250 [00:02<00:02, 49.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  47% 117/250 [00:02<00:02, 50.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  49% 123/250 [00:02<00:02, 50.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 47.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  54% 134/250 [00:02<00:02, 46.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  56% 139/250 [00:02<00:02, 46.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 144/250 [00:03<00:02, 47.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 149/250 [00:03<00:02, 47.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 50.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 51.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 52.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 52.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 52.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 49.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 48.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  78% 196/250 [00:04<00:01, 49.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  80% 201/250 [00:04<00:00, 49.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  82% 206/250 [00:04<00:00, 49.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 211/250 [00:04<00:00, 48.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 216/250 [00:04<00:00, 46.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 46.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 49.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 50.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 50.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 245/250 [00:05<00:00, 50.94it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:26:25 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.969 | nll_loss 0.007 | accuracy 62.3 | wps 52204.9 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:26:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:26:25 | INFO | train | epoch 002 | loss 0.99 | nll_loss 0.007 | accuracy 54.6 | wps 13154.2 | ups 12.33 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 4.567 | train_wall 11 | gb_free 14.3 | wall 36\n",
            "2023-11-14 13:26:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:26:25 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:26:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 224/225 [00:12<00:00, 20.34it/s, loss=0.964, nll_loss=0.007, accuracy=65, wps=22574.8, ups=20.59, wpb=1096.6, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=2.97, train_wall=0, gb_free=14.3, wall=49]2023-11-14 13:26:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 3/25 [00:00<00:00, 26.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  40% 10/25 [00:00<00:00, 48.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 18/25 [00:00<00:00, 61.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:26:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.964 | nll_loss 0.007 | accuracy 61 | wps 76341.3 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 61\n",
            "2023-11-14 13:26:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 42.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 63.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 68.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 40/250 [00:00<00:02, 70.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 69.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 71.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 71.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 71.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 70.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 70.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 70.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 111/250 [00:01<00:01, 70.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  48% 119/250 [00:01<00:01, 71.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  51% 127/250 [00:01<00:01, 70.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 135/250 [00:01<00:01, 72.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 72.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  60% 151/250 [00:02<00:01, 72.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 73.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  67% 167/250 [00:02<00:01, 71.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  70% 175/250 [00:02<00:01, 72.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  73% 183/250 [00:02<00:00, 71.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 191/250 [00:02<00:00, 71.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 199/250 [00:02<00:00, 72.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  83% 207/250 [00:02<00:00, 71.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 70.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 71.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 70.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  96% 239/250 [00:03<00:00, 70.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  99% 247/250 [00:03<00:00, 72.67it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:26:42 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.945 | nll_loss 0.007 | accuracy 66.8 | wps 74801.2 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:26:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:26:42 | INFO | train | epoch 003 | loss 0.959 | nll_loss 0.007 | accuracy 60.9 | wps 14282.3 | ups 13.38 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 4.601 | train_wall 11 | gb_free 14.3 | wall 53\n",
            "2023-11-14 13:26:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:26:42 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:26:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 17.98it/s, loss=0.89, nll_loss=0.006, accuracy=67.5, wps=19587.5, ups=16.92, wpb=1157.6, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=5.728, train_wall=0, gb_free=14.3, wall=66]2023-11-14 13:26:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 36.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 49.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 63.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:26:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.949 | nll_loss 0.007 | accuracy 62 | wps 74101.2 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 62\n",
            "2023-11-14 13:26:55 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 42.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 65.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 70.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 72.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 72.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 72.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 73.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 72.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 70.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 71.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 72.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  42% 105/250 [00:01<00:01, 73.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 72.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 72.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 73.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  55% 137/250 [00:01<00:01, 73.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 73.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 73.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 72.42it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 72.42it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 177/250 [00:02<00:01, 72.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  74% 185/250 [00:02<00:00, 71.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  77% 193/250 [00:02<00:00, 71.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  80% 201/250 [00:02<00:00, 71.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  84% 209/250 [00:02<00:00, 72.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  87% 217/250 [00:03<00:00, 72.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 225/250 [00:03<00:00, 73.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  93% 233/250 [00:03<00:00, 70.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  96% 241/250 [00:03<00:00, 71.81it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset: 100% 250/250 [00:03<00:00, 75.03it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:26:59 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.921 | nll_loss 0.007 | accuracy 65.4 | wps 75984.7 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:26:59 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:26:59 | INFO | train | epoch 004 | loss 0.926 | nll_loss 0.007 | accuracy 65.2 | wps 14354.8 | ups 13.45 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 4.719 | train_wall 11 | gb_free 14.3 | wall 70\n",
            "2023-11-14 13:26:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:26:59 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:26:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:11<00:00, 18.08it/s, loss=0.902, nll_loss=0.006, accuracy=67.5, wps=19706.4, ups=17.64, wpb=1117, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=5.875, train_wall=0, gb_free=14.3, wall=82]2023-11-14 13:27:11 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:27:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 38.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 20/25 [00:00<00:00, 50.86it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:27:11 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.94 | nll_loss 0.007 | accuracy 61.5 | wps 60713.2 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 62\n",
            "2023-11-14 13:27:11 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:26,  9.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   4% 10/250 [00:00<00:07, 33.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 40.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 45.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 49.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  14% 35/250 [00:00<00:03, 54.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 54.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 47/250 [00:00<00:03, 55.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 55.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 55.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 55.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 56.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 55.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 54.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 54.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 53.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 101/250 [00:01<00:02, 53.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 53.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 54.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 53.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 52.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 52.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 53.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 143/250 [00:02<00:02, 52.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 53.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 54.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 53.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 53.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 53.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 52.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 52.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 53.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 55.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 56.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  84% 210/250 [00:04<00:00, 55.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 216/250 [00:04<00:00, 56.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 223/250 [00:04<00:00, 57.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 229/250 [00:04<00:00, 57.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 62.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 66.43it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:27:16 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.911 | nll_loss 0.007 | accuracy 67.5 | wps 57737.6 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:27:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:27:16 | INFO | train | epoch 005 | loss 0.898 | nll_loss 0.007 | accuracy 68.6 | wps 14030 | ups 13.15 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.724 | train_wall 11 | gb_free 14.3 | wall 87\n",
            "2023-11-14 13:27:16 | INFO | fairseq_cli.train | done training in 86.6 seconds\n",
            "2023-11-14 13:27:20.069579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:27:20.069640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:27:20.069673: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:27:20.077223: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:27:21.204550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:27:22 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', 'restore_file': '/content/', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/1', restore_file='/content/', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:27:25 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:27:25 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:27:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:27:25 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:27:25 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:27:25 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:27:25 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:27:25 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:27:25 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:27:29 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:27:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:27:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:27:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:27:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:27:29 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:27:29 | INFO | fairseq.trainer | Preparing to load checkpoint /content/\n",
            "2023-11-14 13:27:29 | INFO | fairseq.trainer | No existing checkpoint found /content/\n",
            "2023-11-14 13:27:29 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:27:29 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:27:29 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:27:29 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:27:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:27:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:27:29 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:27:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:12<00:00, 20.55it/s, loss=0.997, nll_loss=0.008, accuracy=47.5, wps=22522.7, ups=21.72, wpb=1037, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=4.605, train_wall=0, gb_free=14.3, wall=13]2023-11-14 13:27:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  9.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 7/25 [00:00<00:00, 36.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 54.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 66.02it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:27:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.994 | nll_loss 0.008 | accuracy 51.5 | wps 74300.6 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:27:42 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 17.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   3% 7/250 [00:00<00:06, 35.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 55.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 62.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 67.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 68.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  18% 46/250 [00:00<00:02, 69.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 54/250 [00:00<00:02, 71.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  25% 62/250 [00:00<00:02, 72.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 73.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 78/250 [00:01<00:02, 73.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 72.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 73.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 73.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  44% 110/250 [00:01<00:01, 72.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 72.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 73.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 73.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 72.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 73.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 74.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 75.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  70% 174/250 [00:02<00:01, 75.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  73% 183/250 [00:02<00:00, 76.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 191/250 [00:02<00:00, 75.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  80% 199/250 [00:02<00:00, 72.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  83% 207/250 [00:02<00:00, 72.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 72.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 72.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 71.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  96% 239/250 [00:03<00:00, 72.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  99% 247/250 [00:03<00:00, 73.96it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:27:46 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.989 | nll_loss 0.008 | accuracy 53.6 | wps 76565.1 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:27:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:27:46 | INFO | train | epoch 001 | loss 1.005 | nll_loss 0.008 | accuracy 50.2 | wps 15112.6 | ups 14.18 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 4.561 | train_wall 12 | gb_free 14.3 | wall 17\n",
            "2023-11-14 13:27:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:27:46 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:27:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 222/225 [00:12<00:00, 20.76it/s, loss=0.935, nll_loss=0.007, accuracy=67.5, wps=22396.4, ups=22.01, wpb=1017.4, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=5.179, train_wall=0, gb_free=14.2, wall=30]2023-11-14 13:27:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 40.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 56.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:27:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.979 | nll_loss 0.007 | accuracy 65 | wps 73835.4 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 65\n",
            "2023-11-14 13:27:59 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 45.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:04, 57.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 62.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 65.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 68.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 48/250 [00:00<00:02, 67.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  22% 56/250 [00:00<00:02, 69.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 64/250 [00:00<00:02, 71.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  29% 72/250 [00:01<00:02, 71.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 80/250 [00:01<00:02, 72.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  35% 88/250 [00:01<00:02, 72.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 73.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 104/250 [00:01<00:01, 73.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 112/250 [00:01<00:01, 73.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 120/250 [00:01<00:01, 72.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  51% 128/250 [00:01<00:01, 72.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  54% 136/250 [00:01<00:01, 69.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 70.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 70.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 72.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 168/250 [00:02<00:01, 73.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  70% 176/250 [00:02<00:00, 74.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 184/250 [00:02<00:00, 74.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  77% 192/250 [00:02<00:00, 74.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  80% 200/250 [00:02<00:00, 72.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  83% 208/250 [00:02<00:00, 73.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 216/250 [00:03<00:00, 71.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  90% 224/250 [00:03<00:00, 70.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  93% 232/250 [00:03<00:00, 71.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 240/250 [00:03<00:00, 70.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset: 100% 249/250 [00:03<00:00, 74.78it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:28:02 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.966 | nll_loss 0.007 | accuracy 66.7 | wps 75336.5 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:28:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:28:02 | INFO | train | epoch 002 | loss 0.982 | nll_loss 0.007 | accuracy 56.7 | wps 14510.9 | ups 13.6 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 4.587 | train_wall 11 | gb_free 14.3 | wall 34\n",
            "2023-11-14 13:28:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:28:02 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:28:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 223/225 [00:12<00:00, 17.65it/s, loss=0.911, nll_loss=0.007, accuracy=65, wps=18729, ups=16.82, wpb=1113.4, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=4.599, train_wall=0, gb_free=14.3, wall=46]2023-11-14 13:28:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 22.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 35.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 37.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 49.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:28:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.957 | nll_loss 0.007 | accuracy 64 | wps 53567.5 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 65\n",
            "2023-11-14 13:28:15 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:34,  7.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 35.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 40.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 44.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 47.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 50.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 51.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 46/250 [00:01<00:04, 50.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 51.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 52.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 52.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 52.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 53.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 52.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 88/250 [00:01<00:03, 52.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 53.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 102/250 [00:02<00:02, 59.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 109/250 [00:02<00:02, 62.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 117/250 [00:02<00:01, 66.68it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 124/250 [00:02<00:01, 66.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 131/250 [00:02<00:01, 66.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 139/250 [00:02<00:01, 68.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 69.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  62% 155/250 [00:02<00:01, 70.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  65% 163/250 [00:02<00:01, 73.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  68% 171/250 [00:02<00:01, 73.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 179/250 [00:03<00:00, 74.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  75% 187/250 [00:03<00:00, 73.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  78% 195/250 [00:03<00:00, 72.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 72.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 211/250 [00:03<00:00, 72.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 219/250 [00:03<00:00, 71.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  91% 227/250 [00:03<00:00, 71.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  94% 235/250 [00:03<00:00, 70.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  97% 243/250 [00:03<00:00, 72.05it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:28:19 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.936 | nll_loss 0.007 | accuracy 68.1 | wps 65964.8 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:28:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:28:19 | INFO | train | epoch 003 | loss 0.95 | nll_loss 0.007 | accuracy 63.1 | wps 14131.2 | ups 13.24 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 4.619 | train_wall 11 | gb_free 14.3 | wall 51\n",
            "2023-11-14 13:28:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:28:19 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:28:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:11<00:00, 19.13it/s, loss=0.838, nll_loss=0.007, accuracy=77.5, wps=19075, ups=19.05, wpb=1001, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=2.96, train_wall=0, gb_free=14.3, wall=62]2023-11-14 13:28:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  8.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 4/25 [00:00<00:01, 20.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 10/25 [00:00<00:00, 37.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 40.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 21/25 [00:00<00:00, 45.39it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:28:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.949 | nll_loss 0.007 | accuracy 60 | wps 51414.6 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 65\n",
            "2023-11-14 13:28:32 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   0% 1/250 [00:00<00:35,  7.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 43.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 45.43it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 48.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 50.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 51.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 52.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 53.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 54.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 53.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 57.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 56.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  33% 83/250 [00:01<00:02, 55.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 55.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 56.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  40% 101/250 [00:01<00:02, 56.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 54.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 54.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 54.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 53.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 54.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 54.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 54.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 54.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  62% 155/250 [00:02<00:01, 54.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  65% 162/250 [00:03<00:01, 56.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 169/250 [00:03<00:01, 59.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  70% 175/250 [00:03<00:01, 58.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  72% 181/250 [00:03<00:01, 58.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  75% 187/250 [00:03<00:01, 57.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  77% 193/250 [00:03<00:01, 56.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 55.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  82% 205/250 [00:03<00:00, 55.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 57.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 57.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 226/250 [00:04<00:00, 58.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  93% 232/250 [00:04<00:00, 59.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 60.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 58.53it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:28:36 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.918 | nll_loss 0.007 | accuracy 61.8 | wps 58381.4 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:28:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:28:36 | INFO | train | epoch 004 | loss 0.91 | nll_loss 0.007 | accuracy 66.9 | wps 14226 | ups 13.33 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 4.761 | train_wall 11 | gb_free 14.3 | wall 68\n",
            "2023-11-14 13:28:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:28:36 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:28:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 222/225 [00:11<00:00, 21.19it/s, loss=0.82, nll_loss=0.007, accuracy=72.5, wps=20369.3, ups=21.36, wpb=953.4, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=4.639, train_wall=0, gb_free=14.3, wall=79]2023-11-14 13:28:48 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:28:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 44.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 56.45it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:28:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.928 | nll_loss 0.007 | accuracy 63 | wps 73442.3 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 65\n",
            "2023-11-14 13:28:48 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:33,  7.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   2% 6/250 [00:00<00:08, 28.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 39.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   7% 18/250 [00:00<00:04, 46.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 50.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 52.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  14% 36/250 [00:00<00:03, 53.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  17% 43/250 [00:00<00:03, 56.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  20% 49/250 [00:01<00:03, 54.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 55/250 [00:01<00:03, 55.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 62/250 [00:01<00:03, 57.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 55.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 56.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  32% 80/250 [00:01<00:02, 57.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 57.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  37% 93/250 [00:01<00:02, 58.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 100/250 [00:01<00:02, 59.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  42% 106/250 [00:01<00:02, 58.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 57.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 58.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 58.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 57.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 137/250 [00:02<00:01, 57.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 57.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 58.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  62% 156/250 [00:02<00:01, 59.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  65% 162/250 [00:02<00:01, 58.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 168/250 [00:03<00:01, 58.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 175/250 [00:03<00:01, 59.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 182/250 [00:03<00:01, 59.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 59.71it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  78% 195/250 [00:03<00:00, 59.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 58.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 58.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 58.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 60.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 60.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  94% 234/250 [00:04<00:00, 57.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  96% 241/250 [00:04<00:00, 59.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset: 100% 249/250 [00:04<00:00, 62.70it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:28:53 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.896 | nll_loss 0.007 | accuracy 68.1 | wps 60878.3 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:28:53 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:28:53 | INFO | train | epoch 005 | loss 0.883 | nll_loss 0.007 | accuracy 69.2 | wps 14731.2 | ups 13.8 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.888 | train_wall 11 | gb_free 14.3 | wall 84\n",
            "2023-11-14 13:28:53 | INFO | fairseq_cli.train | done training in 83.6 seconds\n",
            "2023-11-14 13:28:58.189814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:28:58.189872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:28:58.189912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:28:58.200726: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-14 13:28:59.539773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-14 13:29:00 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-14 13:29:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 1125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', 'restore_file': '/content/', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=1125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu1125_lr1e-05_me5/2', restore_file='/content/', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/altegrad.lab3/data/cls.books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=67, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='1125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': '/content/altegrad.lab3/data/cls.books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 67, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-14 13:29:03 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-14 13:29:03 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-14 13:29:04 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-14 13:29:04 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/valid\n",
            "2023-11-14 13:29:04 | INFO | fairseq.data.data_utils | loaded 200 examples from: /content/altegrad.lab3/data/cls.books-bin/label/valid\n",
            "2023-11-14 13:29:04 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-14 13:29:04 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/test\n",
            "2023-11-14 13:29:04 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: /content/altegrad.lab3/data/cls.books-bin/label/test\n",
            "2023-11-14 13:29:04 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-14 13:29:06 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-14 13:29:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:29:06 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-14 13:29:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-14 13:29:06 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-14 13:29:06 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-14 13:29:06 | INFO | fairseq.trainer | Preparing to load checkpoint /content/\n",
            "2023-11-14 13:29:06 | INFO | fairseq.trainer | No existing checkpoint found /content/\n",
            "2023-11-14 13:29:06 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-14 13:29:06 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/input0/train\n",
            "2023-11-14 13:29:06 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: /content/altegrad.lab3/data/cls.books-bin/label/train\n",
            "2023-11-14 13:29:06 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-14 13:29:06 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-14 13:29:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:29:06 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-14 13:29:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:12<00:00, 20.29it/s, loss=0.989, nll_loss=0.007, accuracy=57.5, wps=21252.3, ups=18.89, wpb=1125.2, bsz=8, num_updates=220, lr=8.55388e-06, gnorm=5.523, train_wall=0, gb_free=14.2, wall=13]2023-11-14 13:29:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 56.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:29:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.997 | nll_loss 0.008 | accuracy 50 | wps 73535.6 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:29:20 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 41.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 57.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 60.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 65.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 68.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 69.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 71.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 68.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 63.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 77/250 [00:01<00:02, 60.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 84/250 [00:01<00:02, 59.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 91/250 [00:01<00:02, 62.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  40% 99/250 [00:01<00:02, 64.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 107/250 [00:01<00:02, 67.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 115/250 [00:01<00:01, 68.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  49% 123/250 [00:01<00:01, 70.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 131/250 [00:02<00:01, 72.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  56% 139/250 [00:02<00:01, 70.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 71.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  62% 155/250 [00:02<00:01, 71.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  65% 163/250 [00:02<00:01, 70.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  68% 171/250 [00:02<00:01, 71.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  72% 179/250 [00:02<00:00, 72.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  75% 187/250 [00:02<00:00, 73.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  78% 195/250 [00:02<00:00, 73.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 74.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  84% 211/250 [00:03<00:00, 72.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  88% 219/250 [00:03<00:00, 72.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  91% 227/250 [00:03<00:00, 71.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  94% 235/250 [00:03<00:00, 70.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 244/250 [00:03<00:00, 74.01it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:29:23 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.995 | nll_loss 0.008 | accuracy 50 | wps 73062.8 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-14 13:29:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-14 13:29:23 | INFO | train | epoch 001 | loss 1 | nll_loss 0.007 | accuracy 51.6 | wps 14682.5 | ups 13.73 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 8.50662e-06 | gnorm 4.604 | train_wall 12 | gb_free 14.3 | wall 17\n",
            "2023-11-14 13:29:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:29:23 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-14 13:29:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:11<00:00, 18.27it/s, loss=1.01, nll_loss=0.01, accuracy=50, wps=15663.1, ups=18.84, wpb=831.4, bsz=8, num_updates=445, lr=6.42722e-06, gnorm=4.107, train_wall=0, gb_free=14.3, wall=29]2023-11-14 13:29:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 37.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 40.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 49.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:29:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.982 | nll_loss 0.007 | accuracy 55.5 | wps 54556.2 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 55.5\n",
            "2023-11-14 13:29:36 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:34,  7.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 23.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 42.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 45.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 48.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 49.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 50.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 51.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 52.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 53.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 54.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 53.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 53.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 53.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  35% 88/250 [00:01<00:03, 51.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 52.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 100/250 [00:02<00:02, 51.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 52.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 52.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  47% 118/250 [00:02<00:02, 52.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 124/250 [00:02<00:02, 52.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 130/250 [00:02<00:02, 53.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 55.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 55.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 52.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 52.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 53.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 51.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 54.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 55.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 186/250 [00:03<00:01, 56.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  77% 192/250 [00:03<00:01, 55.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 55.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  82% 205/250 [00:03<00:00, 59.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  85% 213/250 [00:04<00:00, 62.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  88% 220/250 [00:04<00:00, 64.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 228/250 [00:04<00:00, 67.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 236/250 [00:04<00:00, 70.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 244/250 [00:04<00:00, 71.70it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:29:41 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.975 | nll_loss 0.007 | accuracy 56.4 | wps 58877 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-14 13:29:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-14 13:29:41 | INFO | train | epoch 002 | loss 0.988 | nll_loss 0.007 | accuracy 56.1 | wps 13962.3 | ups 13.08 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 6.37996e-06 | gnorm 4.592 | train_wall 11 | gb_free 14.3 | wall 35\n",
            "2023-11-14 13:29:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:29:41 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-14 13:29:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 224/225 [00:11<00:00, 18.40it/s, loss=0.958, nll_loss=0.006, accuracy=65, wps=22322.1, ups=18.88, wpb=1182, bsz=8, num_updates=670, lr=4.30057e-06, gnorm=6.269, train_wall=0, gb_free=14.3, wall=46]2023-11-14 13:29:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 28.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 40.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 18/25 [00:00<00:00, 46.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 53.25it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:29:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.958 | nll_loss 0.007 | accuracy 61.5 | wps 58300.2 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 61.5\n",
            "2023-11-14 13:29:53 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:27,  8.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 5/250 [00:00<00:09, 25.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 38.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   7% 18/250 [00:00<00:04, 47.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  10% 25/250 [00:00<00:04, 52.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 31/250 [00:00<00:04, 54.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  15% 37/250 [00:00<00:03, 55.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  17% 43/250 [00:00<00:03, 55.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  20% 50/250 [00:00<00:03, 57.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 56/250 [00:01<00:03, 57.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  25% 62/250 [00:01<00:03, 56.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 55.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 55.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 55.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 55.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 55.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 55.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 54.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 110/250 [00:02<00:02, 55.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  46% 116/250 [00:02<00:02, 55.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  49% 122/250 [00:02<00:02, 54.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 54.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 134/250 [00:02<00:02, 54.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 140/250 [00:02<00:01, 55.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  58% 146/250 [00:02<00:01, 54.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 54.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 55.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 164/250 [00:03<00:01, 54.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  68% 170/250 [00:03<00:01, 55.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  70% 176/250 [00:03<00:01, 56.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  73% 182/250 [00:03<00:01, 52.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  75% 188/250 [00:03<00:01, 54.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  78% 195/250 [00:03<00:00, 56.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 53.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 54.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 53.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 52.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  90% 225/250 [00:04<00:00, 52.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 231/250 [00:04<00:00, 52.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 52.47it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 58.54it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:29:57 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.944 | nll_loss 0.007 | accuracy 64.5 | wps 57832.9 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-14 13:29:57 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-14 13:29:57 | INFO | train | epoch 003 | loss 0.96 | nll_loss 0.007 | accuracy 60.7 | wps 14243.7 | ups 13.35 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 4.25331e-06 | gnorm 4.68 | train_wall 11 | gb_free 14.2 | wall 51\n",
            "2023-11-14 13:29:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:29:57 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-14 13:29:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 222/225 [00:11<00:00, 21.01it/s, loss=0.913, nll_loss=0.006, accuracy=70, wps=24318.7, ups=21.04, wpb=1156, bsz=8, num_updates=895, lr=2.17391e-06, gnorm=5.953, train_wall=0, gb_free=14.3, wall=63]   2023-11-14 13:30:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 40.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 55.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:30:10 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.937 | nll_loss 0.007 | accuracy 62.5 | wps 72732.7 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 62.5\n",
            "2023-11-14 13:30:10 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 47.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:03, 59.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 65.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 69.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 71.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 73.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 72.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 72.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 73.28it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 65.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  35% 88/250 [00:01<00:02, 62.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 60.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 60.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 109/250 [00:01<00:02, 59.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  46% 116/250 [00:01<00:02, 59.04it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  49% 122/250 [00:01<00:02, 57.42it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 56.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:02<00:02, 56.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  56% 140/250 [00:02<00:01, 57.04it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  58% 146/250 [00:02<00:01, 57.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 56.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 57.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 165/250 [00:02<00:01, 57.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 171/250 [00:02<00:01, 57.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 177/250 [00:02<00:01, 57.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 183/250 [00:03<00:01, 57.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 190/250 [00:03<00:01, 58.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  79% 197/250 [00:03<00:00, 60.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 59.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  84% 210/250 [00:03<00:00, 60.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  87% 217/250 [00:03<00:00, 58.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 58.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 59.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  94% 236/250 [00:03<00:00, 57.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  97% 242/250 [00:04<00:00, 53.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset: 100% 249/250 [00:04<00:00, 57.72it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:30:14 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.919 | nll_loss 0.007 | accuracy 68.2 | wps 63942.8 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-14 13:30:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-14 13:30:14 | INFO | train | epoch 004 | loss 0.923 | nll_loss 0.007 | accuracy 66.5 | wps 14576.5 | ups 13.66 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 2.12665e-06 | gnorm 4.724 | train_wall 11 | gb_free 14.3 | wall 68\n",
            "2023-11-14 13:30:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-14 13:30:14 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-14 13:30:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:13<00:00, 18.98it/s, loss=0.883, nll_loss=0.006, accuracy=75, wps=19564.8, ups=17.88, wpb=1094.4, bsz=8, num_updates=1120, lr=4.7259e-08, gnorm=4.339, train_wall=0, gb_free=14.3, wall=81]2023-11-14 13:30:28 | INFO | fairseq_cli.train | Stopping training due to num_updates: 1125 >= max_update: 1125\n",
            "2023-11-14 13:30:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 22.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 35.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 40.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 48.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-14 13:30:28 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.928 | nll_loss 0.007 | accuracy 63 | wps 53366.6 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 63\n",
            "2023-11-14 13:30:28 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:30,  8.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   2% 4/250 [00:00<00:12, 19.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   4% 10/250 [00:00<00:06, 35.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 15/250 [00:00<00:05, 39.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   8% 21/250 [00:00<00:05, 45.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  11% 27/250 [00:00<00:04, 48.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  13% 32/250 [00:00<00:04, 48.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  15% 38/250 [00:00<00:04, 50.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  18% 44/250 [00:01<00:04, 49.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  20% 50/250 [00:01<00:03, 52.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 56/250 [00:01<00:03, 54.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 62/250 [00:01<00:03, 53.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 53.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 55.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 54.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 55.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 54.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 54.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  42% 104/250 [00:02<00:02, 53.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 110/250 [00:02<00:02, 53.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  46% 116/250 [00:02<00:02, 53.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  49% 122/250 [00:02<00:02, 52.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 53.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  54% 136/250 [00:02<00:01, 59.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 63.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 66.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 68.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 68.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 175/250 [00:03<00:01, 71.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 183/250 [00:03<00:00, 73.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 191/250 [00:03<00:00, 71.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 71.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 72.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 73.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 72.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 72.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 73.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  99% 248/250 [00:04<00:00, 75.83it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-14 13:30:32 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.908 | nll_loss 0.007 | accuracy 67.4 | wps 64019.8 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-14 13:30:32 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-14 13:30:32 | INFO | train | epoch 005 | loss 0.897 | nll_loss 0.007 | accuracy 68.8 | wps 13038.3 | ups 12.22 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 0 | gnorm 4.818 | train_wall 12 | gb_free 14.3 | wall 86\n",
            "2023-11-14 13:30:32 | INFO | fairseq_cli.train | done training in 86.0 seconds\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $((6*$MAX_UPDATE/100)) \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHACXaPSLwu4"
      },
      "source": [
        "## <b>Tensorboard Visualisation </B>\n",
        "\n",
        "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwVvJNExS2dl"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tensorboard_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAR_P343MCKC"
      },
      "source": [
        "# <b>Part 2: Finetuning $RoBERTa_{small}^{fr}$ using HuggingFace's Transfromers</b>\n",
        "In this section of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning using their run_glue script)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3M090L45oPn"
      },
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
        "\n",
        "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:\n",
        "        #create train.json, valid.json and test.json\n",
        "        for i, review in enumerate(reviews):\n",
        "          f.write(json.dumps({'label': labels[i], 'text': review})+'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICnN2FvnhTbs"
      },
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c6vM239VsAT",
        "outputId": "7b4449ed-9eb6-4e4c-a525-fd8bc8179368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/261.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES= 8 # batch size.\n",
        "LR= 1e-5 #learning rate\n",
        "MAX_EPOCH= 5 \n",
        "NUM_CLASSES= 2 \n",
        "SEEDS= 3\n",
        "TASK= \"sentence_prediction\"\n",
        "CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg7wrUZvyuJu",
        "outputId": "cc5f9bdc-f8d9-488e-e39b-2bb474a559d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-14 13:38:05.880516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:38:05.880569: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:38:05.880610: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:38:10.275678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:38:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:38:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/valid,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=0,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:38:18 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:38:18 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/valid.json\n",
            "Using custom data configuration default-0b42c7f1e4c120e5\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Using custom data configuration default-0b42c7f1e4c120e5\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:38:19 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 12052.60it/s]\n",
            "Downloading took 0.0 min\n",
            "11/14/2023 13:38:19 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/14/2023 13:38:19 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 871.82it/s]\n",
            "Generating train split\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1800 examples [00:00, 34280.88 examples/s]\n",
            "Generating validation split\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 200 examples [00:00, 23826.53 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/14/2023 13:38:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/14/2023 13:38:19 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:38:19,224 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:38:19,231 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:38:19,231 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:38:19,231 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:38:19,233 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:38:19,234 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:38:19,234 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:38:19,235 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:38:19,235 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:38:19,235 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:38:19,235 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:38:19,236 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:38:19,398 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:38:19,399 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:38:19,577 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:38:20,051 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:38:20,051 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1800 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "11/14/2023 13:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "Running tokenizer on dataset: 100% 1800/1800 [00:01<00:00, 1391.79 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/200 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f54fedc95d60246e.arrow\n",
            "11/14/2023 13:38:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f54fedc95d60246e.arrow\n",
            "Running tokenizer on dataset: 100% 200/200 [00:00<00:00, 1075.42 examples/s]\n",
            "11/14/2023 13:38:21 - INFO - __main__ - Sample 1729 of the training set: {'label': 0, 'text': \"mais qu'est ce qui lui prend a tom Clancy ??? l'histoire ne décolle pas , c'est mou ,parfois invraisemblable !!!    bref relisez les premiers romans mais pas celui ci !!\\n\", 'input_ids': [0, 585, 813, 25, 449, 366, 476, 1240, 10039, 10, 2002, 28138, 1476, 11138, 95, 25, 14925, 107, 17844, 890, 402, 6, 4, 432, 25, 449, 13460, 6, 4, 1515, 1979, 159, 23, 310, 5092, 187, 3292, 1581, 3473, 29911, 9213, 7, 907, 191, 22841, 3016, 7, 585, 402, 8029, 791, 3088, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:38:21 - INFO - __main__ - Sample 788 of the training set: {'label': 1, 'text': \"pour débutant c'est super...  Ce n'est pas un livre où il n'y a que des recettes, il reprend des techniques de bases qui se revèlent indispensables pour progresser dans la gastronomie et en cuisine.\\n\", 'input_ids': [0, 482, 9683, 895, 432, 25, 449, 1092, 27, 1227, 536, 25, 449, 402, 51, 7451, 4693, 200, 536, 25, 53, 10, 41, 210, 18245, 7, 4, 200, 405, 12095, 210, 14276, 8, 13887, 476, 40, 11252, 1274, 7596, 18853, 7, 482, 12265, 56, 637, 21, 15496, 13, 81, 22, 15032, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:38:21 - INFO - __main__ - Sample 1552 of the training set: {'label': 0, 'text': 'C\\'est le deuxième Manchette que je lis (après \"Le petit bleu de la côte Ouest\"), et ça me passionne toujours aussi peu.  Il ne se passe pas grand-chose pendant les deux premiers tiers du bouquin. Vraiment pas mon style de polar.\\n', 'input_ids': [0, 294, 25, 449, 94, 20754, 1079, 26132, 41, 55, 363, 7, 15, 17979, 44, 2631, 4289, 24285, 8, 21, 2440, 67, 6, 31236, 17834, 81, 1837, 158, 10876, 85, 4618, 2613, 2998, 5, 688, 107, 40, 4512, 402, 4226, 9, 13477, 13, 8174, 191, 3548, 22841, 28742, 113, 7270, 9110, 5, 16184, 14, 550, 402, 1584, 7216, 8, 155, 300, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 11.1MB/s]\n",
            "[INFO|trainer.py:738] 2023-11-14 13:38:25,407 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:38:25,419 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:38:25,420 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:38:25,420 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:38:25,420 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:38:25,420 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:38:25,420 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:38:25,420 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:38:25,420 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.7002, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.7024, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.703, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.688, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6946, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.7109, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6787, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6822, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6971, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.6769, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.7025, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.6826, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.6932, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6848, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.703, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.6768, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6929, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6986, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6779, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.6877, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6807, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6726, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6739, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6681, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.66, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6899, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.681, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.6836, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6677, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6697, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.6612, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6785, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6602, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6642, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6423, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6584, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6438, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6653, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.605, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6116, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6108, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.6397, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.6024, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.6243, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.5833, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:17<00:46, 19.17it/s][INFO|trainer.py:738] 2023-11-14 13:38:42,638 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:38:42,641 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:38:42,641 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:38:42,641 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 24% 6/25 [00:00<00:00, 48.93it/s]\u001b[A\n",
            " 44% 11/25 [00:00<00:00, 40.67it/s]\u001b[A\n",
            " 68% 17/25 [00:00<00:00, 46.95it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.596209704875946, 'eval_accuracy': 0.68, 'eval_runtime': 0.5541, 'eval_samples_per_second': 360.953, 'eval_steps_per_second': 45.119, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:17<00:46, 19.17it/s]\n",
            "100% 25/25 [00:00<00:00, 51.90it/s]\u001b[A\n",
            "{'loss': 0.632, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.6084, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.5288, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.6147, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.5098, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.4782, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.5556, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.5805, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.5259, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.5236, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.5127, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.5292, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.487, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.4852, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.4446, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.5702, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4963, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.5078, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.5768, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.6314, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.4712, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.4752, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.5069, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.5125, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.5257, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.416, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5462, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5298, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4733, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.4696, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.507, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.4398, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.6384, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.3771, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5257, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.6624, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.4841, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.5201, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4598, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.5449, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.5274, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.6062, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.5979, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4002, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.5879, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:28<00:36, 18.72it/s][INFO|trainer.py:738] 2023-11-14 13:38:54,292 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:38:54,295 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:38:54,298 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:38:54,298 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 28% 7/25 [00:00<00:00, 61.58it/s]\u001b[A\n",
            " 56% 14/25 [00:00<00:00, 55.14it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5219318866729736, 'eval_accuracy': 0.73, 'eval_runtime': 0.5199, 'eval_samples_per_second': 384.682, 'eval_steps_per_second': 48.085, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:29<00:36, 18.72it/s]\n",
            "100% 25/25 [00:00<00:00, 50.74it/s]\u001b[A\n",
            "{'loss': 0.3542, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.3634, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.3561, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.4026, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.6188, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.38, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.543, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.4643, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.4573, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4011, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:31<00:30, 20.35it/s][INFO|trainer.py:2883] 2023-11-14 13:38:57,339 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:38:57,341 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:38:57,652 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:38:57,653 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:38:57,653 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.4469, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.4095, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.4758, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.351, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.4853, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.3531, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.388, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.4181, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.4095, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3328, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.5451, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.3659, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4349, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.45, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.5186, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.4076, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.4403, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.5356, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.3954, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.558, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.4238, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.5341, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.5208, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3428, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.5551, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.3432, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3731, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.5301, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.6222, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.6506, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.4102, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.3908, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.7901, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.4763, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.5566, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:41<00:20, 22.20it/s][INFO|trainer.py:738] 2023-11-14 13:39:06,886 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:39:06,888 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:39:06,888 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:39:06,888 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 36% 9/25 [00:00<00:00, 80.14it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.46668899059295654, 'eval_accuracy': 0.765, 'eval_runtime': 0.3569, 'eval_samples_per_second': 560.386, 'eval_steps_per_second': 70.048, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:41<00:20, 22.20it/s]\n",
            "100% 25/25 [00:00<00:00, 73.79it/s]\u001b[A\n",
            "{'loss': 0.5584, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.4758, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.4778, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.4229, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.4149, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.3565, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.6219, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.3357, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.3294, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.3972, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.3576, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.4262, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.42, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.4546, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.4305, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.412, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.4239, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.4378, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.365, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.3843, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.3821, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.4866, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.4786, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.3486, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.428, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.3504, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.4254, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.2886, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.3391, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.5505, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.2463, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.5733, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.386, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.6059, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3861, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.3318, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.4159, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.4084, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.3845, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.2833, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.336, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.4755, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.4557, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.5865, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.2881, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:52<00:10, 20.97it/s][INFO|trainer.py:738] 2023-11-14 13:39:17,510 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:39:17,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:39:17,513 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:39:17,513 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 70.67it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 65.07it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.42906737327575684, 'eval_accuracy': 0.785, 'eval_runtime': 0.4156, 'eval_samples_per_second': 481.25, 'eval_steps_per_second': 60.156, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:52<00:10, 20.97it/s]\n",
            "100% 25/25 [00:00<00:00, 63.48it/s]\u001b[A\n",
            "{'loss': 0.2524, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.4846, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.421, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.3718, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3434, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.3705, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.4089, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.5123, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.252, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.3413, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.4144, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.3821, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.188, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.2826, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.4179, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3056, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.381, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.3096, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.3425, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.4826, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [00:57<00:06, 20.71it/s][INFO|trainer.py:2883] 2023-11-14 13:39:22,776 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:39:22,777 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:39:23,055 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:39:23,056 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:39:23,057 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.4964, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.2669, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.358, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.3533, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.4453, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.3662, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3447, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.2948, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.5383, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.3438, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.339, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.4712, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.4532, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.4488, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.3726, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.4062, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.501, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.5274, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3005, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.4095, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.3528, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5405, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.4619, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.4474, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.4236, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:04<00:00, 21.72it/s][INFO|trainer.py:738] 2023-11-14 13:39:29,663 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:39:29,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:39:29,665 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:39:29,665 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 36% 9/25 [00:00<00:00, 82.49it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.42772021889686584, 'eval_accuracy': 0.79, 'eval_runtime': 0.3579, 'eval_samples_per_second': 558.753, 'eval_steps_per_second': 69.844, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:04<00:00, 21.72it/s]\n",
            "100% 25/25 [00:00<00:00, 74.47it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:39:30,024 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 64.6034, 'train_samples_per_second': 139.311, 'train_steps_per_second': 17.414, 'train_loss': 0.49150977887047664, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:04<00:00, 17.41it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:39:30,026 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:39:30,026 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:39:30,230 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:39:30,231 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:39:30,232 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4915\n",
            "  train_runtime            = 0:01:04.60\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    139.311\n",
            "  train_steps_per_second   =     17.414\n",
            "11/14/2023 13:39:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:39:30,255 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:39:30,258 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:39:30,258 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:39:30,258 >>   Batch size = 8\n",
            "100% 25/25 [00:00<00:00, 71.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =       0.79\n",
            "  eval_loss               =     0.4277\n",
            "  eval_runtime            = 0:00:00.37\n",
            "  eval_samples            =        200\n",
            "  eval_samples_per_second =    538.498\n",
            "  eval_steps_per_second   =     67.312\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:39:30,632 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.79}]}\n",
            "2023-11-14 13:39:35.316178: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:39:35.316239: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:39:35.316271: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:39:36.422577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:39:39 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:39:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/test,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=0,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:39:39 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:39:39 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/test.json\n",
            "Using custom data configuration default-432f54c1cb7c184e\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Using custom data configuration default-432f54c1cb7c184e\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:39:40 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 14004.35it/s]\n",
            "Downloading took 0.0 min\n",
            "11/14/2023 13:39:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "11/14/2023 13:39:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1454.84it/s]\n",
            "Generating train split\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1800 examples [00:00, 107771.93 examples/s]\n",
            "Generating validation split\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 2000 examples [00:00, 153705.07 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "11/14/2023 13:39:40 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/14/2023 13:39:40 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:39:40,098 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:39:40,108 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:39:40,108 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:39:40,108 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:39:40,109 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:39:40,113 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:39:40,113 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:39:40,113 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:39:40,113 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:39:40,113 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:39:40,113 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:39:40,114 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:39:40,269 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:39:40,270 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:39:40,502 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:39:40,950 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:39:40,950 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/1800 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "11/14/2023 13:39:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "Running tokenizer on dataset: 100% 1800/1800 [00:01<00:00, 1321.47 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4c8e5f940881a864.arrow\n",
            "11/14/2023 13:39:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4c8e5f940881a864.arrow\n",
            "Running tokenizer on dataset: 100% 2000/2000 [00:01<00:00, 1474.87 examples/s]\n",
            "11/14/2023 13:39:43 - INFO - __main__ - Sample 1729 of the training set: {'label': 0, 'text': \"mais qu'est ce qui lui prend a tom Clancy ??? l'histoire ne décolle pas , c'est mou ,parfois invraisemblable !!!    bref relisez les premiers romans mais pas celui ci !!\\n\", 'input_ids': [0, 585, 813, 25, 449, 366, 476, 1240, 10039, 10, 2002, 28138, 1476, 11138, 95, 25, 14925, 107, 17844, 890, 402, 6, 4, 432, 25, 449, 13460, 6, 4, 1515, 1979, 159, 23, 310, 5092, 187, 3292, 1581, 3473, 29911, 9213, 7, 907, 191, 22841, 3016, 7, 585, 402, 8029, 791, 3088, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:39:43 - INFO - __main__ - Sample 788 of the training set: {'label': 1, 'text': \"pour débutant c'est super...  Ce n'est pas un livre où il n'y a que des recettes, il reprend des techniques de bases qui se revèlent indispensables pour progresser dans la gastronomie et en cuisine.\\n\", 'input_ids': [0, 482, 9683, 895, 432, 25, 449, 1092, 27, 1227, 536, 25, 449, 402, 51, 7451, 4693, 200, 536, 25, 53, 10, 41, 210, 18245, 7, 4, 200, 405, 12095, 210, 14276, 8, 13887, 476, 40, 11252, 1274, 7596, 18853, 7, 482, 12265, 56, 637, 21, 15496, 13, 81, 22, 15032, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:39:43 - INFO - __main__ - Sample 1552 of the training set: {'label': 0, 'text': 'C\\'est le deuxième Manchette que je lis (après \"Le petit bleu de la côte Ouest\"), et ça me passionne toujours aussi peu.  Il ne se passe pas grand-chose pendant les deux premiers tiers du bouquin. Vraiment pas mon style de polar.\\n', 'input_ids': [0, 294, 25, 449, 94, 20754, 1079, 26132, 41, 55, 363, 7, 15, 17979, 44, 2631, 4289, 24285, 8, 21, 2440, 67, 6, 31236, 17834, 81, 1837, 158, 10876, 85, 4618, 2613, 2998, 5, 688, 107, 40, 4512, 402, 4226, 9, 13477, 13, 8174, 191, 3548, 22841, 28742, 113, 7270, 9110, 5, 16184, 14, 550, 402, 1584, 7216, 8, 155, 300, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:738] 2023-11-14 13:39:47,124 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:39:47,132 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:39:47,132 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:39:47,132 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:39:47,132 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:39:47,132 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:39:47,132 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:39:47,132 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:39:47,134 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.7002, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.7024, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.703, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.688, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6946, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.7109, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6787, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6822, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6971, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.6769, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.7025, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.6826, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.6932, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6848, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.703, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.6768, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6929, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6986, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6779, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.6877, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6807, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6726, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6739, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6681, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.66, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6899, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.681, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.6836, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6677, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6697, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.6612, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6785, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6602, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6642, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6423, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6584, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6438, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6653, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.605, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6116, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6108, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.6397, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.6024, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.6243, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.5833, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:11<00:42, 21.32it/s][INFO|trainer.py:738] 2023-11-14 13:39:58,336 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:39:58,339 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:39:58,339 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:39:58,340 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 7/250 [00:00<00:03, 67.98it/s]\u001b[A\n",
            "  6% 14/250 [00:00<00:03, 64.33it/s]\u001b[A\n",
            "  8% 21/250 [00:00<00:03, 60.42it/s]\u001b[A\n",
            " 11% 28/250 [00:00<00:03, 60.46it/s]\u001b[A\n",
            " 14% 35/250 [00:00<00:03, 61.32it/s]\u001b[A\n",
            " 17% 42/250 [00:00<00:03, 61.07it/s]\u001b[A\n",
            " 20% 49/250 [00:00<00:03, 60.95it/s]\u001b[A\n",
            " 22% 56/250 [00:00<00:03, 61.02it/s]\u001b[A\n",
            " 25% 63/250 [00:01<00:03, 61.16it/s]\u001b[A\n",
            " 28% 70/250 [00:01<00:02, 60.25it/s]\u001b[A\n",
            " 31% 77/250 [00:01<00:02, 60.14it/s]\u001b[A\n",
            " 34% 84/250 [00:01<00:02, 60.29it/s]\u001b[A\n",
            " 36% 91/250 [00:01<00:02, 60.58it/s]\u001b[A\n",
            " 39% 98/250 [00:01<00:02, 60.70it/s]\u001b[A\n",
            " 42% 105/250 [00:01<00:02, 60.88it/s]\u001b[A\n",
            " 45% 112/250 [00:01<00:02, 61.21it/s]\u001b[A\n",
            " 48% 119/250 [00:01<00:02, 61.34it/s]\u001b[A\n",
            " 50% 126/250 [00:02<00:02, 61.54it/s]\u001b[A\n",
            " 53% 133/250 [00:02<00:01, 61.10it/s]\u001b[A\n",
            " 56% 140/250 [00:02<00:01, 59.88it/s]\u001b[A\n",
            " 59% 147/250 [00:02<00:01, 60.27it/s]\u001b[A\n",
            " 62% 154/250 [00:02<00:01, 60.63it/s]\u001b[A\n",
            " 64% 161/250 [00:02<00:01, 61.05it/s]\u001b[A\n",
            " 67% 168/250 [00:02<00:01, 61.05it/s]\u001b[A\n",
            " 70% 175/250 [00:02<00:01, 61.10it/s]\u001b[A\n",
            " 73% 182/250 [00:02<00:01, 60.59it/s]\u001b[A\n",
            " 76% 189/250 [00:03<00:01, 60.77it/s]\u001b[A\n",
            " 78% 196/250 [00:03<00:00, 60.68it/s]\u001b[A\n",
            " 81% 203/250 [00:03<00:00, 60.75it/s]\u001b[A\n",
            " 84% 210/250 [00:03<00:00, 61.34it/s]\u001b[A\n",
            " 87% 217/250 [00:03<00:00, 61.80it/s]\u001b[A\n",
            " 90% 224/250 [00:03<00:00, 61.87it/s]\u001b[A\n",
            " 92% 231/250 [00:03<00:00, 61.78it/s]\u001b[A\n",
            " 95% 238/250 [00:03<00:00, 62.14it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5726951956748962, 'eval_accuracy': 0.737, 'eval_runtime': 4.1427, 'eval_samples_per_second': 482.778, 'eval_steps_per_second': 60.347, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:15<00:42, 21.32it/s]\n",
            "100% 250/250 [00:04<00:00, 61.47it/s]\u001b[A\n",
            "{'loss': 0.632, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.6084, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.5288, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.6147, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.5098, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.4782, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.5556, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.5805, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.5259, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.5236, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.5127, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.5292, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.487, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.4852, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.4446, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.5702, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4963, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.5078, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.5768, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.6314, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.4712, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.4752, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.5069, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.5125, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.5257, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.416, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5462, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5298, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4733, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.4696, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.507, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.4398, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.6384, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.3771, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5257, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.6624, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.4841, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.5201, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4598, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.5449, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.5274, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.6062, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.5979, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4002, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.5879, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:26<00:31, 21.22it/s][INFO|trainer.py:738] 2023-11-14 13:40:13,444 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:40:13,446 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:40:13,446 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:40:13,446 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 9/250 [00:00<00:03, 78.57it/s]\u001b[A\n",
            "  7% 17/250 [00:00<00:03, 71.88it/s]\u001b[A\n",
            " 10% 25/250 [00:00<00:03, 70.87it/s]\u001b[A\n",
            " 13% 33/250 [00:00<00:03, 55.59it/s]\u001b[A\n",
            " 16% 39/250 [00:00<00:05, 39.70it/s]\u001b[A\n",
            " 18% 44/250 [00:00<00:05, 35.83it/s]\u001b[A\n",
            " 20% 49/250 [00:01<00:05, 38.38it/s]\u001b[A\n",
            " 22% 55/250 [00:01<00:04, 42.37it/s]\u001b[A\n",
            " 25% 62/250 [00:01<00:03, 48.91it/s]\u001b[A\n",
            " 28% 69/250 [00:01<00:03, 53.68it/s]\u001b[A\n",
            " 31% 77/250 [00:01<00:02, 58.45it/s]\u001b[A\n",
            " 34% 84/250 [00:01<00:03, 49.36it/s]\u001b[A\n",
            " 36% 90/250 [00:01<00:03, 40.86it/s]\u001b[A\n",
            " 38% 95/250 [00:02<00:04, 36.65it/s]\u001b[A\n",
            " 40% 101/250 [00:02<00:03, 40.26it/s]\u001b[A\n",
            " 43% 107/250 [00:02<00:03, 43.74it/s]\u001b[A\n",
            " 46% 114/250 [00:02<00:02, 49.42it/s]\u001b[A\n",
            " 48% 121/250 [00:02<00:02, 54.26it/s]\u001b[A\n",
            " 51% 128/250 [00:02<00:02, 58.13it/s]\u001b[A\n",
            " 54% 135/250 [00:02<00:01, 59.89it/s]\u001b[A\n",
            " 57% 142/250 [00:02<00:02, 51.07it/s]\u001b[A\n",
            " 59% 148/250 [00:03<00:02, 41.20it/s]\u001b[A\n",
            " 61% 153/250 [00:03<00:02, 37.44it/s]\u001b[A\n",
            " 63% 158/250 [00:03<00:02, 36.33it/s]\u001b[A\n",
            " 66% 166/250 [00:03<00:01, 44.44it/s]\u001b[A\n",
            " 69% 173/250 [00:03<00:01, 50.15it/s]\u001b[A\n",
            " 72% 180/250 [00:03<00:01, 54.36it/s]\u001b[A\n",
            " 75% 187/250 [00:03<00:01, 57.99it/s]\u001b[A\n",
            " 78% 194/250 [00:03<00:00, 60.96it/s]\u001b[A\n",
            " 80% 201/250 [00:04<00:00, 62.49it/s]\u001b[A\n",
            " 83% 208/250 [00:04<00:00, 63.68it/s]\u001b[A\n",
            " 86% 215/250 [00:04<00:00, 64.31it/s]\u001b[A\n",
            " 89% 222/250 [00:04<00:00, 65.35it/s]\u001b[A\n",
            " 92% 229/250 [00:04<00:00, 66.65it/s]\u001b[A\n",
            " 94% 236/250 [00:04<00:00, 66.46it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.49401581287384033, 'eval_accuracy': 0.771, 'eval_runtime': 4.841, 'eval_samples_per_second': 413.138, 'eval_steps_per_second': 51.642, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:31<00:31, 21.22it/s]\n",
            "100% 250/250 [00:04<00:00, 66.89it/s]\u001b[A\n",
            "{'loss': 0.3542, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.3634, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.3561, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.4026, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.6188, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.38, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.543, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.4643, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.4573, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4011, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:33<00:31, 19.60it/s][INFO|trainer.py:2883] 2023-11-14 13:40:20,754 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:40:20,756 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:40:21,112 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:40:21,114 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:40:21,115 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.4469, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.4095, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.4758, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.351, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.4853, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.3531, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.388, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.4181, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.4095, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3328, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.5451, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.3659, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4349, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.45, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.5186, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.4076, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.4403, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.5356, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.3954, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.558, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.4238, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.5341, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.5208, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3428, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.5551, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.3432, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3731, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.5301, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.6222, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.6506, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.4102, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.3908, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.7901, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.4763, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.5566, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:43<00:21, 20.55it/s][INFO|trainer.py:738] 2023-11-14 13:40:30,609 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:40:30,610 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:40:30,610 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:40:30,611 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 76.76it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 69.99it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 69.11it/s]\u001b[A\n",
            " 12% 31/250 [00:00<00:03, 68.84it/s]\u001b[A\n",
            " 15% 38/250 [00:00<00:03, 68.00it/s]\u001b[A\n",
            " 18% 45/250 [00:00<00:03, 68.21it/s]\u001b[A\n",
            " 21% 52/250 [00:00<00:02, 67.95it/s]\u001b[A\n",
            " 24% 59/250 [00:00<00:02, 67.29it/s]\u001b[A\n",
            " 26% 66/250 [00:00<00:02, 67.24it/s]\u001b[A\n",
            " 29% 73/250 [00:01<00:02, 67.07it/s]\u001b[A\n",
            " 32% 80/250 [00:01<00:02, 67.06it/s]\u001b[A\n",
            " 35% 87/250 [00:01<00:02, 67.14it/s]\u001b[A\n",
            " 38% 94/250 [00:01<00:02, 66.98it/s]\u001b[A\n",
            " 40% 101/250 [00:01<00:02, 65.82it/s]\u001b[A\n",
            " 43% 108/250 [00:01<00:02, 66.33it/s]\u001b[A\n",
            " 46% 115/250 [00:01<00:02, 66.25it/s]\u001b[A\n",
            " 49% 122/250 [00:01<00:01, 66.41it/s]\u001b[A\n",
            " 52% 129/250 [00:01<00:01, 66.93it/s]\u001b[A\n",
            " 54% 136/250 [00:02<00:01, 66.56it/s]\u001b[A\n",
            " 57% 143/250 [00:02<00:01, 66.80it/s]\u001b[A\n",
            " 60% 150/250 [00:02<00:01, 66.83it/s]\u001b[A\n",
            " 63% 157/250 [00:02<00:01, 66.90it/s]\u001b[A\n",
            " 66% 164/250 [00:02<00:01, 67.30it/s]\u001b[A\n",
            " 68% 171/250 [00:02<00:01, 67.73it/s]\u001b[A\n",
            " 71% 178/250 [00:02<00:01, 67.20it/s]\u001b[A\n",
            " 74% 185/250 [00:02<00:00, 67.90it/s]\u001b[A\n",
            " 77% 192/250 [00:02<00:00, 67.71it/s]\u001b[A\n",
            " 80% 199/250 [00:02<00:00, 67.30it/s]\u001b[A\n",
            " 82% 206/250 [00:03<00:00, 67.34it/s]\u001b[A\n",
            " 85% 213/250 [00:03<00:00, 66.96it/s]\u001b[A\n",
            " 88% 220/250 [00:03<00:00, 66.83it/s]\u001b[A\n",
            " 91% 227/250 [00:03<00:00, 67.14it/s]\u001b[A\n",
            " 94% 234/250 [00:03<00:00, 67.68it/s]\u001b[A\n",
            " 96% 241/250 [00:03<00:00, 67.43it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.465406596660614, 'eval_accuracy': 0.789, 'eval_runtime': 3.7395, 'eval_samples_per_second': 534.825, 'eval_steps_per_second': 66.853, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:47<00:21, 20.55it/s]\n",
            "100% 250/250 [00:03<00:00, 67.55it/s]\u001b[A\n",
            "{'loss': 0.5584, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.4758, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.4778, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.4229, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.4149, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.3565, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.6219, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.3357, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.3294, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.3972, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.3576, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.4262, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.42, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.4546, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.4305, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.412, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.4239, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.4378, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.365, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.3843, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.3821, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.4866, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.4786, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.3486, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.428, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.3504, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.4254, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.2886, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.3391, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.5505, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.2463, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.5733, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.386, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.6059, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3861, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.3318, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.4159, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.4084, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.3845, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.2833, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.336, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.4755, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.4557, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.5865, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.2881, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:58<00:11, 20.24it/s][INFO|trainer.py:738] 2023-11-14 13:40:45,247 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:40:45,250 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:40:45,250 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:40:45,250 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 71.02it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 64.79it/s]\u001b[A\n",
            "  9% 23/250 [00:00<00:03, 62.77it/s]\u001b[A\n",
            " 12% 30/250 [00:00<00:03, 61.90it/s]\u001b[A\n",
            " 15% 37/250 [00:00<00:03, 60.66it/s]\u001b[A\n",
            " 18% 44/250 [00:00<00:03, 61.06it/s]\u001b[A\n",
            " 20% 51/250 [00:00<00:03, 61.06it/s]\u001b[A\n",
            " 23% 58/250 [00:00<00:03, 59.86it/s]\u001b[A\n",
            " 26% 65/250 [00:01<00:03, 60.07it/s]\u001b[A\n",
            " 29% 72/250 [00:01<00:02, 60.37it/s]\u001b[A\n",
            " 32% 79/250 [00:01<00:02, 59.90it/s]\u001b[A\n",
            " 34% 86/250 [00:01<00:02, 60.16it/s]\u001b[A\n",
            " 37% 93/250 [00:01<00:02, 60.61it/s]\u001b[A\n",
            " 40% 100/250 [00:01<00:02, 60.69it/s]\u001b[A\n",
            " 43% 107/250 [00:01<00:02, 60.81it/s]\u001b[A\n",
            " 46% 114/250 [00:01<00:02, 60.71it/s]\u001b[A\n",
            " 48% 121/250 [00:01<00:02, 59.79it/s]\u001b[A\n",
            " 51% 128/250 [00:02<00:02, 60.59it/s]\u001b[A\n",
            " 54% 135/250 [00:02<00:01, 60.84it/s]\u001b[A\n",
            " 57% 142/250 [00:02<00:01, 60.98it/s]\u001b[A\n",
            " 60% 149/250 [00:02<00:01, 61.08it/s]\u001b[A\n",
            " 62% 156/250 [00:02<00:01, 61.13it/s]\u001b[A\n",
            " 65% 163/250 [00:02<00:01, 61.27it/s]\u001b[A\n",
            " 68% 170/250 [00:02<00:01, 61.30it/s]\u001b[A\n",
            " 71% 177/250 [00:02<00:01, 59.94it/s]\u001b[A\n",
            " 74% 184/250 [00:03<00:01, 60.05it/s]\u001b[A\n",
            " 76% 191/250 [00:03<00:00, 59.87it/s]\u001b[A\n",
            " 79% 197/250 [00:03<00:00, 59.20it/s]\u001b[A\n",
            " 82% 204/250 [00:03<00:00, 59.74it/s]\u001b[A\n",
            " 84% 210/250 [00:03<00:00, 59.48it/s]\u001b[A\n",
            " 87% 217/250 [00:03<00:00, 59.88it/s]\u001b[A\n",
            " 90% 224/250 [00:03<00:00, 60.30it/s]\u001b[A\n",
            " 92% 231/250 [00:03<00:00, 59.90it/s]\u001b[A\n",
            " 95% 238/250 [00:03<00:00, 60.15it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4541153907775879, 'eval_accuracy': 0.797, 'eval_runtime': 4.1653, 'eval_samples_per_second': 480.157, 'eval_steps_per_second': 60.02, 'epoch': 4.0}\n",
            " 80% 900/1125 [01:02<00:11, 20.24it/s]\n",
            "100% 250/250 [00:04<00:00, 60.02it/s]\u001b[A\n",
            "{'loss': 0.2524, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.4846, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.421, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.3718, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3434, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.3705, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.4089, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.5123, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.252, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.3413, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.4144, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.3821, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.188, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.2826, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.4179, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3056, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.381, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.3096, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.3425, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.4826, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [01:07<00:05, 21.51it/s][INFO|trainer.py:2883] 2023-11-14 13:40:54,211 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:40:54,212 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:40:54,484 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:40:54,485 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:40:54,485 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.4964, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.2669, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.358, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.3533, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.4453, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.3662, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3447, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.2948, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.5383, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.3438, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.339, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.4712, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.4532, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.4488, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.3726, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.4062, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.501, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.5274, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3005, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.4095, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.3528, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5405, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.4619, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.4474, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.4236, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:13<00:00, 21.54it/s][INFO|trainer.py:738] 2023-11-14 13:41:00,786 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:41:00,788 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:41:00,788 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:41:00,788 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 9/250 [00:00<00:03, 79.53it/s]\u001b[A\n",
            "  7% 17/250 [00:00<00:03, 74.07it/s]\u001b[A\n",
            " 10% 25/250 [00:00<00:03, 70.96it/s]\u001b[A\n",
            " 13% 33/250 [00:00<00:03, 67.11it/s]\u001b[A\n",
            " 16% 40/250 [00:00<00:03, 65.31it/s]\u001b[A\n",
            " 19% 47/250 [00:00<00:03, 62.30it/s]\u001b[A\n",
            " 22% 54/250 [00:00<00:03, 61.99it/s]\u001b[A\n",
            " 24% 61/250 [00:00<00:03, 61.90it/s]\u001b[A\n",
            " 27% 68/250 [00:01<00:02, 61.96it/s]\u001b[A\n",
            " 30% 75/250 [00:01<00:02, 61.93it/s]\u001b[A\n",
            " 33% 82/250 [00:01<00:02, 62.01it/s]\u001b[A\n",
            " 36% 89/250 [00:01<00:02, 62.05it/s]\u001b[A\n",
            " 38% 96/250 [00:01<00:02, 61.53it/s]\u001b[A\n",
            " 41% 103/250 [00:01<00:02, 61.47it/s]\u001b[A\n",
            " 44% 110/250 [00:01<00:02, 61.49it/s]\u001b[A\n",
            " 47% 117/250 [00:01<00:02, 61.32it/s]\u001b[A\n",
            " 50% 124/250 [00:01<00:02, 61.48it/s]\u001b[A\n",
            " 52% 131/250 [00:02<00:01, 61.55it/s]\u001b[A\n",
            " 55% 138/250 [00:02<00:01, 61.40it/s]\u001b[A\n",
            " 58% 145/250 [00:02<00:01, 61.91it/s]\u001b[A\n",
            " 61% 152/250 [00:02<00:01, 62.13it/s]\u001b[A\n",
            " 64% 159/250 [00:02<00:01, 62.29it/s]\u001b[A\n",
            " 66% 166/250 [00:02<00:01, 62.07it/s]\u001b[A\n",
            " 69% 173/250 [00:02<00:01, 61.91it/s]\u001b[A\n",
            " 72% 180/250 [00:02<00:01, 60.32it/s]\u001b[A\n",
            " 75% 187/250 [00:02<00:01, 60.54it/s]\u001b[A\n",
            " 78% 194/250 [00:03<00:00, 60.69it/s]\u001b[A\n",
            " 80% 201/250 [00:03<00:00, 61.10it/s]\u001b[A\n",
            " 83% 208/250 [00:03<00:00, 61.28it/s]\u001b[A\n",
            " 86% 215/250 [00:03<00:00, 61.29it/s]\u001b[A\n",
            " 89% 222/250 [00:03<00:00, 61.39it/s]\u001b[A\n",
            " 92% 229/250 [00:03<00:00, 61.08it/s]\u001b[A\n",
            " 94% 236/250 [00:03<00:00, 61.36it/s]\u001b[A\n",
            " 97% 243/250 [00:03<00:00, 60.29it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.4587746858596802, 'eval_accuracy': 0.8005, 'eval_runtime': 4.0564, 'eval_samples_per_second': 493.049, 'eval_steps_per_second': 61.631, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:17<00:00, 21.54it/s]\n",
            "100% 250/250 [00:04<00:00, 61.60it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:41:04,846 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 77.7119, 'train_samples_per_second': 115.812, 'train_steps_per_second': 14.477, 'train_loss': 0.49150977887047664, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:17<00:00, 14.48it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:41:04,848 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:41:04,850 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:41:05,172 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:41:05,173 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:41:05,174 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4915\n",
            "  train_runtime            = 0:01:17.71\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    115.812\n",
            "  train_steps_per_second   =     14.477\n",
            "11/14/2023 13:41:05 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:41:05,209 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:41:05,211 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:41:05,211 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:41:05,211 >>   Batch size = 8\n",
            "100% 250/250 [00:04<00:00, 60.23it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.8005\n",
            "  eval_loss               =     0.4588\n",
            "  eval_runtime            = 0:00:04.17\n",
            "  eval_samples            =       2000\n",
            "  eval_samples_per_second =    479.297\n",
            "  eval_steps_per_second   =     59.912\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:41:09,387 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8005}]}\n",
            "2023-11-14 13:41:14.732399: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:41:14.732449: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:41:14.732481: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:41:15.853413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:41:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:41:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/valid,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=1,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:41:18 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:41:18 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/valid.json\n",
            "Using custom data configuration default-0b42c7f1e4c120e5\n",
            "11/14/2023 13:41:18 - INFO - datasets.builder - Using custom data configuration default-0b42c7f1e4c120e5\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:41:18 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 13:41:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:41:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:41:18 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:41:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:41:18,466 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:41:18,470 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:41:18,470 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:41:18,470 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:41:18,471 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:41:18,472 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:41:18,472 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:41:18,472 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:41:18,472 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:41:18,472 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:41:18,472 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:41:18,473 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:41:18,568 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:41:18,569 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:41:18,666 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:41:19,397 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:41:19,397 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "11/14/2023 13:41:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "Running tokenizer on dataset:   0% 0/200 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f796adcf61d1b8b5.arrow\n",
            "11/14/2023 13:41:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f796adcf61d1b8b5.arrow\n",
            "Running tokenizer on dataset: 100% 200/200 [00:00<00:00, 2049.17 examples/s]\n",
            "11/14/2023 13:41:19 - INFO - __main__ - Sample 275 of the training set: {'label': 1, 'text': 'On croit bien évidemment tout savoir sur l\\'horreur indicible des camps de concentration. Et bien non, \"si c\\'est un homme\" laisse littéralement sans voix ! Tout commentaire paraît déplacé et pour tout dire insignifiant. Lisez, vous comprendrez.  Mon seul commentaire est donc qu\\'il faut impérativement recommander la lecture de cet ouvrage à tous et particulièrement aux grands adolescents.\\n', 'input_ids': [0, 1360, 5181, 204, 1202, 27809, 1530, 6958, 511, 95, 25, 2893, 106, 415, 26070, 1581, 210, 4689, 7, 8, 30525, 5, 1183, 1202, 324, 4, 44, 167, 432, 25, 449, 51, 13971, 58, 17087, 2845, 14397, 7041, 2814, 22356, 573, 9757, 27288, 119, 8412, 16884, 138, 4747, 81, 482, 1530, 2664, 30128, 25306, 5, 21780, 164, 4, 773, 16504, 164, 5, 3424, 10724, 27288, 390, 3882, 813, 25, 346, 4219, 7578, 2446, 12754, 550, 29040, 42, 21, 15429, 8, 6572, 6, 25818, 239, 2862, 81, 6, 20707, 1723, 17041, 29943, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:41:19 - INFO - __main__ - Sample 1165 of the training set: {'label': 0, 'text': \"L'intrigue est faible, mais l'epopee dans la jungle est divertissante, un peu longue peut-etre; la fin est sans surprise.\\n\", 'input_ids': [0, 313, 25, 31850, 390, 22315, 4, 585, 95, 25, 13, 10337, 3264, 637, 21, 16391, 130, 390, 22248, 16220, 13, 4, 51, 2998, 18512, 2293, 9, 123, 106, 73, 21, 1408, 390, 2814, 14727, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:41:19 - INFO - __main__ - Sample 1735 of the training set: {'label': 0, 'text': \"Rien ne se tient et rien ne va dans ce Jésus parlait Araméen. Pourtant, l'idée à l'origine du livre était bonne : Jésus, lorsqu'il enseignait, parlait un dialecte hébraïque, l'Araméen palestinien. Or, son enseignement nous est parvenu dans les traductions grecques des textes du Nouveau Testament, les originaux hébreux ayant été égarés. De fait, l'annonce par l'auteur de la découverte d'un Evangile écrit en Araméen autorisait une relecture des textes grecs - car on sait qu'une traduction transforme le sens d'un texte et l'altère. Revenir donc aux sources, en comparant Grec et Araméen, aurait pu permettre de relire les paroles de Jésus en collant encore plus à ce qu'il avait réellement voulu dire.  Mais, hélas, Eric Edelmann, l'auteur du livre, n'a en fait rien fait de l'idée de départ de son propre ouvrage ! Au lieu de retraduire ou repenser les Evangiles pour en donner un sens plus précis, il prend une option philosophiquement acceptable, mais indéfendable d'un point de vue chrétien, le gnosticisme - position qu'il récuse, persuadé de la différence entre la gnose (connaissance) et gnosticisme (religion basée sur la connaissance). Selon Edelmann, en effet, Jésus a donné un enseignement permettant à tout disciple le suivant de réaliser sa transformation intérieure, de prendre la voie de l'élévation spirituelle. Or, l'erreur majeure de l'auteur aura été de considérer que cette transformation est indépendante du caeur du message du Christ. Ainsi, Edelmann fait fi du plus grand enseignement de la Thora revendiqué par Jésus : aimer le seul et unique Dieu, et aimer son prochain comme sois même. Car l'amour, cet amour totale et entier à la base du Christianisme reste la condition sine qua non pour que tout un chacun, nous nous retrouvions réellement transformés. Sans compter qu'au-delà même de cet amour humain, il y a l'Amour de Dieu : Jésus, en ressuscitant, trompe la mort et donne à chacun la garantie de l'immortalité de l'âme - d'où le sacrifice du Christ, sacrifice d'Amour pur. Il refuse l'idée de résurrection du Christ, préférant réduire son triomphe sur la mort à une maîtrise de son corps comme des grands maîtres tibétains ou yogis. Edelmann occulte donc volontairement les apparitions du Christ aux douze et à Maryam et ce sans la moindre justification.  De fait, en occultant la base même du Christianisme, Eric Edelmann se trompe complètement sur le message de Jésus. Il relit les paraboles, et tente d'y retrouver le sens premier du texte. Mais, il a beau se référer à l'Araméen, en réalité, le nouveau sens dégagé n'est pas toujours éloigné du précédent contrairement à ce qu'il prétend - en l'occurrence, soulignons le, il apporte néanmoins de nombreux points de vues intéressants sur les termes utilisés dans le Nouveau Testament, permettant au néophyte d'appréhender un texte plus difficile qu'on ne le croit ; on apprécie aussi le sort qu'il fait à la traduction de la Bible par la Bible de Jérusalem : les faiblesses sont patentes et on apprécie de se voir mis en garde. Mais Edelmann, hélas, pousse loin les choses en prétendant que pour Jésus, il n'y a ni Bien ni Mal, mais juste une bonne façon d'avancer sur le chemin intérieur. Quelle fatuité ! Comment alors cataloguer le chemin qui ne conduit pas au bien être comparé à celui conduisant à la pureté : ne sont-ce pas le Mal et le Bien ? Cette prétention ridicule à nier l'idée de pêché, à nier l'idée de faute, revient à faire de Jésus un simple gourou. Il le met d'ailleurs sur le même plan que les maître indiens : après tout, pourquoi pas proposer un dialogue interreligieux ; pourquoi pas donner une lecture spirituelle de Jésus. Mais en tout cas, il ne jamais espérer ou croire pouvoir devenir meilleur, plus maître de soi, plus noble et sage en oubliant que l'Amour se révèle la seule clé permettant notre transformation.  Cela ne rate pas : la relecture des paraboles évangéliques d'Edelmann ne tiennent pas la route une seule seconde. Disons le : on ne retiens rien de ses verbiages ! En effet, l'auteur ne cesse de répéter tout au long du livre que les propos de Jésus sont fort clairs, et ses instructions précises. Mais en le lisant, on ne voit absolument pas comment faire pour atteindre cet état d'éveil ! Oh, certes, on a compris qu'il fallait apprendre à considérer les choses sous un angle juste, faire preuve d'humilité, apprendre à faire confiance à notre voix intérieur, accepter de nous reconnaître comme faibles du point de vue psychique et travailler sur nos résistances, mais avait-on besoin de ce livre pour cela ? La réponse est non : car au-delà de toutes les erreurs de l'auteur, il oublie qu'un enseignement de type gourou passe par l'oral : que Jésus a eu beau écrire tout ce qu'il pouvait écrire, ses paroles resteront à jamais lettre morte pour celui refusant d'accepter la réalité de l'Amour de Dieu.  A l'origine des premières Eglises, les gnostiques apparurent : ils récupérèrent la figure du Christ pour en faire le héraut de leur enseignement. Refusant de reconnaître que Dieu était le père du Monde, il croyait qu'il y avait deux entités, dont l'une cruelle avait créé la matière et qu'il fallait rejeter le corps pour atteindre la libération par la mort, voir son esprit voler dans les étoiles, et retrouver la demeure des dieux. Ils furent considérés comme hérétiques puisque leurs traditions et leurs philosophies étaient en contradiction avec l'essence même du Christianisme... Aujourd'hui encore, ce mouvement n'est pas mort : ce livre en atteste. Tant qu'on lira les paroles du Christ en étant persuadé qu'on peut se passer d'aimer, qu'on peut se contenter d'atteindre l'éveil en acquérant des connaissances, on restera un gnostique, un être incomplet, incapable de se libérer des réelles chaînes de l'Etre. Et l'auteur aura beau condamner le gnosticisme et célébrer la gnose, cela ne change rien : il pêche, soit selon le sens étymologique du mot, rate sa cible.\\n\", 'input_ids': [0, 27301, 107, 40, 27616, 81, 5794, 107, 288, 637, 366, 31888, 8658, 204, 4373, 4387, 33, 5, 30401, 4, 95, 25, 17495, 239, 95, 25, 17168, 113, 7451, 5518, 6649, 147, 31888, 4, 22011, 25, 346, 22, 7, 11403, 4204, 4, 8658, 204, 51, 31256, 13, 10792, 1671, 4069, 724, 4, 95, 25, 268, 1376, 398, 33, 17091, 3116, 5, 1880, 4, 617, 6, 21486, 1326, 390, 23831, 459, 637, 191, 17334, 2627, 24474, 3312, 210, 12317, 7, 113, 31619, 16412, 4, 191, 15862, 2943, 10792, 2194, 2943, 15873, 2649, 2353, 142, 1399, 5, 248, 1820, 4, 95, 25, 22543, 335, 95, 25, 15608, 8, 21, 22362, 103, 25, 290, 22439, 11504, 15713, 22, 4373, 4387, 33, 7427, 7, 4204, 616, 405, 3937, 3139, 210, 12317, 7, 24474, 7, 20, 1403, 97, 16470, 813, 25, 1205, 845, 18349, 8861, 13, 94, 4847, 103, 25, 290, 12317, 81, 95, 25, 3259, 4352, 5, 664, 17792, 3882, 1723, 21134, 4, 22, 9947, 540, 4310, 224, 81, 4373, 4387, 33, 4, 17669, 1572, 18496, 8, 9213, 106, 191, 9744, 7, 8, 31888, 22, 16487, 540, 3888, 758, 239, 366, 813, 25, 346, 6294, 30172, 27485, 2664, 5, 2997, 4, 10792, 1521, 4, 14038, 25053, 2830, 4, 95, 25, 15608, 113, 7451, 4, 536, 25, 11, 22, 1820, 5794, 1820, 8, 95, 25, 17495, 8, 19169, 8, 617, 14706, 6, 25818, 573, 2781, 7936, 8, 12041, 22836, 521, 4526, 33, 1372, 191, 22439, 11504, 7, 482, 22, 10124, 51, 4847, 758, 22424, 4, 200, 10039, 616, 10818, 21430, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "[INFO|trainer.py:738] 2023-11-14 13:41:22,121 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:41:22,126 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:41:22,126 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:41:22,127 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:41:22,127 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:41:22,127 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:41:22,127 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:41:22,127 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:41:22,127 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.737, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.7332, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6631, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.7355, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6993, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.6733, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6968, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6998, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6936, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.6963, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.7322, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.7305, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.7001, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6934, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6845, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.7007, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6841, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6968, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6859, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.7012, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6836, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6923, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6729, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6819, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.6853, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6877, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.6827, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.6741, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6988, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6438, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.6615, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6468, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6389, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6641, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6832, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6666, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6337, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6451, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.6334, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6102, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6363, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5987, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.6692, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.6354, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.6341, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:11<00:42, 21.18it/s][INFO|trainer.py:738] 2023-11-14 13:41:33,901 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:41:33,903 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:41:33,903 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:41:33,903 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 78.04it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 73.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6117699146270752, 'eval_accuracy': 0.685, 'eval_runtime': 0.3698, 'eval_samples_per_second': 540.853, 'eval_steps_per_second': 67.607, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:12<00:42, 21.18it/s]\n",
            "100% 25/25 [00:00<00:00, 71.58it/s]\u001b[A\n",
            "{'loss': 0.5904, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.6181, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.6574, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.5781, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.5884, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.5496, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.5944, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.4828, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.5312, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.5833, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.5818, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.5773, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.51, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.5678, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.5534, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.5996, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4927, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.5812, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.5439, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.5153, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.5187, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.5408, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.4218, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.4973, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.4244, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.4611, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5812, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5478, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4468, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.4828, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.5085, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.5241, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.5362, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.4895, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5246, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.4826, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.5778, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.4248, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4609, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.5039, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.4845, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.5673, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.4848, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4195, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.4686, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:22<00:33, 20.06it/s][INFO|trainer.py:738] 2023-11-14 13:41:44,911 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:41:44,916 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:41:44,916 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:41:44,916 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 69.65it/s]\u001b[A\n",
            " 60% 15/25 [00:00<00:00, 64.94it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4730178713798523, 'eval_accuracy': 0.79, 'eval_runtime': 0.429, 'eval_samples_per_second': 466.202, 'eval_steps_per_second': 58.275, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:23<00:33, 20.06it/s]\n",
            "100% 25/25 [00:00<00:00, 61.24it/s]\u001b[A\n",
            "{'loss': 0.5048, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.4101, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.5177, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.5046, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.4407, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.4671, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.4518, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.3792, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.4872, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4402, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:25<00:31, 19.81it/s][INFO|trainer.py:2883] 2023-11-14 13:41:47,850 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:41:47,851 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:41:48,111 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:41:48,112 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:41:48,113 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.5017, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.4425, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.469, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.3263, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.3557, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.5957, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.4068, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.6134, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.4518, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.4686, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.4073, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.3019, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4291, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.431, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2891, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3971, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.5097, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.4457, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.4271, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.403, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.4781, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.5062, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.3759, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.5783, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.566, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.4618, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.5046, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.4605, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.2764, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.3862, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.3327, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.4715, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.5044, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.4788, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.3726, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:35<00:21, 21.05it/s][INFO|trainer.py:738] 2023-11-14 13:41:57,435 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:41:57,436 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:41:57,436 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:41:57,436 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 79.02it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 71.04it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4756459891796112, 'eval_accuracy': 0.76, 'eval_runtime': 0.3788, 'eval_samples_per_second': 528.023, 'eval_steps_per_second': 66.003, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:35<00:21, 21.05it/s]\n",
            "100% 25/25 [00:00<00:00, 69.65it/s]\u001b[A\n",
            "{'loss': 0.3951, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.3026, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.3455, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.3958, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.401, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.557, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.3879, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.4487, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.5405, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.403, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.4183, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.3515, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.321, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.3695, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.3968, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.2766, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.5237, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.3758, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.4342, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.4356, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.547, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.6411, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.2751, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.406, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.3553, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.246, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.2349, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.3914, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.2437, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.4053, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.3681, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.5245, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.4569, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.234, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3463, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.5858, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.5076, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.7426, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.3698, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.3112, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.2818, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.3463, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.6663, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.4741, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.3812, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:46<00:11, 20.11it/s][INFO|trainer.py:738] 2023-11-14 13:42:08,803 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:42:08,805 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:42:08,805 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:42:08,805 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 69.28it/s]\u001b[A\n",
            " 60% 15/25 [00:00<00:00, 64.22it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4417361915111542, 'eval_accuracy': 0.795, 'eval_runtime': 0.4253, 'eval_samples_per_second': 470.302, 'eval_steps_per_second': 58.788, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:47<00:11, 20.11it/s]\n",
            "100% 25/25 [00:00<00:00, 62.25it/s]\u001b[A\n",
            "{'loss': 0.3238, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.4048, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.5056, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.3508, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3932, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.5281, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.4805, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.3733, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.2898, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.4467, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.4211, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.2827, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.4119, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.3321, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.4687, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3965, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.2295, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.3652, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.4403, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.418, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [00:52<00:06, 20.08it/s][INFO|trainer.py:2883] 2023-11-14 13:42:14,272 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:42:14,275 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:42:14,545 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:42:14,546 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:42:14,546 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.5079, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.3996, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.4008, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.4866, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.2861, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.2055, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3795, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.3965, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.3084, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.3697, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.3253, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.5829, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.2825, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.3274, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.2311, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.348, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.3713, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.497, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3145, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.3204, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.3964, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5854, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.4374, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.3399, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.2748, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:58<00:00, 21.42it/s][INFO|trainer.py:738] 2023-11-14 13:42:21,094 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:42:21,096 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:42:21,096 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:42:21,096 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 78.03it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 72.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.42570579051971436, 'eval_accuracy': 0.8, 'eval_runtime': 0.3705, 'eval_samples_per_second': 539.845, 'eval_steps_per_second': 67.481, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:59<00:00, 21.42it/s]\n",
            "100% 25/25 [00:00<00:00, 70.83it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:42:21,468 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 59.3414, 'train_samples_per_second': 151.665, 'train_steps_per_second': 18.958, 'train_loss': 0.4882842870288425, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:59<00:00, 18.96it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:42:21,471 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:42:21,472 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:42:21,655 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:42:21,656 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:42:21,656 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4883\n",
            "  train_runtime            = 0:00:59.34\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    151.665\n",
            "  train_steps_per_second   =     18.958\n",
            "11/14/2023 13:42:21 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:42:21,678 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:42:21,680 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:42:21,681 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:42:21,681 >>   Batch size = 8\n",
            "100% 25/25 [00:00<00:00, 69.98it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =        0.8\n",
            "  eval_loss               =     0.4257\n",
            "  eval_runtime            = 0:00:00.37\n",
            "  eval_samples            =        200\n",
            "  eval_samples_per_second =    529.805\n",
            "  eval_steps_per_second   =     66.226\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:42:22,061 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8}]}\n",
            "2023-11-14 13:42:27.617647: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:42:27.617719: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:42:27.617761: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:42:29.644760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:42:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:42:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/test,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=1,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:42:34 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:42:34 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/test.json\n",
            "Using custom data configuration default-432f54c1cb7c184e\n",
            "11/14/2023 13:42:34 - INFO - datasets.builder - Using custom data configuration default-432f54c1cb7c184e\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:42:34 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 13:42:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:42:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:42:34 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:42:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:42:34,482 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:42:34,487 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:42:34,488 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:42:34,488 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:42:34,493 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:42:34,494 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:42:34,494 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:42:34,495 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:42:34,495 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:42:34,495 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:42:34,495 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:42:34,496 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:42:34,703 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:42:34,705 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:42:34,877 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:42:35,481 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:42:35,481 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "11/14/2023 13:42:35 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "Running tokenizer on dataset:   0% 0/2000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ebf09547b96d3b2.arrow\n",
            "11/14/2023 13:42:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ebf09547b96d3b2.arrow\n",
            "Running tokenizer on dataset: 100% 2000/2000 [00:01<00:00, 1175.66 examples/s]\n",
            "11/14/2023 13:42:37 - INFO - __main__ - Sample 275 of the training set: {'label': 1, 'text': 'On croit bien évidemment tout savoir sur l\\'horreur indicible des camps de concentration. Et bien non, \"si c\\'est un homme\" laisse littéralement sans voix ! Tout commentaire paraît déplacé et pour tout dire insignifiant. Lisez, vous comprendrez.  Mon seul commentaire est donc qu\\'il faut impérativement recommander la lecture de cet ouvrage à tous et particulièrement aux grands adolescents.\\n', 'input_ids': [0, 1360, 5181, 204, 1202, 27809, 1530, 6958, 511, 95, 25, 2893, 106, 415, 26070, 1581, 210, 4689, 7, 8, 30525, 5, 1183, 1202, 324, 4, 44, 167, 432, 25, 449, 51, 13971, 58, 17087, 2845, 14397, 7041, 2814, 22356, 573, 9757, 27288, 119, 8412, 16884, 138, 4747, 81, 482, 1530, 2664, 30128, 25306, 5, 21780, 164, 4, 773, 16504, 164, 5, 3424, 10724, 27288, 390, 3882, 813, 25, 346, 4219, 7578, 2446, 12754, 550, 29040, 42, 21, 15429, 8, 6572, 6, 25818, 239, 2862, 81, 6, 20707, 1723, 17041, 29943, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:42:37 - INFO - __main__ - Sample 1165 of the training set: {'label': 0, 'text': \"L'intrigue est faible, mais l'epopee dans la jungle est divertissante, un peu longue peut-etre; la fin est sans surprise.\\n\", 'input_ids': [0, 313, 25, 31850, 390, 22315, 4, 585, 95, 25, 13, 10337, 3264, 637, 21, 16391, 130, 390, 22248, 16220, 13, 4, 51, 2998, 18512, 2293, 9, 123, 106, 73, 21, 1408, 390, 2814, 14727, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:42:37 - INFO - __main__ - Sample 1735 of the training set: {'label': 0, 'text': \"Rien ne se tient et rien ne va dans ce Jésus parlait Araméen. Pourtant, l'idée à l'origine du livre était bonne : Jésus, lorsqu'il enseignait, parlait un dialecte hébraïque, l'Araméen palestinien. Or, son enseignement nous est parvenu dans les traductions grecques des textes du Nouveau Testament, les originaux hébreux ayant été égarés. De fait, l'annonce par l'auteur de la découverte d'un Evangile écrit en Araméen autorisait une relecture des textes grecs - car on sait qu'une traduction transforme le sens d'un texte et l'altère. Revenir donc aux sources, en comparant Grec et Araméen, aurait pu permettre de relire les paroles de Jésus en collant encore plus à ce qu'il avait réellement voulu dire.  Mais, hélas, Eric Edelmann, l'auteur du livre, n'a en fait rien fait de l'idée de départ de son propre ouvrage ! Au lieu de retraduire ou repenser les Evangiles pour en donner un sens plus précis, il prend une option philosophiquement acceptable, mais indéfendable d'un point de vue chrétien, le gnosticisme - position qu'il récuse, persuadé de la différence entre la gnose (connaissance) et gnosticisme (religion basée sur la connaissance). Selon Edelmann, en effet, Jésus a donné un enseignement permettant à tout disciple le suivant de réaliser sa transformation intérieure, de prendre la voie de l'élévation spirituelle. Or, l'erreur majeure de l'auteur aura été de considérer que cette transformation est indépendante du caeur du message du Christ. Ainsi, Edelmann fait fi du plus grand enseignement de la Thora revendiqué par Jésus : aimer le seul et unique Dieu, et aimer son prochain comme sois même. Car l'amour, cet amour totale et entier à la base du Christianisme reste la condition sine qua non pour que tout un chacun, nous nous retrouvions réellement transformés. Sans compter qu'au-delà même de cet amour humain, il y a l'Amour de Dieu : Jésus, en ressuscitant, trompe la mort et donne à chacun la garantie de l'immortalité de l'âme - d'où le sacrifice du Christ, sacrifice d'Amour pur. Il refuse l'idée de résurrection du Christ, préférant réduire son triomphe sur la mort à une maîtrise de son corps comme des grands maîtres tibétains ou yogis. Edelmann occulte donc volontairement les apparitions du Christ aux douze et à Maryam et ce sans la moindre justification.  De fait, en occultant la base même du Christianisme, Eric Edelmann se trompe complètement sur le message de Jésus. Il relit les paraboles, et tente d'y retrouver le sens premier du texte. Mais, il a beau se référer à l'Araméen, en réalité, le nouveau sens dégagé n'est pas toujours éloigné du précédent contrairement à ce qu'il prétend - en l'occurrence, soulignons le, il apporte néanmoins de nombreux points de vues intéressants sur les termes utilisés dans le Nouveau Testament, permettant au néophyte d'appréhender un texte plus difficile qu'on ne le croit ; on apprécie aussi le sort qu'il fait à la traduction de la Bible par la Bible de Jérusalem : les faiblesses sont patentes et on apprécie de se voir mis en garde. Mais Edelmann, hélas, pousse loin les choses en prétendant que pour Jésus, il n'y a ni Bien ni Mal, mais juste une bonne façon d'avancer sur le chemin intérieur. Quelle fatuité ! Comment alors cataloguer le chemin qui ne conduit pas au bien être comparé à celui conduisant à la pureté : ne sont-ce pas le Mal et le Bien ? Cette prétention ridicule à nier l'idée de pêché, à nier l'idée de faute, revient à faire de Jésus un simple gourou. Il le met d'ailleurs sur le même plan que les maître indiens : après tout, pourquoi pas proposer un dialogue interreligieux ; pourquoi pas donner une lecture spirituelle de Jésus. Mais en tout cas, il ne jamais espérer ou croire pouvoir devenir meilleur, plus maître de soi, plus noble et sage en oubliant que l'Amour se révèle la seule clé permettant notre transformation.  Cela ne rate pas : la relecture des paraboles évangéliques d'Edelmann ne tiennent pas la route une seule seconde. Disons le : on ne retiens rien de ses verbiages ! En effet, l'auteur ne cesse de répéter tout au long du livre que les propos de Jésus sont fort clairs, et ses instructions précises. Mais en le lisant, on ne voit absolument pas comment faire pour atteindre cet état d'éveil ! Oh, certes, on a compris qu'il fallait apprendre à considérer les choses sous un angle juste, faire preuve d'humilité, apprendre à faire confiance à notre voix intérieur, accepter de nous reconnaître comme faibles du point de vue psychique et travailler sur nos résistances, mais avait-on besoin de ce livre pour cela ? La réponse est non : car au-delà de toutes les erreurs de l'auteur, il oublie qu'un enseignement de type gourou passe par l'oral : que Jésus a eu beau écrire tout ce qu'il pouvait écrire, ses paroles resteront à jamais lettre morte pour celui refusant d'accepter la réalité de l'Amour de Dieu.  A l'origine des premières Eglises, les gnostiques apparurent : ils récupérèrent la figure du Christ pour en faire le héraut de leur enseignement. Refusant de reconnaître que Dieu était le père du Monde, il croyait qu'il y avait deux entités, dont l'une cruelle avait créé la matière et qu'il fallait rejeter le corps pour atteindre la libération par la mort, voir son esprit voler dans les étoiles, et retrouver la demeure des dieux. Ils furent considérés comme hérétiques puisque leurs traditions et leurs philosophies étaient en contradiction avec l'essence même du Christianisme... Aujourd'hui encore, ce mouvement n'est pas mort : ce livre en atteste. Tant qu'on lira les paroles du Christ en étant persuadé qu'on peut se passer d'aimer, qu'on peut se contenter d'atteindre l'éveil en acquérant des connaissances, on restera un gnostique, un être incomplet, incapable de se libérer des réelles chaînes de l'Etre. Et l'auteur aura beau condamner le gnosticisme et célébrer la gnose, cela ne change rien : il pêche, soit selon le sens étymologique du mot, rate sa cible.\\n\", 'input_ids': [0, 27301, 107, 40, 27616, 81, 5794, 107, 288, 637, 366, 31888, 8658, 204, 4373, 4387, 33, 5, 30401, 4, 95, 25, 17495, 239, 95, 25, 17168, 113, 7451, 5518, 6649, 147, 31888, 4, 22011, 25, 346, 22, 7, 11403, 4204, 4, 8658, 204, 51, 31256, 13, 10792, 1671, 4069, 724, 4, 95, 25, 268, 1376, 398, 33, 17091, 3116, 5, 1880, 4, 617, 6, 21486, 1326, 390, 23831, 459, 637, 191, 17334, 2627, 24474, 3312, 210, 12317, 7, 113, 31619, 16412, 4, 191, 15862, 2943, 10792, 2194, 2943, 15873, 2649, 2353, 142, 1399, 5, 248, 1820, 4, 95, 25, 22543, 335, 95, 25, 15608, 8, 21, 22362, 103, 25, 290, 22439, 11504, 15713, 22, 4373, 4387, 33, 7427, 7, 4204, 616, 405, 3937, 3139, 210, 12317, 7, 24474, 7, 20, 1403, 97, 16470, 813, 25, 1205, 845, 18349, 8861, 13, 94, 4847, 103, 25, 290, 12317, 81, 95, 25, 3259, 4352, 5, 664, 17792, 3882, 1723, 21134, 4, 22, 9947, 540, 4310, 224, 81, 4373, 4387, 33, 4, 17669, 1572, 18496, 8, 9213, 106, 191, 9744, 7, 8, 31888, 22, 16487, 540, 3888, 758, 239, 366, 813, 25, 346, 6294, 30172, 27485, 2664, 5, 2997, 4, 10792, 1521, 4, 14038, 25053, 2830, 4, 95, 25, 15608, 113, 7451, 4, 536, 25, 11, 22, 1820, 5794, 1820, 8, 95, 25, 17495, 8, 19169, 8, 617, 14706, 6, 25818, 573, 2781, 7936, 8, 12041, 22836, 521, 4526, 33, 1372, 191, 22439, 11504, 7, 482, 22, 10124, 51, 4847, 758, 22424, 4, 200, 10039, 616, 10818, 21430, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "[INFO|trainer.py:738] 2023-11-14 13:42:40,903 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:42:40,911 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:42:40,912 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:42:40,912 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:42:40,912 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:42:40,912 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:42:40,912 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:42:40,912 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:42:40,912 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.737, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.7332, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6631, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.7355, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.6993, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.6733, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.6968, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6998, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6936, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.6963, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.7322, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.7305, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.7001, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6934, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6845, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.7007, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6841, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6968, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6859, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.7012, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6836, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6923, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6729, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6819, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.6853, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6877, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.6827, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.6741, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6988, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6438, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.6615, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6468, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6389, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6641, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6832, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6666, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6337, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6451, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.6334, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6102, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6363, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5987, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.6692, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.6354, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.6341, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:11<00:44, 20.16it/s][INFO|trainer.py:738] 2023-11-14 13:42:52,515 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:42:52,518 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:42:52,518 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:42:52,518 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 70.49it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 63.10it/s]\u001b[A\n",
            "  9% 23/250 [00:00<00:03, 62.28it/s]\u001b[A\n",
            " 12% 30/250 [00:00<00:03, 62.24it/s]\u001b[A\n",
            " 15% 37/250 [00:00<00:03, 60.56it/s]\u001b[A\n",
            " 18% 44/250 [00:00<00:03, 60.47it/s]\u001b[A\n",
            " 20% 51/250 [00:00<00:03, 60.15it/s]\u001b[A\n",
            " 23% 58/250 [00:00<00:03, 60.59it/s]\u001b[A\n",
            " 26% 65/250 [00:01<00:03, 60.81it/s]\u001b[A\n",
            " 29% 72/250 [00:01<00:02, 61.11it/s]\u001b[A\n",
            " 32% 79/250 [00:01<00:02, 59.77it/s]\u001b[A\n",
            " 34% 85/250 [00:01<00:02, 59.61it/s]\u001b[A\n",
            " 37% 92/250 [00:01<00:02, 60.32it/s]\u001b[A\n",
            " 40% 99/250 [00:01<00:02, 60.93it/s]\u001b[A\n",
            " 42% 106/250 [00:01<00:02, 61.04it/s]\u001b[A\n",
            " 45% 113/250 [00:01<00:02, 60.11it/s]\u001b[A\n",
            " 48% 120/250 [00:01<00:02, 59.70it/s]\u001b[A\n",
            " 51% 127/250 [00:02<00:02, 60.06it/s]\u001b[A\n",
            " 54% 134/250 [00:02<00:01, 60.21it/s]\u001b[A\n",
            " 56% 141/250 [00:02<00:01, 60.38it/s]\u001b[A\n",
            " 59% 148/250 [00:02<00:01, 60.62it/s]\u001b[A\n",
            " 62% 155/250 [00:02<00:01, 61.11it/s]\u001b[A\n",
            " 65% 162/250 [00:02<00:01, 61.42it/s]\u001b[A\n",
            " 68% 169/250 [00:02<00:01, 61.48it/s]\u001b[A\n",
            " 70% 176/250 [00:02<00:01, 60.56it/s]\u001b[A\n",
            " 73% 183/250 [00:03<00:01, 58.90it/s]\u001b[A\n",
            " 76% 190/250 [00:03<00:01, 59.83it/s]\u001b[A\n",
            " 79% 197/250 [00:03<00:00, 60.15it/s]\u001b[A\n",
            " 82% 204/250 [00:03<00:00, 60.24it/s]\u001b[A\n",
            " 84% 211/250 [00:03<00:00, 60.61it/s]\u001b[A\n",
            " 87% 218/250 [00:03<00:00, 59.82it/s]\u001b[A\n",
            " 90% 225/250 [00:03<00:00, 60.43it/s]\u001b[A\n",
            " 93% 232/250 [00:03<00:00, 60.98it/s]\u001b[A\n",
            " 96% 239/250 [00:03<00:00, 60.41it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5829306840896606, 'eval_accuracy': 0.7335, 'eval_runtime': 4.1606, 'eval_samples_per_second': 480.699, 'eval_steps_per_second': 60.087, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:15<00:44, 20.16it/s]\n",
            "100% 250/250 [00:04<00:00, 60.71it/s]\u001b[A\n",
            "{'loss': 0.5904, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.6181, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.6574, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.5781, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.5884, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.5496, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.5944, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.4828, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.5312, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.5833, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.5818, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.5773, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.51, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.5678, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.5534, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.5996, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4927, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.5812, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.5439, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.5153, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.5187, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.5408, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.4218, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.4973, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.4244, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.4611, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5812, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5478, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4468, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.4828, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.5085, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.5241, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.5362, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.4895, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5246, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.4826, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.5778, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.4248, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4609, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.5039, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.4845, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.5673, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.4848, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4195, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.4686, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:26<00:31, 21.16it/s][INFO|trainer.py:738] 2023-11-14 13:43:07,553 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:43:07,555 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:43:07,556 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:43:07,556 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 78.94it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 72.32it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 68.54it/s]\u001b[A\n",
            " 12% 31/250 [00:00<00:03, 68.61it/s]\u001b[A\n",
            " 15% 38/250 [00:00<00:03, 68.38it/s]\u001b[A\n",
            " 18% 45/250 [00:00<00:03, 67.47it/s]\u001b[A\n",
            " 21% 52/250 [00:00<00:02, 67.89it/s]\u001b[A\n",
            " 24% 59/250 [00:00<00:02, 67.84it/s]\u001b[A\n",
            " 26% 66/250 [00:00<00:02, 66.74it/s]\u001b[A\n",
            " 29% 73/250 [00:01<00:02, 67.16it/s]\u001b[A\n",
            " 32% 80/250 [00:01<00:02, 67.63it/s]\u001b[A\n",
            " 35% 87/250 [00:01<00:02, 66.53it/s]\u001b[A\n",
            " 38% 94/250 [00:01<00:02, 66.84it/s]\u001b[A\n",
            " 40% 101/250 [00:01<00:02, 67.24it/s]\u001b[A\n",
            " 43% 108/250 [00:01<00:02, 67.36it/s]\u001b[A\n",
            " 46% 115/250 [00:01<00:02, 67.33it/s]\u001b[A\n",
            " 49% 122/250 [00:01<00:01, 67.59it/s]\u001b[A\n",
            " 52% 129/250 [00:01<00:01, 66.99it/s]\u001b[A\n",
            " 54% 136/250 [00:02<00:01, 67.35it/s]\u001b[A\n",
            " 57% 143/250 [00:02<00:01, 67.49it/s]\u001b[A\n",
            " 60% 150/250 [00:02<00:01, 66.93it/s]\u001b[A\n",
            " 63% 157/250 [00:02<00:01, 66.95it/s]\u001b[A\n",
            " 66% 164/250 [00:02<00:01, 67.23it/s]\u001b[A\n",
            " 68% 171/250 [00:02<00:01, 67.60it/s]\u001b[A\n",
            " 71% 178/250 [00:02<00:01, 67.63it/s]\u001b[A\n",
            " 74% 185/250 [00:02<00:00, 66.77it/s]\u001b[A\n",
            " 77% 192/250 [00:02<00:00, 66.98it/s]\u001b[A\n",
            " 80% 199/250 [00:02<00:00, 66.79it/s]\u001b[A\n",
            " 82% 206/250 [00:03<00:00, 67.44it/s]\u001b[A\n",
            " 85% 213/250 [00:03<00:00, 67.67it/s]\u001b[A\n",
            " 88% 220/250 [00:03<00:00, 66.94it/s]\u001b[A\n",
            " 91% 227/250 [00:03<00:00, 67.31it/s]\u001b[A\n",
            " 94% 234/250 [00:03<00:00, 66.64it/s]\u001b[A\n",
            " 96% 241/250 [00:03<00:00, 67.50it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4796521067619324, 'eval_accuracy': 0.7655, 'eval_runtime': 3.7297, 'eval_samples_per_second': 536.24, 'eval_steps_per_second': 67.03, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:30<00:31, 21.16it/s]\n",
            "100% 250/250 [00:03<00:00, 67.62it/s]\u001b[A\n",
            "{'loss': 0.5048, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.4101, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.5177, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.5046, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.4407, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.4671, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.4518, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.3792, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.4872, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4402, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:32<00:31, 19.71it/s][INFO|trainer.py:2883] 2023-11-14 13:43:13,769 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:43:13,770 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:43:14,098 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:43:14,099 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:43:14,100 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.5017, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.4425, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.469, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.3263, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.3557, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.5957, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.4068, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.6134, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.4518, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.4686, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.4073, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.3019, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4291, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.431, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2891, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3971, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.5097, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.4457, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.4271, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.403, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.4781, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.5062, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.3759, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.5783, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.566, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.4618, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.5046, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.4605, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.2764, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.3862, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.3327, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.4715, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.5044, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.4788, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.3726, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:42<00:22, 20.02it/s][INFO|trainer.py:738] 2023-11-14 13:43:23,527 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:43:23,529 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:43:23,529 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:43:23,529 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 77.31it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 71.37it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 68.55it/s]\u001b[A\n",
            " 12% 31/250 [00:00<00:03, 68.28it/s]\u001b[A\n",
            " 15% 38/250 [00:00<00:03, 66.84it/s]\u001b[A\n",
            " 18% 45/250 [00:00<00:03, 67.45it/s]\u001b[A\n",
            " 21% 52/250 [00:00<00:02, 66.99it/s]\u001b[A\n",
            " 24% 59/250 [00:00<00:02, 66.27it/s]\u001b[A\n",
            " 26% 66/250 [00:00<00:02, 66.72it/s]\u001b[A\n",
            " 29% 73/250 [00:01<00:02, 67.08it/s]\u001b[A\n",
            " 32% 80/250 [00:01<00:02, 66.48it/s]\u001b[A\n",
            " 35% 87/250 [00:01<00:02, 66.96it/s]\u001b[A\n",
            " 38% 94/250 [00:01<00:02, 67.21it/s]\u001b[A\n",
            " 40% 101/250 [00:01<00:02, 66.64it/s]\u001b[A\n",
            " 43% 108/250 [00:01<00:02, 67.15it/s]\u001b[A\n",
            " 46% 115/250 [00:01<00:02, 67.17it/s]\u001b[A\n",
            " 49% 122/250 [00:01<00:01, 66.18it/s]\u001b[A\n",
            " 52% 129/250 [00:01<00:01, 66.45it/s]\u001b[A\n",
            " 54% 136/250 [00:02<00:01, 66.99it/s]\u001b[A\n",
            " 57% 143/250 [00:02<00:01, 66.28it/s]\u001b[A\n",
            " 60% 150/250 [00:02<00:01, 66.85it/s]\u001b[A\n",
            " 63% 157/250 [00:02<00:01, 67.09it/s]\u001b[A\n",
            " 66% 164/250 [00:02<00:01, 66.72it/s]\u001b[A\n",
            " 68% 171/250 [00:02<00:01, 66.64it/s]\u001b[A\n",
            " 71% 178/250 [00:02<00:01, 67.03it/s]\u001b[A\n",
            " 74% 185/250 [00:02<00:00, 66.78it/s]\u001b[A\n",
            " 77% 192/250 [00:02<00:00, 66.65it/s]\u001b[A\n",
            " 80% 199/250 [00:02<00:00, 67.01it/s]\u001b[A\n",
            " 82% 206/250 [00:03<00:00, 66.52it/s]\u001b[A\n",
            " 85% 213/250 [00:03<00:00, 67.34it/s]\u001b[A\n",
            " 88% 220/250 [00:03<00:00, 67.10it/s]\u001b[A\n",
            " 91% 227/250 [00:03<00:00, 66.55it/s]\u001b[A\n",
            " 94% 234/250 [00:03<00:00, 67.20it/s]\u001b[A\n",
            " 96% 241/250 [00:03<00:00, 67.50it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4640734791755676, 'eval_accuracy': 0.788, 'eval_runtime': 3.7535, 'eval_samples_per_second': 532.829, 'eval_steps_per_second': 66.604, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:46<00:22, 20.02it/s]\n",
            "100% 250/250 [00:03<00:00, 67.09it/s]\u001b[A\n",
            "{'loss': 0.3951, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.3026, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.3455, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.3958, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.401, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.557, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.3879, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.4487, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.5405, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.403, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.4183, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.3515, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.321, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.3695, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.3968, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.2766, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.5237, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.3758, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.4342, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.4356, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.547, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.6411, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.2751, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.406, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.3553, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.246, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.2349, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.3914, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.2437, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.4053, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.3681, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.5245, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.4569, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.234, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3463, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.5858, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.5076, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.7426, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.3698, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.3112, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.2818, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.3463, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.6663, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.4741, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.3812, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:57<00:11, 20.17it/s][INFO|trainer.py:738] 2023-11-14 13:43:38,248 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:43:38,253 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:43:38,253 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:43:38,253 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 69.94it/s]\u001b[A\n",
            "  6% 15/250 [00:00<00:03, 63.94it/s]\u001b[A\n",
            "  9% 22/250 [00:00<00:03, 60.88it/s]\u001b[A\n",
            " 12% 29/250 [00:00<00:03, 60.28it/s]\u001b[A\n",
            " 14% 36/250 [00:00<00:03, 60.37it/s]\u001b[A\n",
            " 17% 43/250 [00:00<00:03, 60.77it/s]\u001b[A\n",
            " 20% 50/250 [00:00<00:03, 60.71it/s]\u001b[A\n",
            " 23% 57/250 [00:00<00:03, 59.68it/s]\u001b[A\n",
            " 25% 63/250 [00:01<00:03, 59.22it/s]\u001b[A\n",
            " 28% 69/250 [00:01<00:03, 57.30it/s]\u001b[A\n",
            " 30% 76/250 [00:01<00:02, 58.50it/s]\u001b[A\n",
            " 33% 83/250 [00:01<00:02, 59.04it/s]\u001b[A\n",
            " 36% 89/250 [00:01<00:02, 58.58it/s]\u001b[A\n",
            " 38% 96/250 [00:01<00:02, 59.05it/s]\u001b[A\n",
            " 41% 103/250 [00:01<00:02, 59.32it/s]\u001b[A\n",
            " 44% 110/250 [00:01<00:02, 59.65it/s]\u001b[A\n",
            " 47% 117/250 [00:01<00:02, 59.90it/s]\u001b[A\n",
            " 49% 123/250 [00:02<00:02, 59.84it/s]\u001b[A\n",
            " 52% 130/250 [00:02<00:01, 60.12it/s]\u001b[A\n",
            " 55% 137/250 [00:02<00:01, 60.74it/s]\u001b[A\n",
            " 58% 144/250 [00:02<00:01, 60.51it/s]\u001b[A\n",
            " 60% 151/250 [00:02<00:01, 60.47it/s]\u001b[A\n",
            " 63% 158/250 [00:02<00:01, 60.74it/s]\u001b[A\n",
            " 66% 165/250 [00:02<00:01, 61.08it/s]\u001b[A\n",
            " 69% 172/250 [00:02<00:01, 61.16it/s]\u001b[A\n",
            " 72% 179/250 [00:02<00:01, 57.96it/s]\u001b[A\n",
            " 74% 186/250 [00:03<00:01, 58.65it/s]\u001b[A\n",
            " 77% 193/250 [00:03<00:00, 59.43it/s]\u001b[A\n",
            " 80% 200/250 [00:03<00:00, 59.79it/s]\u001b[A\n",
            " 83% 207/250 [00:03<00:00, 60.27it/s]\u001b[A\n",
            " 86% 214/250 [00:03<00:00, 58.83it/s]\u001b[A\n",
            " 88% 221/250 [00:03<00:00, 59.32it/s]\u001b[A\n",
            " 91% 227/250 [00:03<00:00, 59.25it/s]\u001b[A\n",
            " 93% 233/250 [00:03<00:00, 58.68it/s]\u001b[A\n",
            " 96% 239/250 [00:04<00:00, 58.92it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.45323145389556885, 'eval_accuracy': 0.799, 'eval_runtime': 4.2473, 'eval_samples_per_second': 470.889, 'eval_steps_per_second': 58.861, 'epoch': 4.0}\n",
            " 80% 900/1125 [01:01<00:11, 20.17it/s]\n",
            "100% 250/250 [00:04<00:00, 58.90it/s]\u001b[A\n",
            "{'loss': 0.3238, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.4048, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.5056, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.3508, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3932, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.5281, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.4805, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.3733, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.2898, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.4467, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.4211, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.2827, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.4119, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.3321, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.4687, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3965, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.2295, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.3652, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.4403, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.418, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [01:06<00:05, 21.23it/s][INFO|trainer.py:2883] 2023-11-14 13:43:47,338 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:43:47,339 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:43:47,583 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:43:47,584 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:43:47,585 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.5079, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.3996, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.4008, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.4866, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.2861, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.2055, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3795, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.3965, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.3084, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.3697, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.3253, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.5829, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.2825, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.3274, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.2311, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.348, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.3713, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.497, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3145, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.3204, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.3964, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5854, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.4374, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.3399, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.2748, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:12<00:00, 21.56it/s][INFO|trainer.py:738] 2023-11-14 13:43:53,905 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:43:53,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:43:53,906 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:43:53,906 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 78.38it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 73.61it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 71.30it/s]\u001b[A\n",
            " 13% 32/250 [00:00<00:03, 70.33it/s]\u001b[A\n",
            " 16% 40/250 [00:00<00:02, 70.21it/s]\u001b[A\n",
            " 19% 48/250 [00:00<00:02, 69.67it/s]\u001b[A\n",
            " 22% 55/250 [00:00<00:02, 69.76it/s]\u001b[A\n",
            " 25% 62/250 [00:00<00:02, 67.21it/s]\u001b[A\n",
            " 28% 69/250 [00:01<00:02, 65.01it/s]\u001b[A\n",
            " 30% 76/250 [00:01<00:02, 64.04it/s]\u001b[A\n",
            " 33% 83/250 [00:01<00:02, 63.60it/s]\u001b[A\n",
            " 36% 90/250 [00:01<00:02, 62.63it/s]\u001b[A\n",
            " 39% 97/250 [00:01<00:02, 62.51it/s]\u001b[A\n",
            " 42% 104/250 [00:01<00:02, 61.74it/s]\u001b[A\n",
            " 44% 111/250 [00:01<00:02, 61.12it/s]\u001b[A\n",
            " 47% 118/250 [00:01<00:02, 61.31it/s]\u001b[A\n",
            " 50% 125/250 [00:01<00:02, 61.41it/s]\u001b[A\n",
            " 53% 132/250 [00:02<00:01, 61.36it/s]\u001b[A\n",
            " 56% 139/250 [00:02<00:01, 61.67it/s]\u001b[A\n",
            " 58% 146/250 [00:02<00:01, 61.68it/s]\u001b[A\n",
            " 61% 153/250 [00:02<00:01, 61.52it/s]\u001b[A\n",
            " 64% 160/250 [00:02<00:01, 61.51it/s]\u001b[A\n",
            " 67% 167/250 [00:02<00:01, 61.59it/s]\u001b[A\n",
            " 70% 174/250 [00:02<00:01, 61.61it/s]\u001b[A\n",
            " 72% 181/250 [00:02<00:01, 61.76it/s]\u001b[A\n",
            " 75% 188/250 [00:02<00:01, 61.97it/s]\u001b[A\n",
            " 78% 195/250 [00:03<00:00, 61.55it/s]\u001b[A\n",
            " 81% 202/250 [00:03<00:00, 61.75it/s]\u001b[A\n",
            " 84% 209/250 [00:03<00:00, 61.93it/s]\u001b[A\n",
            " 86% 216/250 [00:03<00:00, 60.80it/s]\u001b[A\n",
            " 89% 223/250 [00:03<00:00, 60.63it/s]\u001b[A\n",
            " 92% 230/250 [00:03<00:00, 60.86it/s]\u001b[A\n",
            " 95% 237/250 [00:03<00:00, 60.83it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.450804203748703, 'eval_accuracy': 0.803, 'eval_runtime': 4.0028, 'eval_samples_per_second': 499.651, 'eval_steps_per_second': 62.456, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:16<00:00, 21.56it/s]\n",
            "100% 250/250 [00:03<00:00, 60.46it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:43:57,910 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 76.9982, 'train_samples_per_second': 116.886, 'train_steps_per_second': 14.611, 'train_loss': 0.4882842870288425, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:16<00:00, 14.61it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:43:57,913 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:43:57,914 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:43:58,284 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:43:58,286 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:43:58,288 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4883\n",
            "  train_runtime            = 0:01:16.99\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    116.886\n",
            "  train_steps_per_second   =     14.611\n",
            "11/14/2023 13:43:58 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:43:58,321 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:43:58,323 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:43:58,323 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:43:58,323 >>   Batch size = 8\n",
            "100% 250/250 [00:04<00:00, 59.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =      0.803\n",
            "  eval_loss               =     0.4508\n",
            "  eval_runtime            = 0:00:04.18\n",
            "  eval_samples            =       2000\n",
            "  eval_samples_per_second =     477.73\n",
            "  eval_steps_per_second   =     59.716\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:44:02,519 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.803}]}\n",
            "2023-11-14 13:44:08.261620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:44:08.261684: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:44:08.261721: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:44:09.441816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:44:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:44:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/valid,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=2,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:44:11 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:44:11 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/valid.json\n",
            "Using custom data configuration default-0b42c7f1e4c120e5\n",
            "11/14/2023 13:44:11 - INFO - datasets.builder - Using custom data configuration default-0b42c7f1e4c120e5\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:44:11 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 13:44:11 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:44:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:44:12 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:44:12 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:44:12,021 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:44:12,025 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:44:12,025 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:44:12,025 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:44:12,026 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:44:12,027 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:44:12,027 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:44:12,027 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:44:12,027 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:44:12,027 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:44:12,027 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:44:12,028 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:44:12,126 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:44:12,126 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:44:12,224 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:44:12,505 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:44:12,505 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "11/14/2023 13:44:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2b699aeeb2a034ae.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f796adcf61d1b8b5.arrow\n",
            "11/14/2023 13:44:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0b42c7f1e4c120e5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f796adcf61d1b8b5.arrow\n",
            "11/14/2023 13:44:12 - INFO - __main__ - Sample 1767 of the training set: {'label': 0, 'text': \"Les trois premiers tomes m'ont totalement transporté! j'ai été époustoufflé par les rebondissements, dans le tome 4 je retrouve le plaisir de lire le style d'écriture de l'autrice mais alors qu'est ce que je suis déçu par le contenu. J'ai mis un jour pour lire chaque tome et le quatrième je l'ai lu en une semaine tellement chaque chapitre me désespéraient c'est pour dire! Certes beaucoup de rebondissements encore mais tellement décevant, cela devient rapidement inintéressant en fait.\\n\", 'input_ids': [0, 1161, 9488, 22841, 7509, 7, 321, 25, 3588, 25440, 2976, 398, 38, 1117, 25, 437, 2649, 356, 615, 16167, 7455, 2034, 335, 191, 405, 14232, 13979, 7, 4, 637, 94, 7509, 193, 55, 20917, 94, 10640, 8, 13123, 94, 7216, 103, 25, 29890, 8, 95, 25, 703, 6819, 585, 5700, 813, 25, 449, 366, 41, 55, 2608, 1904, 6492, 335, 94, 15488, 5, 644, 25, 437, 898, 51, 4841, 482, 13123, 6866, 7509, 81, 94, 1649, 2143, 8038, 55, 95, 25, 437, 1938, 22, 616, 10958, 20945, 6866, 31094, 158, 14479, 89, 4803, 18636, 432, 25, 449, 482, 2664, 38, 3854, 1109, 5239, 8, 405, 14232, 13979, 7, 3888, 585, 20945, 8920, 13, 9661, 4, 4481, 26541, 16622, 23, 2259, 3638, 16220, 22, 1820, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:44:12 - INFO - __main__ - Sample 1738 of the training set: {'label': 0, 'text': 'Livre intéressant, avec pas mal d\\'humour, mais il y a un gros \"mais\".   Après avoir lu le principe de lucifer volume 1 et 2 de bloom, on peut penser que la théorie centrale de ce livre est discutable, voire vraiment incomplète. Le groupe influe énormément sur nos comportements, il n\\'y a pas que nos gènes!   Donc lisez ce livre, puis lisez le principe de lucifer pour une vision différente.\\n', 'input_ids': [0, 29601, 19111, 4, 1098, 402, 732, 103, 25, 30494, 4, 585, 200, 112, 10, 51, 11492, 44, 5915, 592, 12131, 4858, 1938, 94, 15866, 8, 18826, 1686, 7570, 105, 81, 114, 8, 9444, 287, 4, 97, 2293, 21023, 41, 21, 31406, 11809, 8, 366, 7451, 390, 22163, 1581, 4, 5226, 13, 6718, 19186, 10703, 67, 5, 526, 11224, 8749, 13, 31872, 511, 727, 26569, 7, 4, 200, 536, 25, 53, 10, 402, 41, 727, 6, 25426, 7, 38, 16214, 23947, 164, 366, 7451, 4, 7784, 23947, 164, 94, 15866, 8, 18826, 1686, 482, 616, 11240, 26629, 13, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:44:12 - INFO - __main__ - Sample 115 of the training set: {'label': 1, 'text': 'Livre absolument superbe avec des maisons ravissantes, des personnages superjolis. A avoir dans sa bibliothèque !\\n', 'input_ids': [0, 29601, 28596, 25144, 1098, 210, 7781, 7, 10903, 16220, 89, 4, 210, 19371, 7, 1092, 440, 2175, 5, 62, 4858, 637, 57, 31092, 573, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:738] 2023-11-14 13:44:15,107 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:44:15,112 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:44:15,112 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:44:15,112 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:44:15,112 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:44:15,112 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:44:15,112 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:44:15,112 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:44:15,113 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.6636, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.6776, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6907, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.6984, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.7057, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.6917, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.7218, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6783, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6716, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.7024, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.6851, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.6772, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.6741, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6796, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6731, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.6719, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6917, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6752, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6834, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.6763, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6703, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6545, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6788, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6871, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.6513, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6435, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.7039, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.66, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.675, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6806, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.646, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6565, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6199, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6428, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6025, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6496, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6314, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6872, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.5474, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6132, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6637, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5369, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.589, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.5781, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.6016, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:11<00:44, 20.24it/s][INFO|trainer.py:738] 2023-11-14 13:44:26,843 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:44:26,845 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:44:26,845 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:44:26,845 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 71.10it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 65.21it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6240662336349487, 'eval_accuracy': 0.655, 'eval_runtime': 0.4374, 'eval_samples_per_second': 457.286, 'eval_steps_per_second': 57.161, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:12<00:44, 20.24it/s]\n",
            "100% 25/25 [00:00<00:00, 58.97it/s]\u001b[A\n",
            "{'loss': 0.6392, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.59, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.5209, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.5375, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.6267, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.6188, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.4702, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.6084, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.488, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.4519, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.6111, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.4985, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.5102, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.3624, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.5021, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.4562, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4582, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.4499, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.4381, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.685, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.4657, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.4916, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.6853, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.4028, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.6045, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.5512, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5953, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5687, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4401, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.5699, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.5558, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.5037, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.5236, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.4195, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5014, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.49, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.6623, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.5812, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4206, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.4842, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.455, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.3477, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.3602, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4663, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.3849, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:22<00:32, 20.64it/s][INFO|trainer.py:738] 2023-11-14 13:44:37,919 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:44:37,922 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:44:37,922 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:44:37,922 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 66.87it/s]\u001b[A\n",
            " 60% 15/25 [00:00<00:00, 60.96it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5309996008872986, 'eval_accuracy': 0.715, 'eval_runtime': 0.4307, 'eval_samples_per_second': 464.341, 'eval_steps_per_second': 58.043, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:23<00:32, 20.64it/s]\n",
            "100% 25/25 [00:00<00:00, 61.12it/s]\u001b[A\n",
            "{'loss': 0.4577, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.523, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.3401, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.3492, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.2787, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.4025, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.4287, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.4708, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.5114, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4518, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:25<00:31, 20.13it/s][INFO|trainer.py:2883] 2023-11-14 13:44:40,838 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:44:40,840 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:44:41,104 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:44:41,105 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:44:41,108 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.3512, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.5883, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.4551, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.4417, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.5026, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.4164, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.3916, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.3925, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.3583, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.58, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.5258, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.6885, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4371, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.45, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.405, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.5188, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.3809, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.3929, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.4572, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.4208, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.5531, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.4782, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.4827, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3317, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.4435, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.4029, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3806, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.5224, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.2832, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.6012, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.5347, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.4986, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.546, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.3884, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.4428, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:35<00:21, 20.85it/s][INFO|trainer.py:738] 2023-11-14 13:44:50,515 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:44:50,517 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:44:50,517 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:44:50,517 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 76.88it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 71.41it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4415494501590729, 'eval_accuracy': 0.8, 'eval_runtime': 0.3853, 'eval_samples_per_second': 519.073, 'eval_steps_per_second': 64.884, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:35<00:21, 20.85it/s]\n",
            "100% 25/25 [00:00<00:00, 68.46it/s]\u001b[A\n",
            "{'loss': 0.3335, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.4721, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.3618, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.558, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.5462, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.3202, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.3774, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.3818, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.499, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.3169, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.5308, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.4117, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.4016, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.2942, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.6166, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.4169, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.4271, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.3709, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.3192, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.3459, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.3292, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.4224, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.3361, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.4827, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.4241, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.3874, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.352, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.247, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.4971, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.2609, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.4526, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.3234, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.4172, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.2913, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3969, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.307, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.3578, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.6099, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.4569, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.4265, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.4334, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.403, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.4547, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.5693, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.4676, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:46<00:11, 20.21it/s][INFO|trainer.py:738] 2023-11-14 13:45:01,828 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:45:01,830 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:45:01,830 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:45:01,830 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 28% 7/25 [00:00<00:00, 69.65it/s]\u001b[A\n",
            " 56% 14/25 [00:00<00:00, 62.81it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4099631607532501, 'eval_accuracy': 0.805, 'eval_runtime': 0.4309, 'eval_samples_per_second': 464.148, 'eval_steps_per_second': 58.018, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:47<00:11, 20.21it/s]\n",
            "100% 25/25 [00:00<00:00, 61.13it/s]\u001b[A\n",
            "{'loss': 0.484, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.3686, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.3184, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.4389, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3574, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.4153, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.5123, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.3134, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.4695, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.3993, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.2191, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.6688, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.2052, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.366, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.482, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3524, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.3482, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.5022, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.2697, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.2254, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [00:52<00:06, 19.68it/s][INFO|trainer.py:2883] 2023-11-14 13:45:07,298 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:45:07,300 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:45:07,528 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:45:07,529 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:45:07,530 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.3768, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.3145, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.5227, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.4722, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.3814, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.3204, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3058, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.2195, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.3801, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.4706, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.3222, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.3755, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.2761, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.3727, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.511, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.3247, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.5627, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.3722, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3554, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.5047, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.2834, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5028, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.3823, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.4096, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.4661, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:59<00:00, 21.24it/s][INFO|trainer.py:738] 2023-11-14 13:45:14,193 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:45:14,195 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:45:14,195 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:45:14,195 >>   Batch size = 8\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            " 32% 8/25 [00:00<00:00, 77.39it/s]\u001b[A\n",
            " 64% 16/25 [00:00<00:00, 72.75it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.4174812436103821, 'eval_accuracy': 0.795, 'eval_runtime': 0.3718, 'eval_samples_per_second': 537.899, 'eval_steps_per_second': 67.237, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:59<00:00, 21.24it/s]\n",
            "100% 25/25 [00:00<00:00, 71.20it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:45:14,568 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 59.4557, 'train_samples_per_second': 151.373, 'train_steps_per_second': 18.922, 'train_loss': 0.4839278375837538, 'epoch': 5.0}\n",
            "100% 1125/1125 [00:59<00:00, 18.92it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:45:14,571 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:45:14,572 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:45:14,763 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:45:14,764 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:45:14,765 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4839\n",
            "  train_runtime            = 0:00:59.45\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    151.373\n",
            "  train_steps_per_second   =     18.922\n",
            "11/14/2023 13:45:14 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:45:14,785 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:45:14,787 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:45:14,787 >>   Num examples = 200\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:45:14,787 >>   Batch size = 8\n",
            "100% 25/25 [00:00<00:00, 68.84it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =      0.795\n",
            "  eval_loss               =     0.4175\n",
            "  eval_runtime            = 0:00:00.38\n",
            "  eval_samples            =        200\n",
            "  eval_samples_per_second =    525.042\n",
            "  eval_steps_per_second   =      65.63\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:45:15,174 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.795}]}\n",
            "2023-11-14 13:45:19.766487: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-14 13:45:19.766544: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-14 13:45:19.766576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-14 13:45:20.919122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "11/14/2023 13:45:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "11/14/2023 13:45:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.98,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=225,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/test,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=2,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.06,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "11/14/2023 13:45:24 - INFO - __main__ - load a local file for train: /content/altegrad.lab3/data/cls.books-json/train.json\n",
            "11/14/2023 13:45:24 - INFO - __main__ - load a local file for validation: /content/altegrad.lab3/data/cls.books-json/test.json\n",
            "Using custom data configuration default-432f54c1cb7c184e\n",
            "11/14/2023 13:45:24 - INFO - datasets.builder - Using custom data configuration default-432f54c1cb7c184e\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "11/14/2023 13:45:24 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "11/14/2023 13:45:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "11/14/2023 13:45:24 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "11/14/2023 13:45:24 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:45:24,656 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:45:24,663 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:569] 2023-11-14 13:45:24,663 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:45:24,663 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:45:24,665 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:45:24,666 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:45:24,666 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:45:24,666 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:45:24,666 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-11-14 13:45:24,666 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:45:24,667 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:45:24,668 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-11-14 13:45:24,818 >> loading configuration file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-11-14 13:45:24,819 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 258,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3118] 2023-11-14 13:45:24,980 >> loading weights file /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3940] 2023-11-14 13:45:25,411 >> Some weights of the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3952] 2023-11-14 13:45:25,411 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "11/14/2023 13:45:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-230805a53997f34b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ebf09547b96d3b2.arrow\n",
            "11/14/2023 13:45:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-432f54c1cb7c184e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4ebf09547b96d3b2.arrow\n",
            "11/14/2023 13:45:25 - INFO - __main__ - Sample 1767 of the training set: {'label': 0, 'text': \"Les trois premiers tomes m'ont totalement transporté! j'ai été époustoufflé par les rebondissements, dans le tome 4 je retrouve le plaisir de lire le style d'écriture de l'autrice mais alors qu'est ce que je suis déçu par le contenu. J'ai mis un jour pour lire chaque tome et le quatrième je l'ai lu en une semaine tellement chaque chapitre me désespéraient c'est pour dire! Certes beaucoup de rebondissements encore mais tellement décevant, cela devient rapidement inintéressant en fait.\\n\", 'input_ids': [0, 1161, 9488, 22841, 7509, 7, 321, 25, 3588, 25440, 2976, 398, 38, 1117, 25, 437, 2649, 356, 615, 16167, 7455, 2034, 335, 191, 405, 14232, 13979, 7, 4, 637, 94, 7509, 193, 55, 20917, 94, 10640, 8, 13123, 94, 7216, 103, 25, 29890, 8, 95, 25, 703, 6819, 585, 5700, 813, 25, 449, 366, 41, 55, 2608, 1904, 6492, 335, 94, 15488, 5, 644, 25, 437, 898, 51, 4841, 482, 13123, 6866, 7509, 81, 94, 1649, 2143, 8038, 55, 95, 25, 437, 1938, 22, 616, 10958, 20945, 6866, 31094, 158, 14479, 89, 4803, 18636, 432, 25, 449, 482, 2664, 38, 3854, 1109, 5239, 8, 405, 14232, 13979, 7, 3888, 585, 20945, 8920, 13, 9661, 4, 4481, 26541, 16622, 23, 2259, 3638, 16220, 22, 1820, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:45:25 - INFO - __main__ - Sample 1738 of the training set: {'label': 0, 'text': 'Livre intéressant, avec pas mal d\\'humour, mais il y a un gros \"mais\".   Après avoir lu le principe de lucifer volume 1 et 2 de bloom, on peut penser que la théorie centrale de ce livre est discutable, voire vraiment incomplète. Le groupe influe énormément sur nos comportements, il n\\'y a pas que nos gènes!   Donc lisez ce livre, puis lisez le principe de lucifer pour une vision différente.\\n', 'input_ids': [0, 29601, 19111, 4, 1098, 402, 732, 103, 25, 30494, 4, 585, 200, 112, 10, 51, 11492, 44, 5915, 592, 12131, 4858, 1938, 94, 15866, 8, 18826, 1686, 7570, 105, 81, 114, 8, 9444, 287, 4, 97, 2293, 21023, 41, 21, 31406, 11809, 8, 366, 7451, 390, 22163, 1581, 4, 5226, 13, 6718, 19186, 10703, 67, 5, 526, 11224, 8749, 13, 31872, 511, 727, 26569, 7, 4, 200, 536, 25, 53, 10, 402, 41, 727, 6, 25426, 7, 38, 16214, 23947, 164, 366, 7451, 4, 7784, 23947, 164, 94, 15866, 8, 18826, 1686, 482, 616, 11240, 26629, 13, 5, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/14/2023 13:45:25 - INFO - __main__ - Sample 115 of the training set: {'label': 1, 'text': 'Livre absolument superbe avec des maisons ravissantes, des personnages superjolis. A avoir dans sa bibliothèque !\\n', 'input_ids': [0, 29601, 28596, 25144, 1098, 210, 7781, 7, 10903, 16220, 89, 4, 210, 19371, 7, 1092, 440, 2175, 5, 62, 4858, 637, 57, 31092, 573, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:738] 2023-11-14 13:45:28,802 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1724] 2023-11-14 13:45:28,811 >> ***** Running training *****\n",
            "[INFO|trainer.py:1725] 2023-11-14 13:45:28,811 >>   Num examples = 1,800\n",
            "[INFO|trainer.py:1726] 2023-11-14 13:45:28,811 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1727] 2023-11-14 13:45:28,811 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1730] 2023-11-14 13:45:28,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1731] 2023-11-14 13:45:28,811 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1732] 2023-11-14 13:45:28,811 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1733] 2023-11-14 13:45:28,812 >>   Number of trainable parameters = 23,093,250\n",
            "{'loss': 0.6636, 'learning_rate': 7.352941176470589e-07, 'epoch': 0.02}\n",
            "{'loss': 0.6776, 'learning_rate': 1.4705882352941177e-06, 'epoch': 0.04}\n",
            "{'loss': 0.6907, 'learning_rate': 2.2058823529411767e-06, 'epoch': 0.07}\n",
            "{'loss': 0.6984, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.09}\n",
            "{'loss': 0.7057, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.11}\n",
            "{'loss': 0.6917, 'learning_rate': 4.411764705882353e-06, 'epoch': 0.13}\n",
            "{'loss': 0.7218, 'learning_rate': 5.147058823529411e-06, 'epoch': 0.16}\n",
            "{'loss': 0.6783, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.18}\n",
            "{'loss': 0.6716, 'learning_rate': 6.61764705882353e-06, 'epoch': 0.2}\n",
            "{'loss': 0.7024, 'learning_rate': 7.352941176470589e-06, 'epoch': 0.22}\n",
            "{'loss': 0.6851, 'learning_rate': 8.088235294117648e-06, 'epoch': 0.24}\n",
            "{'loss': 0.6772, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.27}\n",
            "{'loss': 0.6741, 'learning_rate': 9.558823529411766e-06, 'epoch': 0.29}\n",
            "{'loss': 0.6796, 'learning_rate': 9.981078524124884e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6731, 'learning_rate': 9.933774834437086e-06, 'epoch': 0.33}\n",
            "{'loss': 0.6719, 'learning_rate': 9.886471144749291e-06, 'epoch': 0.36}\n",
            "{'loss': 0.6917, 'learning_rate': 9.839167455061495e-06, 'epoch': 0.38}\n",
            "{'loss': 0.6752, 'learning_rate': 9.7918637653737e-06, 'epoch': 0.4}\n",
            "{'loss': 0.6834, 'learning_rate': 9.744560075685904e-06, 'epoch': 0.42}\n",
            "{'loss': 0.6763, 'learning_rate': 9.697256385998109e-06, 'epoch': 0.44}\n",
            "{'loss': 0.6703, 'learning_rate': 9.649952696310313e-06, 'epoch': 0.47}\n",
            "{'loss': 0.6545, 'learning_rate': 9.602649006622518e-06, 'epoch': 0.49}\n",
            "{'loss': 0.6788, 'learning_rate': 9.555345316934722e-06, 'epoch': 0.51}\n",
            "{'loss': 0.6871, 'learning_rate': 9.508041627246925e-06, 'epoch': 0.53}\n",
            "{'loss': 0.6513, 'learning_rate': 9.46073793755913e-06, 'epoch': 0.56}\n",
            "{'loss': 0.6435, 'learning_rate': 9.413434247871334e-06, 'epoch': 0.58}\n",
            "{'loss': 0.7039, 'learning_rate': 9.366130558183539e-06, 'epoch': 0.6}\n",
            "{'loss': 0.66, 'learning_rate': 9.318826868495745e-06, 'epoch': 0.62}\n",
            "{'loss': 0.675, 'learning_rate': 9.271523178807948e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6806, 'learning_rate': 9.224219489120152e-06, 'epoch': 0.67}\n",
            "{'loss': 0.646, 'learning_rate': 9.176915799432357e-06, 'epoch': 0.69}\n",
            "{'loss': 0.6565, 'learning_rate': 9.129612109744561e-06, 'epoch': 0.71}\n",
            "{'loss': 0.6199, 'learning_rate': 9.082308420056766e-06, 'epoch': 0.73}\n",
            "{'loss': 0.6428, 'learning_rate': 9.03500473036897e-06, 'epoch': 0.76}\n",
            "{'loss': 0.6025, 'learning_rate': 8.987701040681174e-06, 'epoch': 0.78}\n",
            "{'loss': 0.6496, 'learning_rate': 8.940397350993379e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6314, 'learning_rate': 8.893093661305583e-06, 'epoch': 0.82}\n",
            "{'loss': 0.6872, 'learning_rate': 8.845789971617786e-06, 'epoch': 0.84}\n",
            "{'loss': 0.5474, 'learning_rate': 8.79848628192999e-06, 'epoch': 0.87}\n",
            "{'loss': 0.6132, 'learning_rate': 8.751182592242195e-06, 'epoch': 0.89}\n",
            "{'loss': 0.6637, 'learning_rate': 8.7038789025544e-06, 'epoch': 0.91}\n",
            "{'loss': 0.5369, 'learning_rate': 8.656575212866604e-06, 'epoch': 0.93}\n",
            "{'loss': 0.589, 'learning_rate': 8.609271523178809e-06, 'epoch': 0.96}\n",
            "{'loss': 0.5781, 'learning_rate': 8.561967833491013e-06, 'epoch': 0.98}\n",
            "{'loss': 0.6016, 'learning_rate': 8.514664143803218e-06, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:11<00:42, 21.31it/s][INFO|trainer.py:738] 2023-11-14 13:45:40,302 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:45:40,304 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:45:40,304 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:45:40,304 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 79.13it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 71.14it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 70.07it/s]\u001b[A\n",
            " 13% 32/250 [00:00<00:03, 69.33it/s]\u001b[A\n",
            " 16% 39/250 [00:00<00:03, 69.35it/s]\u001b[A\n",
            " 18% 46/250 [00:00<00:02, 69.20it/s]\u001b[A\n",
            " 21% 53/250 [00:00<00:02, 68.50it/s]\u001b[A\n",
            " 24% 60/250 [00:00<00:02, 68.74it/s]\u001b[A\n",
            " 27% 67/250 [00:00<00:02, 68.76it/s]\u001b[A\n",
            " 30% 74/250 [00:01<00:02, 68.18it/s]\u001b[A\n",
            " 32% 81/250 [00:01<00:02, 65.51it/s]\u001b[A\n",
            " 35% 88/250 [00:01<00:02, 64.30it/s]\u001b[A\n",
            " 38% 95/250 [00:01<00:02, 62.71it/s]\u001b[A\n",
            " 41% 102/250 [00:01<00:02, 62.16it/s]\u001b[A\n",
            " 44% 109/250 [00:01<00:02, 61.14it/s]\u001b[A\n",
            " 46% 116/250 [00:01<00:02, 58.47it/s]\u001b[A\n",
            " 49% 122/250 [00:01<00:02, 58.78it/s]\u001b[A\n",
            " 51% 128/250 [00:01<00:02, 58.05it/s]\u001b[A\n",
            " 54% 134/250 [00:02<00:02, 57.70it/s]\u001b[A\n",
            " 56% 140/250 [00:02<00:01, 58.33it/s]\u001b[A\n",
            " 59% 147/250 [00:02<00:01, 59.05it/s]\u001b[A\n",
            " 61% 153/250 [00:02<00:01, 58.92it/s]\u001b[A\n",
            " 64% 159/250 [00:02<00:01, 58.26it/s]\u001b[A\n",
            " 66% 165/250 [00:02<00:01, 58.42it/s]\u001b[A\n",
            " 68% 171/250 [00:02<00:01, 58.75it/s]\u001b[A\n",
            " 71% 178/250 [00:02<00:01, 59.31it/s]\u001b[A\n",
            " 74% 184/250 [00:02<00:01, 59.29it/s]\u001b[A\n",
            " 76% 190/250 [00:03<00:01, 59.11it/s]\u001b[A\n",
            " 79% 197/250 [00:03<00:00, 59.77it/s]\u001b[A\n",
            " 82% 204/250 [00:03<00:00, 60.55it/s]\u001b[A\n",
            " 84% 211/250 [00:03<00:00, 59.46it/s]\u001b[A\n",
            " 87% 218/250 [00:03<00:00, 59.05it/s]\u001b[A\n",
            " 90% 224/250 [00:03<00:00, 57.93it/s]\u001b[A\n",
            " 92% 231/250 [00:03<00:00, 59.15it/s]\u001b[A\n",
            " 95% 238/250 [00:03<00:00, 59.92it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5893244743347168, 'eval_accuracy': 0.6885, 'eval_runtime': 4.0861, 'eval_samples_per_second': 489.462, 'eval_steps_per_second': 61.183, 'epoch': 1.0}\n",
            " 20% 225/1125 [00:15<00:42, 21.31it/s]\n",
            "100% 250/250 [00:04<00:00, 60.29it/s]\u001b[A\n",
            "{'loss': 0.6392, 'learning_rate': 8.467360454115422e-06, 'epoch': 1.02}\n",
            "{'loss': 0.59, 'learning_rate': 8.420056764427627e-06, 'epoch': 1.04}\n",
            "{'loss': 0.5209, 'learning_rate': 8.37275307473983e-06, 'epoch': 1.07}\n",
            "{'loss': 0.5375, 'learning_rate': 8.325449385052034e-06, 'epoch': 1.09}\n",
            "{'loss': 0.6267, 'learning_rate': 8.278145695364238e-06, 'epoch': 1.11}\n",
            "{'loss': 0.6188, 'learning_rate': 8.230842005676445e-06, 'epoch': 1.13}\n",
            "{'loss': 0.4702, 'learning_rate': 8.183538315988647e-06, 'epoch': 1.16}\n",
            "{'loss': 0.6084, 'learning_rate': 8.136234626300852e-06, 'epoch': 1.18}\n",
            "{'loss': 0.488, 'learning_rate': 8.088930936613056e-06, 'epoch': 1.2}\n",
            "{'loss': 0.4519, 'learning_rate': 8.04162724692526e-06, 'epoch': 1.22}\n",
            "{'loss': 0.6111, 'learning_rate': 7.994323557237465e-06, 'epoch': 1.24}\n",
            "{'loss': 0.4985, 'learning_rate': 7.94701986754967e-06, 'epoch': 1.27}\n",
            "{'loss': 0.5102, 'learning_rate': 7.899716177861874e-06, 'epoch': 1.29}\n",
            "{'loss': 0.3624, 'learning_rate': 7.852412488174079e-06, 'epoch': 1.31}\n",
            "{'loss': 0.5021, 'learning_rate': 7.805108798486283e-06, 'epoch': 1.33}\n",
            "{'loss': 0.4562, 'learning_rate': 7.757805108798488e-06, 'epoch': 1.36}\n",
            "{'loss': 0.4582, 'learning_rate': 7.71050141911069e-06, 'epoch': 1.38}\n",
            "{'loss': 0.4499, 'learning_rate': 7.663197729422895e-06, 'epoch': 1.4}\n",
            "{'loss': 0.4381, 'learning_rate': 7.6158940397351e-06, 'epoch': 1.42}\n",
            "{'loss': 0.685, 'learning_rate': 7.568590350047305e-06, 'epoch': 1.44}\n",
            "{'loss': 0.4657, 'learning_rate': 7.521286660359509e-06, 'epoch': 1.47}\n",
            "{'loss': 0.4916, 'learning_rate': 7.473982970671713e-06, 'epoch': 1.49}\n",
            "{'loss': 0.6853, 'learning_rate': 7.4266792809839175e-06, 'epoch': 1.51}\n",
            "{'loss': 0.4028, 'learning_rate': 7.379375591296122e-06, 'epoch': 1.53}\n",
            "{'loss': 0.6045, 'learning_rate': 7.3320719016083265e-06, 'epoch': 1.56}\n",
            "{'loss': 0.5512, 'learning_rate': 7.28476821192053e-06, 'epoch': 1.58}\n",
            "{'loss': 0.5953, 'learning_rate': 7.237464522232735e-06, 'epoch': 1.6}\n",
            "{'loss': 0.5687, 'learning_rate': 7.190160832544939e-06, 'epoch': 1.62}\n",
            "{'loss': 0.4401, 'learning_rate': 7.1428571428571436e-06, 'epoch': 1.64}\n",
            "{'loss': 0.5699, 'learning_rate': 7.095553453169348e-06, 'epoch': 1.67}\n",
            "{'loss': 0.5558, 'learning_rate': 7.048249763481552e-06, 'epoch': 1.69}\n",
            "{'loss': 0.5037, 'learning_rate': 7.000946073793756e-06, 'epoch': 1.71}\n",
            "{'loss': 0.5236, 'learning_rate': 6.953642384105961e-06, 'epoch': 1.73}\n",
            "{'loss': 0.4195, 'learning_rate': 6.906338694418165e-06, 'epoch': 1.76}\n",
            "{'loss': 0.5014, 'learning_rate': 6.85903500473037e-06, 'epoch': 1.78}\n",
            "{'loss': 0.49, 'learning_rate': 6.811731315042573e-06, 'epoch': 1.8}\n",
            "{'loss': 0.6623, 'learning_rate': 6.764427625354778e-06, 'epoch': 1.82}\n",
            "{'loss': 0.5812, 'learning_rate': 6.717123935666982e-06, 'epoch': 1.84}\n",
            "{'loss': 0.4206, 'learning_rate': 6.669820245979188e-06, 'epoch': 1.87}\n",
            "{'loss': 0.4842, 'learning_rate': 6.622516556291392e-06, 'epoch': 1.89}\n",
            "{'loss': 0.455, 'learning_rate': 6.575212866603595e-06, 'epoch': 1.91}\n",
            "{'loss': 0.3477, 'learning_rate': 6.5279091769158e-06, 'epoch': 1.93}\n",
            "{'loss': 0.3602, 'learning_rate': 6.480605487228005e-06, 'epoch': 1.96}\n",
            "{'loss': 0.4663, 'learning_rate': 6.433301797540209e-06, 'epoch': 1.98}\n",
            "{'loss': 0.3849, 'learning_rate': 6.385998107852413e-06, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:26<00:32, 21.02it/s][INFO|trainer.py:738] 2023-11-14 13:45:55,521 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:45:55,523 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:45:55,523 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:45:55,523 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 74.57it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 71.04it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 68.69it/s]\u001b[A\n",
            " 12% 31/250 [00:00<00:03, 68.25it/s]\u001b[A\n",
            " 15% 38/250 [00:00<00:03, 67.38it/s]\u001b[A\n",
            " 18% 45/250 [00:00<00:03, 67.80it/s]\u001b[A\n",
            " 21% 52/250 [00:00<00:02, 67.96it/s]\u001b[A\n",
            " 24% 59/250 [00:00<00:02, 66.12it/s]\u001b[A\n",
            " 26% 66/250 [00:00<00:02, 66.84it/s]\u001b[A\n",
            " 29% 73/250 [00:01<00:02, 67.36it/s]\u001b[A\n",
            " 32% 80/250 [00:01<00:02, 66.92it/s]\u001b[A\n",
            " 35% 88/250 [00:01<00:02, 67.92it/s]\u001b[A\n",
            " 38% 95/250 [00:01<00:02, 67.90it/s]\u001b[A\n",
            " 41% 102/250 [00:01<00:02, 67.06it/s]\u001b[A\n",
            " 44% 109/250 [00:01<00:02, 67.73it/s]\u001b[A\n",
            " 46% 116/250 [00:01<00:01, 67.57it/s]\u001b[A\n",
            " 49% 123/250 [00:01<00:01, 65.61it/s]\u001b[A\n",
            " 52% 130/250 [00:01<00:01, 66.67it/s]\u001b[A\n",
            " 55% 137/250 [00:02<00:01, 67.01it/s]\u001b[A\n",
            " 58% 144/250 [00:02<00:01, 67.17it/s]\u001b[A\n",
            " 60% 151/250 [00:02<00:01, 67.24it/s]\u001b[A\n",
            " 63% 158/250 [00:02<00:01, 67.58it/s]\u001b[A\n",
            " 66% 165/250 [00:02<00:01, 66.98it/s]\u001b[A\n",
            " 69% 172/250 [00:02<00:01, 67.28it/s]\u001b[A\n",
            " 72% 179/250 [00:02<00:01, 67.58it/s]\u001b[A\n",
            " 74% 186/250 [00:02<00:00, 66.73it/s]\u001b[A\n",
            " 77% 193/250 [00:02<00:00, 67.05it/s]\u001b[A\n",
            " 80% 200/250 [00:02<00:00, 67.30it/s]\u001b[A\n",
            " 83% 207/250 [00:03<00:00, 66.30it/s]\u001b[A\n",
            " 86% 214/250 [00:03<00:00, 66.50it/s]\u001b[A\n",
            " 88% 221/250 [00:03<00:00, 67.18it/s]\u001b[A\n",
            " 91% 228/250 [00:03<00:00, 66.17it/s]\u001b[A\n",
            " 94% 235/250 [00:03<00:00, 67.00it/s]\u001b[A\n",
            " 97% 242/250 [00:03<00:00, 67.55it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.504534900188446, 'eval_accuracy': 0.7595, 'eval_runtime': 3.7463, 'eval_samples_per_second': 533.859, 'eval_steps_per_second': 66.732, 'epoch': 2.0}\n",
            " 40% 450/1125 [00:30<00:32, 21.02it/s]\n",
            "100% 250/250 [00:03<00:00, 66.57it/s]\u001b[A\n",
            "{'loss': 0.4577, 'learning_rate': 6.338694418164617e-06, 'epoch': 2.02}\n",
            "{'loss': 0.523, 'learning_rate': 6.291390728476822e-06, 'epoch': 2.04}\n",
            "{'loss': 0.3401, 'learning_rate': 6.244087038789026e-06, 'epoch': 2.07}\n",
            "{'loss': 0.3492, 'learning_rate': 6.196783349101231e-06, 'epoch': 2.09}\n",
            "{'loss': 0.2787, 'learning_rate': 6.149479659413434e-06, 'epoch': 2.11}\n",
            "{'loss': 0.4025, 'learning_rate': 6.102175969725639e-06, 'epoch': 2.13}\n",
            "{'loss': 0.4287, 'learning_rate': 6.054872280037843e-06, 'epoch': 2.16}\n",
            "{'loss': 0.4708, 'learning_rate': 6.007568590350048e-06, 'epoch': 2.18}\n",
            "{'loss': 0.5114, 'learning_rate': 5.960264900662252e-06, 'epoch': 2.2}\n",
            "{'loss': 0.4518, 'learning_rate': 5.912961210974456e-06, 'epoch': 2.22}\n",
            " 44% 500/1125 [00:32<00:30, 20.29it/s][INFO|trainer.py:2883] 2023-11-14 13:46:01,659 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:46:01,660 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:46:01,897 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:46:01,898 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:46:01,898 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.3512, 'learning_rate': 5.8656575212866605e-06, 'epoch': 2.24}\n",
            "{'loss': 0.5883, 'learning_rate': 5.818353831598865e-06, 'epoch': 2.27}\n",
            "{'loss': 0.4551, 'learning_rate': 5.7710501419110695e-06, 'epoch': 2.29}\n",
            "{'loss': 0.4417, 'learning_rate': 5.723746452223275e-06, 'epoch': 2.31}\n",
            "{'loss': 0.5026, 'learning_rate': 5.676442762535478e-06, 'epoch': 2.33}\n",
            "{'loss': 0.4164, 'learning_rate': 5.629139072847682e-06, 'epoch': 2.36}\n",
            "{'loss': 0.3916, 'learning_rate': 5.581835383159887e-06, 'epoch': 2.38}\n",
            "{'loss': 0.3925, 'learning_rate': 5.534531693472092e-06, 'epoch': 2.4}\n",
            "{'loss': 0.3583, 'learning_rate': 5.487228003784295e-06, 'epoch': 2.42}\n",
            "{'loss': 0.58, 'learning_rate': 5.4399243140965e-06, 'epoch': 2.44}\n",
            "{'loss': 0.5258, 'learning_rate': 5.3926206244087045e-06, 'epoch': 2.47}\n",
            "{'loss': 0.6885, 'learning_rate': 5.345316934720909e-06, 'epoch': 2.49}\n",
            "{'loss': 0.4371, 'learning_rate': 5.2980132450331135e-06, 'epoch': 2.51}\n",
            "{'loss': 0.45, 'learning_rate': 5.250709555345317e-06, 'epoch': 2.53}\n",
            "{'loss': 0.405, 'learning_rate': 5.203405865657522e-06, 'epoch': 2.56}\n",
            "{'loss': 0.5188, 'learning_rate': 5.156102175969726e-06, 'epoch': 2.58}\n",
            "{'loss': 0.3809, 'learning_rate': 5.108798486281931e-06, 'epoch': 2.6}\n",
            "{'loss': 0.3929, 'learning_rate': 5.061494796594135e-06, 'epoch': 2.62}\n",
            "{'loss': 0.4572, 'learning_rate': 5.014191106906339e-06, 'epoch': 2.64}\n",
            "{'loss': 0.4208, 'learning_rate': 4.966887417218543e-06, 'epoch': 2.67}\n",
            "{'loss': 0.5531, 'learning_rate': 4.919583727530748e-06, 'epoch': 2.69}\n",
            "{'loss': 0.4782, 'learning_rate': 4.872280037842952e-06, 'epoch': 2.71}\n",
            "{'loss': 0.4827, 'learning_rate': 4.824976348155157e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3317, 'learning_rate': 4.777672658467361e-06, 'epoch': 2.76}\n",
            "{'loss': 0.4435, 'learning_rate': 4.730368968779565e-06, 'epoch': 2.78}\n",
            "{'loss': 0.4029, 'learning_rate': 4.683065279091769e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3806, 'learning_rate': 4.635761589403974e-06, 'epoch': 2.82}\n",
            "{'loss': 0.5224, 'learning_rate': 4.588457899716178e-06, 'epoch': 2.84}\n",
            "{'loss': 0.2832, 'learning_rate': 4.541154210028383e-06, 'epoch': 2.87}\n",
            "{'loss': 0.6012, 'learning_rate': 4.493850520340587e-06, 'epoch': 2.89}\n",
            "{'loss': 0.5347, 'learning_rate': 4.446546830652792e-06, 'epoch': 2.91}\n",
            "{'loss': 0.4986, 'learning_rate': 4.399243140964995e-06, 'epoch': 2.93}\n",
            "{'loss': 0.546, 'learning_rate': 4.3519394512772e-06, 'epoch': 2.96}\n",
            "{'loss': 0.3884, 'learning_rate': 4.304635761589404e-06, 'epoch': 2.98}\n",
            "{'loss': 0.4428, 'learning_rate': 4.257332071901609e-06, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:42<00:22, 19.81it/s][INFO|trainer.py:738] 2023-11-14 13:46:11,563 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:46:11,566 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:46:11,566 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:46:11,566 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 7/250 [00:00<00:03, 67.72it/s]\u001b[A\n",
            "  6% 14/250 [00:00<00:03, 61.93it/s]\u001b[A\n",
            "  8% 21/250 [00:00<00:03, 60.72it/s]\u001b[A\n",
            " 11% 28/250 [00:00<00:03, 58.50it/s]\u001b[A\n",
            " 14% 34/250 [00:00<00:03, 58.00it/s]\u001b[A\n",
            " 16% 40/250 [00:00<00:03, 57.81it/s]\u001b[A\n",
            " 18% 46/250 [00:00<00:03, 58.34it/s]\u001b[A\n",
            " 21% 52/250 [00:00<00:03, 57.58it/s]\u001b[A\n",
            " 24% 59/250 [00:01<00:03, 58.32it/s]\u001b[A\n",
            " 26% 66/250 [00:01<00:03, 58.96it/s]\u001b[A\n",
            " 29% 72/250 [00:01<00:03, 57.31it/s]\u001b[A\n",
            " 31% 78/250 [00:01<00:03, 56.56it/s]\u001b[A\n",
            " 34% 85/250 [00:01<00:02, 57.72it/s]\u001b[A\n",
            " 36% 91/250 [00:01<00:02, 56.53it/s]\u001b[A\n",
            " 39% 98/250 [00:01<00:02, 58.01it/s]\u001b[A\n",
            " 42% 105/250 [00:01<00:02, 59.02it/s]\u001b[A\n",
            " 44% 111/250 [00:01<00:02, 58.52it/s]\u001b[A\n",
            " 47% 117/250 [00:02<00:02, 58.49it/s]\u001b[A\n",
            " 49% 123/250 [00:02<00:02, 56.41it/s]\u001b[A\n",
            " 52% 129/250 [00:02<00:02, 56.69it/s]\u001b[A\n",
            " 54% 135/250 [00:02<00:02, 56.37it/s]\u001b[A\n",
            " 56% 141/250 [00:02<00:01, 55.86it/s]\u001b[A\n",
            " 59% 147/250 [00:02<00:01, 55.04it/s]\u001b[A\n",
            " 61% 153/250 [00:02<00:01, 54.45it/s]\u001b[A\n",
            " 64% 159/250 [00:02<00:01, 55.04it/s]\u001b[A\n",
            " 66% 165/250 [00:02<00:01, 56.29it/s]\u001b[A\n",
            " 68% 171/250 [00:02<00:01, 56.75it/s]\u001b[A\n",
            " 71% 177/250 [00:03<00:01, 55.18it/s]\u001b[A\n",
            " 73% 183/250 [00:03<00:01, 56.32it/s]\u001b[A\n",
            " 76% 189/250 [00:03<00:01, 55.77it/s]\u001b[A\n",
            " 78% 195/250 [00:03<00:00, 55.06it/s]\u001b[A\n",
            " 81% 202/250 [00:03<00:00, 56.79it/s]\u001b[A\n",
            " 84% 209/250 [00:03<00:00, 58.07it/s]\u001b[A\n",
            " 86% 216/250 [00:03<00:00, 59.05it/s]\u001b[A\n",
            " 89% 223/250 [00:03<00:00, 58.45it/s]\u001b[A\n",
            " 92% 229/250 [00:03<00:00, 58.34it/s]\u001b[A\n",
            " 94% 236/250 [00:04<00:00, 60.02it/s]\u001b[A\n",
            " 97% 243/250 [00:04<00:00, 61.44it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.4572826623916626, 'eval_accuracy': 0.7905, 'eval_runtime': 4.3416, 'eval_samples_per_second': 460.657, 'eval_steps_per_second': 57.582, 'epoch': 3.0}\n",
            " 60% 675/1125 [00:47<00:22, 19.81it/s]\n",
            "100% 250/250 [00:04<00:00, 63.68it/s]\u001b[A\n",
            "{'loss': 0.3335, 'learning_rate': 4.210028382213813e-06, 'epoch': 3.02}\n",
            "{'loss': 0.4721, 'learning_rate': 4.162724692526017e-06, 'epoch': 3.04}\n",
            "{'loss': 0.3618, 'learning_rate': 4.115421002838222e-06, 'epoch': 3.07}\n",
            "{'loss': 0.558, 'learning_rate': 4.068117313150426e-06, 'epoch': 3.09}\n",
            "{'loss': 0.5462, 'learning_rate': 4.02081362346263e-06, 'epoch': 3.11}\n",
            "{'loss': 0.3202, 'learning_rate': 3.973509933774835e-06, 'epoch': 3.13}\n",
            "{'loss': 0.3774, 'learning_rate': 3.926206244087039e-06, 'epoch': 3.16}\n",
            "{'loss': 0.3818, 'learning_rate': 3.878902554399244e-06, 'epoch': 3.18}\n",
            "{'loss': 0.499, 'learning_rate': 3.8315988647114475e-06, 'epoch': 3.2}\n",
            "{'loss': 0.3169, 'learning_rate': 3.7842951750236524e-06, 'epoch': 3.22}\n",
            "{'loss': 0.5308, 'learning_rate': 3.7369914853358565e-06, 'epoch': 3.24}\n",
            "{'loss': 0.4117, 'learning_rate': 3.689687795648061e-06, 'epoch': 3.27}\n",
            "{'loss': 0.4016, 'learning_rate': 3.642384105960265e-06, 'epoch': 3.29}\n",
            "{'loss': 0.2942, 'learning_rate': 3.5950804162724695e-06, 'epoch': 3.31}\n",
            "{'loss': 0.6166, 'learning_rate': 3.547776726584674e-06, 'epoch': 3.33}\n",
            "{'loss': 0.4169, 'learning_rate': 3.500473036896878e-06, 'epoch': 3.36}\n",
            "{'loss': 0.4271, 'learning_rate': 3.4531693472090826e-06, 'epoch': 3.38}\n",
            "{'loss': 0.3709, 'learning_rate': 3.4058656575212866e-06, 'epoch': 3.4}\n",
            "{'loss': 0.3192, 'learning_rate': 3.358561967833491e-06, 'epoch': 3.42}\n",
            "{'loss': 0.3459, 'learning_rate': 3.311258278145696e-06, 'epoch': 3.44}\n",
            "{'loss': 0.3292, 'learning_rate': 3.2639545884579e-06, 'epoch': 3.47}\n",
            "{'loss': 0.4224, 'learning_rate': 3.2166508987701046e-06, 'epoch': 3.49}\n",
            "{'loss': 0.3361, 'learning_rate': 3.1693472090823087e-06, 'epoch': 3.51}\n",
            "{'loss': 0.4827, 'learning_rate': 3.122043519394513e-06, 'epoch': 3.53}\n",
            "{'loss': 0.4241, 'learning_rate': 3.074739829706717e-06, 'epoch': 3.56}\n",
            "{'loss': 0.3874, 'learning_rate': 3.0274361400189217e-06, 'epoch': 3.58}\n",
            "{'loss': 0.352, 'learning_rate': 2.980132450331126e-06, 'epoch': 3.6}\n",
            "{'loss': 0.247, 'learning_rate': 2.9328287606433302e-06, 'epoch': 3.62}\n",
            "{'loss': 0.4971, 'learning_rate': 2.8855250709555347e-06, 'epoch': 3.64}\n",
            "{'loss': 0.2609, 'learning_rate': 2.838221381267739e-06, 'epoch': 3.67}\n",
            "{'loss': 0.4526, 'learning_rate': 2.7909176915799437e-06, 'epoch': 3.69}\n",
            "{'loss': 0.3234, 'learning_rate': 2.7436140018921473e-06, 'epoch': 3.71}\n",
            "{'loss': 0.4172, 'learning_rate': 2.6963103122043523e-06, 'epoch': 3.73}\n",
            "{'loss': 0.2913, 'learning_rate': 2.6490066225165567e-06, 'epoch': 3.76}\n",
            "{'loss': 0.3969, 'learning_rate': 2.601702932828761e-06, 'epoch': 3.78}\n",
            "{'loss': 0.307, 'learning_rate': 2.5543992431409653e-06, 'epoch': 3.8}\n",
            "{'loss': 0.3578, 'learning_rate': 2.5070955534531694e-06, 'epoch': 3.82}\n",
            "{'loss': 0.6099, 'learning_rate': 2.459791863765374e-06, 'epoch': 3.84}\n",
            "{'loss': 0.4569, 'learning_rate': 2.4124881740775783e-06, 'epoch': 3.87}\n",
            "{'loss': 0.4265, 'learning_rate': 2.3651844843897824e-06, 'epoch': 3.89}\n",
            "{'loss': 0.4334, 'learning_rate': 2.317880794701987e-06, 'epoch': 3.91}\n",
            "{'loss': 0.403, 'learning_rate': 2.2705771050141914e-06, 'epoch': 3.93}\n",
            "{'loss': 0.4547, 'learning_rate': 2.223273415326396e-06, 'epoch': 3.96}\n",
            "{'loss': 0.5693, 'learning_rate': 2.1759697256386e-06, 'epoch': 3.98}\n",
            "{'loss': 0.4676, 'learning_rate': 2.1286660359508044e-06, 'epoch': 4.0}\n",
            " 80% 900/1125 [00:57<00:11, 20.29it/s][INFO|trainer.py:738] 2023-11-14 13:46:26,626 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:46:26,629 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:46:26,629 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:46:26,629 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 71.18it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 65.31it/s]\u001b[A\n",
            "  9% 23/250 [00:00<00:03, 63.99it/s]\u001b[A\n",
            " 12% 30/250 [00:00<00:03, 62.64it/s]\u001b[A\n",
            " 15% 37/250 [00:00<00:03, 60.87it/s]\u001b[A\n",
            " 18% 44/250 [00:00<00:03, 60.39it/s]\u001b[A\n",
            " 20% 51/250 [00:00<00:03, 60.80it/s]\u001b[A\n",
            " 23% 58/250 [00:00<00:03, 61.16it/s]\u001b[A\n",
            " 26% 65/250 [00:01<00:03, 61.08it/s]\u001b[A\n",
            " 29% 72/250 [00:01<00:02, 61.29it/s]\u001b[A\n",
            " 32% 79/250 [00:01<00:02, 61.29it/s]\u001b[A\n",
            " 34% 86/250 [00:01<00:02, 61.42it/s]\u001b[A\n",
            " 37% 93/250 [00:01<00:02, 61.40it/s]\u001b[A\n",
            " 40% 100/250 [00:01<00:02, 61.35it/s]\u001b[A\n",
            " 43% 107/250 [00:01<00:02, 60.46it/s]\u001b[A\n",
            " 46% 114/250 [00:01<00:02, 59.98it/s]\u001b[A\n",
            " 48% 121/250 [00:01<00:02, 59.36it/s]\u001b[A\n",
            " 51% 127/250 [00:02<00:02, 57.80it/s]\u001b[A\n",
            " 54% 134/250 [00:02<00:01, 58.71it/s]\u001b[A\n",
            " 56% 140/250 [00:02<00:01, 58.28it/s]\u001b[A\n",
            " 59% 147/250 [00:02<00:01, 59.19it/s]\u001b[A\n",
            " 62% 154/250 [00:02<00:01, 59.81it/s]\u001b[A\n",
            " 64% 160/250 [00:02<00:01, 59.36it/s]\u001b[A\n",
            " 67% 167/250 [00:02<00:01, 59.66it/s]\u001b[A\n",
            " 69% 173/250 [00:02<00:01, 59.73it/s]\u001b[A\n",
            " 72% 179/250 [00:02<00:01, 59.65it/s]\u001b[A\n",
            " 74% 186/250 [00:03<00:01, 60.20it/s]\u001b[A\n",
            " 77% 193/250 [00:03<00:00, 59.96it/s]\u001b[A\n",
            " 80% 199/250 [00:03<00:00, 59.89it/s]\u001b[A\n",
            " 82% 206/250 [00:03<00:00, 60.48it/s]\u001b[A\n",
            " 85% 213/250 [00:03<00:00, 60.91it/s]\u001b[A\n",
            " 88% 220/250 [00:03<00:00, 60.68it/s]\u001b[A\n",
            " 91% 227/250 [00:03<00:00, 60.43it/s]\u001b[A\n",
            " 94% 234/250 [00:03<00:00, 59.51it/s]\u001b[A\n",
            " 96% 241/250 [00:03<00:00, 60.52it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.46051159501075745, 'eval_accuracy': 0.797, 'eval_runtime': 4.1849, 'eval_samples_per_second': 477.907, 'eval_steps_per_second': 59.738, 'epoch': 4.0}\n",
            " 80% 900/1125 [01:01<00:11, 20.29it/s]\n",
            "100% 250/250 [00:04<00:00, 59.59it/s]\u001b[A\n",
            "{'loss': 0.484, 'learning_rate': 2.0813623462630085e-06, 'epoch': 4.02}\n",
            "{'loss': 0.3686, 'learning_rate': 2.034058656575213e-06, 'epoch': 4.04}\n",
            "{'loss': 0.3184, 'learning_rate': 1.9867549668874175e-06, 'epoch': 4.07}\n",
            "{'loss': 0.4389, 'learning_rate': 1.939451277199622e-06, 'epoch': 4.09}\n",
            "{'loss': 0.3574, 'learning_rate': 1.8921475875118262e-06, 'epoch': 4.11}\n",
            "{'loss': 0.4153, 'learning_rate': 1.8448438978240305e-06, 'epoch': 4.13}\n",
            "{'loss': 0.5123, 'learning_rate': 1.7975402081362348e-06, 'epoch': 4.16}\n",
            "{'loss': 0.3134, 'learning_rate': 1.750236518448439e-06, 'epoch': 4.18}\n",
            "{'loss': 0.4695, 'learning_rate': 1.7029328287606433e-06, 'epoch': 4.2}\n",
            "{'loss': 0.3993, 'learning_rate': 1.655629139072848e-06, 'epoch': 4.22}\n",
            "{'loss': 0.2191, 'learning_rate': 1.6083254493850523e-06, 'epoch': 4.24}\n",
            "{'loss': 0.6688, 'learning_rate': 1.5610217596972566e-06, 'epoch': 4.27}\n",
            "{'loss': 0.2052, 'learning_rate': 1.5137180700094608e-06, 'epoch': 4.29}\n",
            "{'loss': 0.366, 'learning_rate': 1.4664143803216651e-06, 'epoch': 4.31}\n",
            "{'loss': 0.482, 'learning_rate': 1.4191106906338694e-06, 'epoch': 4.33}\n",
            "{'loss': 0.3524, 'learning_rate': 1.3718070009460737e-06, 'epoch': 4.36}\n",
            "{'loss': 0.3482, 'learning_rate': 1.3245033112582784e-06, 'epoch': 4.38}\n",
            "{'loss': 0.5022, 'learning_rate': 1.2771996215704826e-06, 'epoch': 4.4}\n",
            "{'loss': 0.2697, 'learning_rate': 1.229895931882687e-06, 'epoch': 4.42}\n",
            "{'loss': 0.2254, 'learning_rate': 1.1825922421948912e-06, 'epoch': 4.44}\n",
            " 89% 1000/1125 [01:06<00:06, 19.97it/s][INFO|trainer.py:2883] 2023-11-14 13:46:35,817 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:46:35,819 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:46:36,139 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:46:36,142 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:46:36,142 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.3768, 'learning_rate': 1.1352885525070957e-06, 'epoch': 4.47}\n",
            "{'loss': 0.3145, 'learning_rate': 1.0879848628193e-06, 'epoch': 4.49}\n",
            "{'loss': 0.5227, 'learning_rate': 1.0406811731315042e-06, 'epoch': 4.51}\n",
            "{'loss': 0.4722, 'learning_rate': 9.933774834437087e-07, 'epoch': 4.53}\n",
            "{'loss': 0.3814, 'learning_rate': 9.460737937559131e-07, 'epoch': 4.56}\n",
            "{'loss': 0.3204, 'learning_rate': 8.987701040681174e-07, 'epoch': 4.58}\n",
            "{'loss': 0.3058, 'learning_rate': 8.514664143803217e-07, 'epoch': 4.6}\n",
            "{'loss': 0.2195, 'learning_rate': 8.041627246925261e-07, 'epoch': 4.62}\n",
            "{'loss': 0.3801, 'learning_rate': 7.568590350047304e-07, 'epoch': 4.64}\n",
            "{'loss': 0.4706, 'learning_rate': 7.095553453169347e-07, 'epoch': 4.67}\n",
            "{'loss': 0.3222, 'learning_rate': 6.622516556291392e-07, 'epoch': 4.69}\n",
            "{'loss': 0.3755, 'learning_rate': 6.149479659413435e-07, 'epoch': 4.71}\n",
            "{'loss': 0.2761, 'learning_rate': 5.676442762535478e-07, 'epoch': 4.73}\n",
            "{'loss': 0.3727, 'learning_rate': 5.203405865657521e-07, 'epoch': 4.76}\n",
            "{'loss': 0.511, 'learning_rate': 4.7303689687795655e-07, 'epoch': 4.78}\n",
            "{'loss': 0.3247, 'learning_rate': 4.2573320719016083e-07, 'epoch': 4.8}\n",
            "{'loss': 0.5627, 'learning_rate': 3.784295175023652e-07, 'epoch': 4.82}\n",
            "{'loss': 0.3722, 'learning_rate': 3.311258278145696e-07, 'epoch': 4.84}\n",
            "{'loss': 0.3554, 'learning_rate': 2.838221381267739e-07, 'epoch': 4.87}\n",
            "{'loss': 0.5047, 'learning_rate': 2.3651844843897828e-07, 'epoch': 4.89}\n",
            "{'loss': 0.2834, 'learning_rate': 1.892147587511826e-07, 'epoch': 4.91}\n",
            "{'loss': 0.5028, 'learning_rate': 1.4191106906338696e-07, 'epoch': 4.93}\n",
            "{'loss': 0.3823, 'learning_rate': 9.46073793755913e-08, 'epoch': 4.96}\n",
            "{'loss': 0.4096, 'learning_rate': 4.730368968779565e-08, 'epoch': 4.98}\n",
            "{'loss': 0.4661, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:13<00:00, 21.44it/s][INFO|trainer.py:738] 2023-11-14 13:46:42,739 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:46:42,741 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:46:42,741 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:46:42,741 >>   Batch size = 8\n",
            "\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 8/250 [00:00<00:03, 78.92it/s]\u001b[A\n",
            "  6% 16/250 [00:00<00:03, 73.65it/s]\u001b[A\n",
            " 10% 24/250 [00:00<00:03, 70.03it/s]\u001b[A\n",
            " 13% 32/250 [00:00<00:03, 69.74it/s]\u001b[A\n",
            " 16% 39/250 [00:00<00:03, 69.45it/s]\u001b[A\n",
            " 18% 46/250 [00:00<00:02, 68.36it/s]\u001b[A\n",
            " 21% 53/250 [00:00<00:02, 67.65it/s]\u001b[A\n",
            " 24% 60/250 [00:00<00:02, 68.35it/s]\u001b[A\n",
            " 27% 67/250 [00:00<00:02, 68.77it/s]\u001b[A\n",
            " 30% 74/250 [00:01<00:02, 67.76it/s]\u001b[A\n",
            " 32% 81/250 [00:01<00:02, 68.13it/s]\u001b[A\n",
            " 35% 88/250 [00:01<00:02, 68.09it/s]\u001b[A\n",
            " 38% 95/250 [00:01<00:02, 68.20it/s]\u001b[A\n",
            " 41% 102/250 [00:01<00:02, 68.69it/s]\u001b[A\n",
            " 44% 109/250 [00:01<00:02, 68.53it/s]\u001b[A\n",
            " 46% 116/250 [00:01<00:01, 68.44it/s]\u001b[A\n",
            " 49% 123/250 [00:01<00:01, 68.81it/s]\u001b[A\n",
            " 52% 130/250 [00:01<00:01, 68.70it/s]\u001b[A\n",
            " 55% 137/250 [00:01<00:01, 68.73it/s]\u001b[A\n",
            " 58% 144/250 [00:02<00:01, 68.51it/s]\u001b[A\n",
            " 60% 151/250 [00:02<00:01, 67.08it/s]\u001b[A\n",
            " 63% 158/250 [00:02<00:01, 67.42it/s]\u001b[A\n",
            " 66% 165/250 [00:02<00:01, 67.91it/s]\u001b[A\n",
            " 69% 172/250 [00:02<00:01, 66.83it/s]\u001b[A\n",
            " 72% 179/250 [00:02<00:01, 67.38it/s]\u001b[A\n",
            " 74% 186/250 [00:02<00:00, 68.14it/s]\u001b[A\n",
            " 77% 193/250 [00:02<00:00, 67.48it/s]\u001b[A\n",
            " 80% 201/250 [00:02<00:00, 68.75it/s]\u001b[A\n",
            " 83% 208/250 [00:03<00:00, 68.83it/s]\u001b[A\n",
            " 86% 215/250 [00:03<00:00, 68.01it/s]\u001b[A\n",
            " 89% 222/250 [00:03<00:00, 68.11it/s]\u001b[A\n",
            " 92% 229/250 [00:03<00:00, 68.66it/s]\u001b[A\n",
            " 94% 236/250 [00:03<00:00, 67.86it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.45731621980667114, 'eval_accuracy': 0.803, 'eval_runtime': 3.6802, 'eval_samples_per_second': 543.453, 'eval_steps_per_second': 67.932, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:17<00:00, 21.44it/s]\n",
            "100% 250/250 [00:03<00:00, 68.60it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1956] 2023-11-14 13:46:46,422 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 77.6104, 'train_samples_per_second': 115.964, 'train_steps_per_second': 14.495, 'train_loss': 0.4839278375837538, 'epoch': 5.0}\n",
            "100% 1125/1125 [01:17<00:00, 14.50it/s]\n",
            "[INFO|trainer.py:2883] 2023-11-14 13:46:46,424 >> Saving model checkpoint to checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2\n",
            "[INFO|configuration_utils.py:461] 2023-11-14 13:46:46,425 >> Configuration saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/config.json\n",
            "[INFO|modeling_utils.py:2193] 2023-11-14 13:46:46,678 >> Model weights saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2434] 2023-11-14 13:46:46,679 >> tokenizer config file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2443] 2023-11-14 13:46:46,680 >> Special tokens file saved in checkpoints/sentence_prediction/books/RoBERTa_small_fr_huggingface_ms8_lr1e-05_me5/2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.4839\n",
            "  train_runtime            = 0:01:17.61\n",
            "  train_samples            =       1800\n",
            "  train_samples_per_second =    115.964\n",
            "  train_steps_per_second   =     14.495\n",
            "11/14/2023 13:46:46 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:738] 2023-11-14 13:46:46,703 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3160] 2023-11-14 13:46:46,705 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3162] 2023-11-14 13:46:46,705 >>   Num examples = 2000\n",
            "[INFO|trainer.py:3165] 2023-11-14 13:46:46,705 >>   Batch size = 8\n",
            "100% 250/250 [00:04<00:00, 61.98it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =      0.803\n",
            "  eval_loss               =     0.4573\n",
            "  eval_runtime            = 0:00:04.04\n",
            "  eval_samples            =       2000\n",
            "  eval_samples_per_second =    494.093\n",
            "  eval_steps_per_second   =     61.762\n",
            "[INFO|modelcard.py:452] 2023-11-14 13:46:50,756 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.803}]}\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "        --model_name_or_path /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace \\\n",
        "        --train_file /content/altegrad.lab3/data/cls.books-json/train.json \\\n",
        "        --validation_file /content/altegrad.lab3/data/cls.books-json/valid.json \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --per_device_train_batch_size $MAX_SENTENCES \\\n",
        "        --num_train_epochs $MAX_EPOCH \\\n",
        "        --max_seq_length 256 \\\n",
        "        --learning_rate $LR \\\n",
        "        --lr_scheduler_type 'linear' \\\n",
        "        --weight_decay 0.01 \\\n",
        "        --adam_beta1 0.9 \\\n",
        "        --adam_beta2 0.98 \\\n",
        "        --adam_epsilon 1e-08 \\\n",
        "        --warmup_ratio 0.06 \\\n",
        "        --seed $SEED \\\n",
        "        --evaluation_strategy steps \\\n",
        "        --eval_steps 225 \\\n",
        "        --logging_steps 5 \\\n",
        "        --logging_dir $SAVE_DIR/valid \\\n",
        "        --metric_for_best_model 'accuracy' \\\n",
        "        --output_dir $SAVE_DIR \\\n",
        "        --overwrite_output_dir )\n",
        "\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "        --model_name_or_path /content/altegrad.lab3/models/RoBERTa_small_fr_HuggingFace \\\n",
        "        --train_file /content/altegrad.lab3/data/cls.books-json/train.json \\\n",
        "        --validation_file /content/altegrad.lab3/data/cls.books-json/test.json \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --per_device_train_batch_size $MAX_SENTENCES \\\n",
        "        --num_train_epochs $MAX_EPOCH \\\n",
        "        --max_seq_length 256 \\\n",
        "        --learning_rate $LR \\\n",
        "        --lr_scheduler_type 'linear' \\\n",
        "        --weight_decay 0.01 \\\n",
        "        --adam_beta1 0.9 \\\n",
        "        --adam_beta2 0.98 \\\n",
        "        --adam_epsilon 1e-08 \\\n",
        "        --warmup_ratio 0.06 \\\n",
        "        --seed $SEED \\\n",
        "        --evaluation_strategy steps \\\n",
        "        --eval_steps 225 \\\n",
        "        --logging_steps 5 \\\n",
        "        --logging_dir $SAVE_DIR/test \\\n",
        "        --metric_for_best_model 'accuracy' \\\n",
        "        --output_dir $SAVE_DIR \\\n",
        "        --overwrite_output_dir )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir checkpoints --port 6007"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GwN3KCm5Ec6r",
        "mUCV4V0ONKeJ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
